"""
OnlineRetrievalAndQA/module2_triple_filter.py
Module 2: LLM Triple Filtering (L·ªçc Triple b·∫±ng LLM)
S·ª≠ d·ª•ng LLM ƒë·ªÉ l·ªçc v√† ƒë√°nh gi√° relevance c·ªßa triples v·ªõi query

M√¥ t·∫£ chi ti·∫øt:
    Module n√†y nh·∫≠n raw triples t·ª´ Module 1 v√† s·ª≠ d·ª•ng LLM ƒë·ªÉ:
    1. ƒê√°nh gi√° relevance v·ªõi query ng∆∞·ªùi d√πng
    2. L·ªçc ra nh·ªØng triples ch·∫•t l∆∞·ª£ng cao  
    3. Scoring v√† ranking theo m·ª©c ƒë·ªô li√™n quan
    4. Backup system v·ªõi multiple LLM providers

Ki·∫øn tr√∫c ch√≠nh:
    - TripleFilterConfig: C·∫•u h√¨nh filtering parameters
    - LLMTripleFilter: Core filtering logic v·ªõi primary/backup LLMs
    - FilteredTriple: Output data structure v·ªõi rich metadata
    - RelevanceScorer: Scoring mechanism v√† level classification
    - BackupManager: Fallback LLM handling v√† error recovery

Workflow:
    Raw Triples + Query ‚Üí LLM Evaluation ‚Üí Relevance Scoring ‚Üí Filtered Triples
"""

from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Tuple
import logging
import time
import json
import re
from dataclasses import dataclass, asdict
from enum import Enum
import uuid
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
import datetime

# LLM integrations
import openai
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
import requests

# Shared utilities
from utils.utils_shared_general import (
    setup_logger,
    log_performance,
    validate_query,
    clean_text,
    save_json,
    load_json,
    get_api_key,
    load_environment_variables,
    PerformanceStats
)

# Module 1 imports
from module1_dual_retrieval import RetrievedItem

from huggingface_hub import InferenceClient

# Setup logger with file output
log_file = Path("outputs/log/module2_triple_filter_{}.log".format(datetime.datetime.now().strftime("%Y%m%d_%H%M%S")))
logger = setup_logger(__name__, log_file=log_file)

# ==================== ENUMS AND CONSTANTS ====================

class RelevanceLevel(Enum):
    """
    C√°c m·ª©c ƒë·ªô li√™n quan c·ªßa triple v·ªõi query
    """
    HIGHLY_RELEVANT = "highly_relevant"          # ƒêi·ªÉm 0.8-1.0
    MODERATELY_RELEVANT = "moderately_relevant"  # ƒêi·ªÉm 0.5-0.8
    SLIGHTLY_RELEVANT = "slightly_relevant"      # ƒêi·ªÉm 0.2-0.5
    NOT_RELEVANT = "not_relevant"               # ƒêi·ªÉm 0.0-0.2

    def get_score_range(self) -> Tuple[float, float]:
        """L·∫•y kho·∫£ng ƒëi·ªÉm s·ªë t∆∞∆°ng ·ª©ng v·ªõi m·ª©c ƒë·ªô relevance"""
        ranges = {
            RelevanceLevel.HIGHLY_RELEVANT: (0.8, 1.0),
            RelevanceLevel.MODERATELY_RELEVANT: (0.5, 0.8),
            RelevanceLevel.SLIGHTLY_RELEVANT: (0.2, 0.5),
            RelevanceLevel.NOT_RELEVANT: (0.0, 0.2)
        }
        return ranges[self]
    
    def get_description(self) -> str:
        """L·∫•y m√¥ t·∫£ chi ti·∫øt v·ªÅ m·ª©c ƒë·ªô relevance"""
        descriptions = {
            RelevanceLevel.HIGHLY_RELEVANT: "R·∫•t li√™n quan - Tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi",
            RelevanceLevel.MODERATELY_RELEVANT: "Li√™n quan v·ª´a ph·∫£i - Cung c·∫•p context h·ªØu √≠ch",
            RelevanceLevel.SLIGHTLY_RELEVANT: "√çt li√™n quan - Th√¥ng tin ph·ª• tr·ª£",
            RelevanceLevel.NOT_RELEVANT: "Kh√¥ng li√™n quan - Th√¥ng tin nhi·ªÖu"
        }
        return descriptions[self]

class FilteringStrategy(Enum):
    """C√°c chi·∫øn l∆∞·ª£c filtering kh√°c nhau"""
    STRICT = "strict"          # Threshold ‚â• 0.7
    MODERATE = "moderate"      # Threshold ‚â• 0.3
    LENIENT = "lenient"        # Threshold ‚â• 0.1
    ADAPTIVE = "adaptive"      # T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh threshold

    def get_threshold(self) -> float:
        """L·∫•y ng∆∞·ª°ng ƒëi·ªÉm s·ªë ƒë·ªÉ l·ªçc triples"""
        thresholds = {
            FilteringStrategy.STRICT: 0.7,
            FilteringStrategy.MODERATE: 0.3,
            FilteringStrategy.LENIENT: 0.1,
            FilteringStrategy.ADAPTIVE: 0.3
        }
        return thresholds[self]

class LLMProvider(Enum):
    """C√°c nh√† cung c·∫•p LLM ƒë∆∞·ª£c h·ªó tr·ª£"""
    QWEN = "qwen2.5-7b-instruct"
    GPT3_5 = "gpt-3.5-turbo"
    HUGGINGFACE_API = "huggingface-api"

# ==================== DATA CLASSES ====================

@dataclass
class TripleFilterConfig:
    """C·∫•u h√¨nh to√†n di·ªán cho h·ªá th·ªëng l·ªçc triple b·∫±ng LLM"""
    # LLM configuration
    primary_llm: LLMProvider = LLMProvider.QWEN
    backup_llm: LLMProvider = LLMProvider.GPT3_5
    
    # Batch processing parameters
    max_triples_per_batch: int = 8
    relevance_threshold: float = 0.3
    
    # Filtering strategy
    filtering_strategy: FilteringStrategy = FilteringStrategy.MODERATE
    
    # Reliability & performance
    max_retries: int = 3
    timeout_seconds: int = 45
    enable_caching: bool = True
    enable_backup: bool = True
    parallel_processing: bool = False
    
    # LLM generation parameters
    temperature: float = 0.1
    max_tokens: int = 800
    
    # Advanced parameters
    confidence_boost_factor: float = 1.2
    
    # API credentials
    huggingface_api_key: Optional[str] = None
    openai_api_key: Optional[str] = None
    
    def __post_init__(self):
        """T·ª± ƒë·ªông t·∫£i API keys t·ª´ environment variables"""
        env_vars = load_environment_variables()
        
        if self.huggingface_api_key is None:
            self.huggingface_api_key = get_api_key("huggingface", env_vars)
            
        if self.openai_api_key is None:
            self.openai_api_key = get_api_key("openai", env_vars)
        
        logger.info("üîë ƒê√£ t·∫£i c·∫•u h√¨nh API keys t·ª´ environment")
        logger.info(f"   üìä HuggingFace API: {'‚úÖ C√≥' if self.huggingface_api_key else '‚ùå Kh√¥ng'}")
        logger.info(f"   ü§ñ OpenAI API: {'‚úÖ C√≥' if self.openai_api_key else '‚ùå Kh√¥ng'}")

@dataclass
class FilteredTriple:
    """Triple ƒë√£ ƒë∆∞·ª£c l·ªçc v√† ƒë√°nh gi√° b·ªüi LLM v·ªõi metadata ƒë·∫ßy ƒë·ªß"""
    triple_id: str
    subject: str
    predicate: str
    object: str
    original_text: str
    query_relevance_score: float
    relevance_level: RelevanceLevel
    confidence_score: float
    llm_explanation: str
    source_passage_id: str
    original_hybrid_retrieval_score: float
    filtering_metadata: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        """Chuy·ªÉn ƒë·ªïi FilteredTriple object th√†nh dictionary"""
        result = asdict(self)
        result['relevance_level'] = self.relevance_level.value
        return result
    
    def is_relevant(self, threshold: Optional[float] = None) -> bool:
        """Ki·ªÉm tra triple c√≥ relevant v·ªõi query kh√¥ng"""
        if threshold is None:
            threshold = 0.3
        return self.query_relevance_score >= threshold
    
    def get_quality_score(self) -> float:
        """T√≠nh ƒëi·ªÉm ch·∫•t l∆∞·ª£ng t·ªïng h·ª£p c·ªßa triple"""
        quality_score = (0.7 * self.query_relevance_score + 0.3 * self.confidence_score)
        return min(quality_score, 1.0)
    
    def get_summary(self) -> str:
        """T·∫°o summary ng·∫Øn g·ªçn v·ªÅ triple"""
        return (f"({self.subject} ‚Üí {self.predicate} ‚Üí {self.object}) "
                f"| Relevance: {self.query_relevance_score:.3f} "
                f"| Level: {self.relevance_level.value} "
                f"| Quality: {self.get_quality_score():.3f}")

@dataclass
class FilteringResult:
    """K·∫øt qu·∫£ ho√†n ch·ªânh c·ªßa qu√° tr√¨nh filtering"""
    filtered_triples: List[FilteredTriple]
    original_count: int
    filtered_count: int
    filtering_time: float
    query: str
    statistics: Dict[str, Any]
    errors: List[Dict[str, Any]]
    
    def to_dict(self) -> Dict[str, Any]:
        """Chuy·ªÉn ƒë·ªïi FilteringResult th√†nh dictionary"""
        return {
            'filtered_triples': [triple.to_dict() for triple in self.filtered_triples],
            'original_count': self.original_count,
            'filtered_count': self.filtered_count,
            'filtering_time': self.filtering_time,
            'query': self.query,
            'statistics': self.statistics,
            'errors': self.errors
        }
    
    def save_to_file(self, filepath: Path):
        """L∆∞u k·∫øt qu·∫£ filtering v√†o file JSON"""
        save_json(self.to_dict(), filepath, indent=2)
        logger.info(f"üíæ ƒê√£ l∆∞u k·∫øt qu·∫£ filtering v√†o file: {filepath}")
        logger.info(f"   üìä T·ªïng triples: {self.original_count} ‚Üí {self.filtered_count}")
        logger.info(f"   ‚è±Ô∏è Th·ªùi gian: {self.filtering_time:.2f} gi√¢y")
    
    def get_filtering_efficiency(self) -> float:
        """T√≠nh hi·ªáu su·∫•t filtering"""
        if self.original_count == 0:
            return 0.0
        return self.filtered_count / self.original_count
    
    def get_relevance_distribution(self) -> Dict[str, int]:
        """Th·ªëng k√™ ph√¢n b·ªë c√°c m·ª©c ƒë·ªô relevance"""
        distribution = {}
        for level in RelevanceLevel:
            count = sum(1 for triple in self.filtered_triples 
                       if triple.relevance_level == level)
            distribution[level.value] = count
        return distribution

# ==================== LLM PROVIDERS ====================

class QwenTripleFilter:
    """LLM filter implementation s·ª≠ d·ª•ng Qwen2.5-7B-Instruct model qua API"""
    
    def __init__(self, config: TripleFilterConfig):
        """Kh·ªüi t·∫°o Qwen filter v·ªõi c·∫•u h√¨nh"""
        self.config = config
        self.client = InferenceClient(
            model="Qwen/Qwen2.5-7B-Instruct",
            token=self.config.huggingface_api_key
        )
        self.cache = {} if config.enable_caching else None
        
        logger.info(f"üß† Kh·ªüi t·∫°o QwenTripleFilter v·ªõi HuggingFace Inference API...")
        logger.info(f"   üíæ Caching: {'B·∫≠t' if config.enable_caching else 'T·∫Øt'}")
        logger.info(f"   üå°Ô∏è Temperature: {config.temperature}")
        logger.info(f"   üìè Max tokens: {config.max_tokens}")
    
    def filter_triples_batch(self, query: str, triples: List[RetrievedItem]) -> List[Dict[str, Any]]:
        """Filter m·ªôt batch triples s·ª≠ d·ª•ng Qwen API"""
        logger.info(f"üß† B·∫Øt ƒë·∫ßu filtering {len(triples)} triples v·ªõi Qwen API...")
        
        # Check cache n·∫øu enabled
        cache_key = None
        if self.cache is not None:
            cache_key = self._generate_cache_key(query, triples)
            if cache_key in self.cache:
                logger.info("üíæ T√¨m th·∫•y k·∫øt qu·∫£ trong cache, s·ª≠ d·ª•ng cached result")
                return self.cache[cache_key]
        
        start_time = time.time()
        
        try:
            # T·∫°o prompt
            prompt = self.create_filtering_prompt(query, triples)
            
            # G·ªçi API v·ªõi chat completion
            logger.info("   üîÑ ƒêang g·ª≠i request ƒë·∫øn HuggingFace Inference API (chat completion)...")
            completion = self.client.chat.completions.create(
                model="Qwen/Qwen2.5-7B-Instruct",
                messages=[{"role": "user", "content": prompt}],
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                top_p=0.01
            )
            
            response_text = completion.choices[0].message.content
            
            logger.info(f"   üìÑ API response length: {len(response_text)} characters")
            logger.info(f"   üîç Response preview: {response_text[:150]}...")
            
            # Parse response th√†nh structured data
            evaluations = self.parse_llm_response(response_text, triples)
            
            # Cache result n·∫øu enabled
            if self.cache is not None and cache_key is not None:
                self.cache[cache_key] = evaluations
                logger.info("üíæ ƒê√£ cache k·∫øt qu·∫£ filtering")
            
            end_time = time.time()
            log_performance("Qwen API batch filtering", start_time, end_time, {
                "batch_size": len(triples),
                "response_length": len(response_text),
                "evaluations_count": len(evaluations)
            })
            
            return evaluations
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói trong Qwen API filtering: {e}")
            logger.exception("Chi ti·∫øt l·ªói:")
            
            # Return fallback evaluations
            return self._create_fallback_evaluations(triples, f"qwen_api_error: {str(e)}")
    
    def create_filtering_prompt(self, query: str, triples: List[RetrievedItem]) -> str:
        """T·∫°o prompt chi ti·∫øt cho LLM ƒë·ªÉ ƒë√°nh gi√° relevance c·ªßa triples"""
        logger.info(f"üìù T·∫°o filtering prompt cho {len(triples)} triples...")
        
        prompt = f"""B·∫°n l√† m·ªôt chuy√™n gia ƒë√°nh gi√° th√¥ng tin v·ªõi kh·∫£ nƒÉng ph√¢n t√≠ch s√¢u v·ªÅ m·ª©c ƒë·ªô li√™n quan c·ªßa d·ªØ li·ªáu. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë√°nh gi√° m·ª©c ƒë·ªô li√™n quan c·ªßa c√°c triple tri th·ª©c v·ªõi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.

TH√îNG TIN ƒê√ÅNH GI√Å:
====================
C√ÇUH·ªéI: "{query}"

C√ÅC TRIPLE C·∫¶N ƒê√ÅNH GI√Å:
========================"""
        
        for i, triple in enumerate(triples, 1):
            subject = triple.metadata.get('subject', '').strip()
            predicate = triple.metadata.get('predicate', '').strip()
            obj = triple.metadata.get('object', '').strip()
            confidence = triple.metadata.get('confidence', 1.0)
            
            prompt += f"""

TRIPLE {i}:
-----------
‚úì Ch·ªß th·ªÉ (Subject): {subject}
‚úì Quan h·ªá (Predicate): {predicate}  
‚úì ƒê·ªëi t∆∞·ª£ng (Object): {obj}
‚úì ƒê·ªô tin c·∫≠y g·ªëc: {confidence:.3f}
‚úì Text g·ªëc: {triple.text}
"""
        
        prompt += f"""

H∆Ø·ªöNG D·∫™N ƒê√ÅNH GI√Å CHI TI·∫æT:
============================
Cho m·ªói triple, h√£y ƒë√°nh gi√° m·ª©c ƒë·ªô li√™n quan v·ªõi c√¢u h·ªèi theo c√°c ti√™u ch√≠ sau:

üìä THANG ƒêI·ªÇM 0.0 ‚Üí 1.0:
‚Ä¢ 0.8-1.0: HIGHLY_RELEVANT 
  ‚Üí Tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi, th√¥ng tin c·ªët l√µi c·∫ßn thi·∫øt
  ‚Üí V√≠ d·ª•: C√¢u h·ªèi v·ªÅ "l·ª£i √≠ch t√°o" ‚Üí Triple "t√°o ch·ª©a vitamin C"

‚Ä¢ 0.5-0.8: MODERATELY_RELEVANT
  ‚Üí Li√™n quan v√† h·ªØu √≠ch, cung c·∫•p context quan tr·ªçng
  ‚Üí V√≠ d·ª•: C√¢u h·ªèi v·ªÅ "l·ª£i √≠ch t√°o" ‚Üí Triple "vitamin C t·ªët cho s·ª©c kh·ªèe"

‚Ä¢ 0.2-0.5: SLIGHTLY_RELEVANT  
  ‚Üí C√≥ li√™n quan nh∆∞ng √≠t quan tr·ªçng, th√¥ng tin ph·ª• tr·ª£
  ‚Üí V√≠ d·ª•: C√¢u h·ªèi v·ªÅ "l·ª£i √≠ch t√°o" ‚Üí Triple "t√°o c√≥ m√†u ƒë·ªè"

‚Ä¢ 0.0-0.2: NOT_RELEVANT
  ‚Üí Kh√¥ng li√™n quan ho·∫∑c nhi·ªÖu, kh√¥ng ƒë√≥ng g√≥p v√†o c√¢u tr·∫£ l·ªùi
  ‚Üí V√≠ d·ª•: C√¢u h·ªèi v·ªÅ "l·ª£i √≠ch t√°o" ‚Üí Triple "cam c√≥ v·ªã chua"

üîç TI√äU CH√ç ƒê√ÅNH GI√Å:
1. M·ª©c ƒë·ªô tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi
2. T√≠nh h·ªØu √≠ch c·ªßa th√¥ng tin v·ªõi ng∆∞·ªùi d√πng
3. Ch·∫•t l∆∞·ª£ng v√† ƒë·ªô ch√≠nh x√°c c·ªßa triple
4. Context v√† background information value

üí° L∆ØU √ù QUAN TR·ªåNG:
‚Ä¢ ƒê√°nh gi√° d·ª±a tr√™n n·ªôi dung, kh√¥ng ph·∫£i c√∫ ph√°p
‚Ä¢ Xem x√©t c·∫£ subject, predicate v√† object
‚Ä¢ ∆Øu ti√™n th√¥ng tin tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi
‚Ä¢ Confidence g·ªëc ch·ªâ mang t√≠nh tham kh·∫£o

TR·∫¢ L·ªúI THEO ƒê·ªäNH D·∫†NG JSON CH√çNH X√ÅC:
=====================================
```json
{{
  "evaluations": [
    {{
      "triple_id": 1,
      "relevance_score": 0.85,
      "relevance_level": "highly_relevant",
      "explanation": "Triple n√†y tr·∫£ l·ªùi tr·ª±c ti·∫øp v·ªÅ l·ª£i √≠ch dinh d∆∞·ª°ng c·ªßa t√°o, cung c·∫•p th√¥ng tin c·ªët l√µi m√† ng∆∞·ªùi d√πng ƒëang t√¨m ki·∫øm. Vitamin C l√† m·ªôt l·ª£i √≠ch s·ª©c kh·ªèe quan tr·ªçng c·ªßa t√°o.",
      "confidence": 0.9
    }},
    {{
      "triple_id": 2,
      "relevance_score": 0.65,
      "relevance_level": "moderately_relevant", 
      "explanation": "Triple cung c·∫•p th√¥ng tin b·ªï sung h·ªØu √≠ch v·ªÅ c∆° ch·∫ø t√°c ƒë·ªông c·ªßa vitamin C ƒë·∫øn s·ª©c kh·ªèe, gi√∫p l√†m r√µ l·ª£i √≠ch c·ªßa t√°o.",
      "confidence": 0.8
    }}
  ]
}}
```

ƒê√ÅNH GI√Å CHI TI·∫æT:"""
        
        logger.info(f"   üìè Prompt length: {len(prompt)} characters")
        logger.info(f"   üî§ Estimated tokens: ~{len(prompt) // 4}")
        
        return prompt
    
    def parse_llm_response(self, response_text: str, triples: List[RetrievedItem]) -> List[Dict[str, Any]]:
        """Parse v√† validate response t·ª´ LLM th√†nh structured data"""
        logger.info("üîç ƒêang parse response t·ª´ Qwen LLM...")
        logger.info(f"   üìÑ Response length: {len(response_text)} characters")
        
        try:
            # B∆∞·ªõc 1: Extract JSON block t·ª´ response
            json_text = self._extract_json_from_response(response_text)
            
            # B∆∞·ªõc 2: Parse JSON
            parsed_data = json.loads(json_text)
            evaluations = parsed_data.get('evaluations', [])
            
            # B∆∞·ªõc 3: Validate v√† clean evaluations
            validated_evaluations = self._validate_evaluations(evaluations, triples)
            
            logger.info(f"‚úÖ Th√†nh c√¥ng parse {len(validated_evaluations)} evaluations t·ª´ LLM")
            logger.info(f"   üìä Ch·∫•t l∆∞·ª£ng parse: {len(validated_evaluations)}/{len(triples)} triples")
            
            return validated_evaluations
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói parse LLM response: {e}")
            logger.error(f"   üìÑ Response preview: {response_text[:300]}...")
            
            # Fallback: t·∫°o default evaluations
            logger.warning("‚ö†Ô∏è S·ª≠ d·ª•ng fallback evaluations v·ªõi moderate relevance...")
            return self._create_fallback_evaluations(triples, "parse_error")
    
    def _extract_json_from_response(self, response_text: str) -> str:
        """Extract JSON block t·ª´ LLM response v·ªõi multiple strategies"""
        # Strategy 1: T√¨m ```json block
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            if json_end != -1:
                json_text = response_text[json_start:json_end].strip()
                logger.info("   üéØ T√¨m th·∫•y JSON trong ```json block")
                return json_text
        
        # Strategy 2: T√¨m JSON object ƒë·∫ßu ti√™n
        start_idx = response_text.find('{')
        if start_idx != -1:
            brace_count = 0
            end_idx = start_idx
            
            for i, char in enumerate(response_text[start_idx:], start_idx):
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        end_idx = i + 1
                        break
            
            json_text = response_text[start_idx:end_idx]
            logger.info("   üéØ T√¨m th·∫•y JSON object ƒë·ªôc l·∫≠p")
            return json_text
        
        # Strategy 3: T√¨m array JSON
        start_idx = response_text.find('[')
        if start_idx != -1:
            bracket_count = 0
            end_idx = start_idx
            
            for i, char in enumerate(response_text[start_idx:], start_idx):
                if char == '[':
                    bracket_count += 1
                elif char == ']':
                    bracket_count -= 1
                    if bracket_count == 0:
                        end_idx = i + 1
                        break
            
            json_text = f'{{"evaluations": {response_text[start_idx:end_idx]}}}'
            logger.info("   üéØ T√¨m th·∫•y JSON array, ƒë√£ wrap v√†o object")
            return json_text
        
        raise ValueError("Kh√¥ng t√¨m th·∫•y JSON valid trong response")
    
    def _validate_evaluations(self, evaluations: List[Dict], triples: List[RetrievedItem]) -> List[Dict[str, Any]]:
        """Validate v√† clean evaluations t·ª´ LLM"""
        logger.info(f"üîç Validating {len(evaluations)} evaluations...")
        validated = []
        
        for i, eval_data in enumerate(evaluations):
            try:
                # Validate required fields
                triple_id = eval_data.get('triple_id', i + 1)
                relevance_score = float(eval_data.get('relevance_score', 0.5))
                relevance_level = eval_data.get('relevance_level', 'moderately_relevant')
                explanation = eval_data.get('explanation', 'Kh√¥ng c√≥ gi·∫£i th√≠ch')
                confidence = float(eval_data.get('confidence', 0.5))
                
                # Validate v√† clamp scores
                relevance_score = max(0.0, min(1.0, relevance_score))
                confidence = max(0.0, min(1.0, confidence))
                
                # Validate relevance level
                valid_levels = [level.value for level in RelevanceLevel]
                if relevance_level not in valid_levels:
                    logger.warning(f"   ‚ö†Ô∏è Invalid relevance level '{relevance_level}', using 'moderately_relevant'")
                    relevance_level = 'moderately_relevant'
                
                # Ensure consistency between score and level
                relevance_score = self._adjust_score_to_level(relevance_score, relevance_level)
                
                validated_eval = {
                    'triple_id': triple_id,
                    'relevance_score': relevance_score,
                    'relevance_level': relevance_level,
                    'explanation': explanation,
                    'confidence': confidence
                }
                
                validated.append(validated_eval)
                logger.info(f"   ‚úÖ Triple {triple_id}: {relevance_score:.3f} ({relevance_level})")
                
            except Exception as e:
                logger.error(f"   ‚ùå L·ªói validate evaluation {i+1}: {e}")
                # T·∫°o default evaluation
                default_eval = {
                    'triple_id': i + 1,
                    'relevance_score': 0.5,
                    'relevance_level': 'moderately_relevant',
                    'explanation': f'L·ªói validation: {str(e)}',
                    'confidence': 0.3
                }
                validated.append(default_eval)
        
        # ƒê·∫£m b·∫£o c√≥ ƒë·ªß evaluations cho t·∫•t c·∫£ triples
        while len(validated) < len(triples):
            missing_id = len(validated) + 1
            logger.warning(f"   ‚ö†Ô∏è Thi·∫øu evaluation cho triple {missing_id}, t·∫°o default")
            default_eval = {
                'triple_id': missing_id,
                'relevance_score': 0.4,
                'relevance_level': 'moderately_relevant',
                'explanation': 'Evaluation b·ªã thi·∫øu, s·ª≠ d·ª•ng gi√° tr·ªã m·∫∑c ƒë·ªãnh',
                'confidence': 0.3
            }
            validated.append(default_eval)
        
        logger.info(f"‚úÖ Validation ho√†n th√†nh: {len(validated)} evaluations")
        return validated
    
    def _adjust_score_to_level(self, score: float, level: str) -> float:
        """ƒêi·ªÅu ch·ªânh score ƒë·ªÉ ph√π h·ª£p v·ªõi relevance level"""
        level_enum = RelevanceLevel(level)
        min_score, max_score = level_enum.get_score_range()
        
        # N·∫øu score n·∫±m ngo√†i range c·ªßa level, ƒëi·ªÅu ch·ªânh v·ªÅ gi·ªØa range
        if score < min_score or score > max_score:
            adjusted_score = (min_score + max_score) / 2
            logger.info(f"   üîß ƒêi·ªÅu ch·ªânh score: {score:.3f} ‚Üí {adjusted_score:.3f} cho level {level}")
            return adjusted_score
        
        return score
    
    def _create_fallback_evaluations(self, triples: List[RetrievedItem], reason: str) -> List[Dict[str, Any]]:
        """T·∫°o fallback evaluations khi LLM parsing fail"""
        logger.warning(f"üîÑ T·∫°o fallback evaluations (reason: {reason})...")
        
        fallback_evaluations = []
        for i, triple in enumerate(triples, 1):
            # S·ª≠ d·ª•ng original retrieval score l√†m base
            base_score = getattr(triple, 'hybrid_score', 0.5)
            # Normalize v·ªÅ moderate range
            fallback_score = 0.3 + (base_score * 0.3)  # Scale to 0.3-0.6 range
            
            fallback_eval = {
                'triple_id': i,
                'relevance_score': fallback_score,
                'relevance_level': 'moderately_relevant',
                'explanation': f'ƒê√°nh gi√° fallback do {reason}. S·ª≠ d·ª•ng retrieval score l√†m base.',
                'confidence': 0.3
            }
            fallback_evaluations.append(fallback_eval)
        
        logger.warning(f"   üìä T·∫°o {len(fallback_evaluations)} fallback evaluations v·ªõi moderate relevance")
        return fallback_evaluations
    
    def _generate_cache_key(self, query: str, triples: List[RetrievedItem]) -> str:
        """T·∫°o cache key cho query v√† triples combination"""
        # T·∫°o hash t·ª´ query v√† triple contents
        triple_texts = [triple.text for triple in triples]
        content = f"{query}|{'|'.join(triple_texts)}"
        
        # Simple hash ƒë·ªÉ l√†m cache key
        cache_key = hashlib.md5(content.encode()).hexdigest()[:16]
        return cache_key

class GPTTripleFilter:
    """
    Backup LLM filter implementation s·ª≠ d·ª•ng OpenAI GPT-3.5-Turbo
    
    Class n√†y serve nh∆∞ backup khi Qwen model fail ho·∫∑c kh√¥ng available.
    S·ª≠ d·ª•ng OpenAI API v·ªõi optimized prompts cho filtering task.
    """
    
    def __init__(self, config: TripleFilterConfig):
        """
        Kh·ªüi t·∫°o GPT filter v·ªõi OpenAI API
        
        Args:
            config (TripleFilterConfig): C·∫•u h√¨nh filter parameters
        """
        self.config = config
        self.client = None
        self.cache = {} if config.enable_caching else None
        
        logger.info(f"ü§ñ Kh·ªüi t·∫°o GPTTripleFilter...")
        logger.info(f"   üíæ Caching: {'B·∫≠t' if config.enable_caching else 'T·∫Øt'}")
        logger.info(f"   üå°Ô∏è Temperature: {config.temperature}")
        logger.info(f"   üìè Max tokens: {config.max_tokens}")
        
        # Initialize OpenAI client
        self._initialize_client()
    
    def _initialize_client(self):
        """Kh·ªüi t·∫°o OpenAI client v·ªõi API key"""
        try:
            if not self.config.openai_api_key:
                raise ValueError("OpenAI API key kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y")
            
            openai.api_key = self.config.openai_api_key
            self.client = openai
            
            logger.info("‚úÖ OpenAI client ƒë√£ s·∫µn s√†ng")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói kh·ªüi t·∫°o OpenAI client: {e}")
            logger.error("üí° G·ª£i √Ω:")
            logger.error("   - Ki·ªÉm tra OPENAI_API_KEY trong environment")
            logger.error("   - ƒê·∫£m b·∫£o API key c√≤n h·∫°n v√† c√≥ credit")
            raise
    
    def create_filtering_prompt(self, query: str, triples: List[RetrievedItem]) -> str:
        """
        T·∫°o prompt t·ªëi ∆∞u cho GPT-3.5-Turbo ƒë·ªÉ ƒë√°nh gi√° triples
        
        Args:
            query (str): User query
            triples (List[RetrievedItem]): Triples c·∫ßn ƒë√°nh gi√°
            
        Returns:
            str: Optimized prompt cho GPT model
        """
        logger.info(f"üìù T·∫°o GPT filtering prompt cho {len(triples)} triples...")
        
        # GPT prompt format t·ªëi ∆∞u h∆°n cho instruction following
        prompt = f"""You are an expert information evaluator. Evaluate the relevance of knowledge triples to the user's question.

USER QUESTION: "{query}"

TRIPLES TO EVALUATE:
"""
        
        for i, triple in enumerate(triples, 1):
            subject = triple.metadata.get('subject', '').strip()
            predicate = triple.metadata.get('predicate', '').strip()
            obj = triple.metadata.get('object', '').strip()
            confidence = triple.metadata.get('confidence', 1.0)
            
            prompt += f"""
Triple {i}:
- Subject: {subject}
- Predicate: {predicate}
- Object: {obj}
- Original confidence: {confidence:.3f}
- Source text: {triple.text}
"""
        
        prompt += f"""

EVALUATION GUIDELINES:
- Score 0.8-1.0: HIGHLY_RELEVANT (directly answers the question)
- Score 0.5-0.8: MODERATELY_RELEVANT (provides useful context)
- Score 0.2-0.5: SLIGHTLY_RELEVANT (tangentially related)
- Score 0.0-0.2: NOT_RELEVANT (unrelated or noise)

Return JSON format:
```json
{{
  "evaluations": [
    {{
      "triple_id": 1,
      "relevance_score": 0.85,
      "relevance_level": "highly_relevant",
      "explanation": "This triple directly answers...",
      "confidence": 0.9
    }}
  ]
}}
```

Evaluate each triple:"""
        
        logger.info(f"   üìè GPT prompt length: {len(prompt)} characters")
        return prompt
    
    def filter_triples_batch(self, query: str, triples: List[RetrievedItem]) -> List[Dict[str, Any]]:
        """
        Filter batch triples s·ª≠ d·ª•ng GPT-3.5-Turbo API
        
        Args:
            query (str): User query
            triples (List[RetrievedItem]): Batch triples
            
        Returns:
            List[Dict[str, Any]]: Evaluations t·ª´ GPT
        """
        logger.info(f"ü§ñ B·∫Øt ƒë·∫ßu filtering {len(triples)} triples v·ªõi GPT-3.5-Turbo...")
        
        # Check cache
        cache_key = None
        if self.cache is not None:
            cache_key = self._generate_cache_key(query, triples)
            if cache_key in self.cache:
                logger.info("üíæ S·ª≠ d·ª•ng cached GPT result")
                return self.cache[cache_key]
        
        start_time = time.time()
        
        try:
            # T·∫°o prompt
            prompt = self.create_filtering_prompt(query, triples)
            
            # API call v·ªõi retry logic
            logger.info("   üîÑ G·ª≠i request ƒë·∫øn OpenAI API...")
            
            response = self.client.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system", 
                        "content": "You are an expert at evaluating the relevance of information triples to user questions. Always respond with valid JSON."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                timeout=self.config.timeout_seconds
            )
            
            response_text = response.choices[0].message.content
            
            logger.info(f"   üìÑ GPT response length: {len(response_text)} characters")
            logger.info(f"   üí∞ Tokens used: {response.usage.total_tokens}")
            
            # Parse response
            evaluations = self._parse_gpt_response(response_text, triples)
            
            # Cache result
            if self.cache is not None and cache_key is not None:
                self.cache[cache_key] = evaluations
                logger.info("üíæ ƒê√£ cache GPT result")
            
            end_time = time.time()
            log_performance("GPT batch filtering", start_time, end_time, {
                "batch_size": len(triples),
                "tokens_used": response.usage.total_tokens,
                "evaluations_count": len(evaluations)
            })
            
            return evaluations
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói GPT filtering: {e}")
            logger.exception("Chi ti·∫øt l·ªói GPT:")
            
            # Return fallback
            return self._create_fallback_evaluations(triples, f"gpt_error: {str(e)}")
    
    def _parse_gpt_response(self, response_text: str, triples: List[RetrievedItem]) -> List[Dict[str, Any]]:
        """Parse GPT response v·ªõi error handling"""
        try:
            # GPT th∆∞·ªùng return JSON clean h∆°n
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                json_text = response_text[json_start:json_end].strip()
            else:
                # T√¨m JSON object
                start_idx = response_text.find('{')
                end_idx = response_text.rfind('}') + 1
                json_text = response_text[start_idx:end_idx]
            
            parsed_data = json.loads(json_text)
            evaluations = parsed_data.get('evaluations', [])
            
            # Validate basic structure
            validated_evaluations = []
            for i, eval_data in enumerate(evaluations):
                validated_eval = {
                    'triple_id': eval_data.get('triple_id', i + 1),
                    'relevance_score': max(0.0, min(1.0, float(eval_data.get('relevance_score', 0.5)))),
                    'relevance_level': eval_data.get('relevance_level', 'moderately_relevant'),
                    'explanation': eval_data.get('explanation', 'GPT evaluation'),
                    'confidence': max(0.0, min(1.0, float(eval_data.get('confidence', 0.5))))
                }
                validated_evaluations.append(validated_eval)
            
            logger.info(f"‚úÖ Parsed {len(validated_evaluations)} GPT evaluations")
            return validated_evaluations
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói parse GPT response: {e}")
            return self._create_fallback_evaluations(triples, "gpt_parse_error")
    
    def _generate_cache_key(self, query: str, triples: List[RetrievedItem]) -> str:
        """T·∫°o cache key cho GPT requests"""
        triple_texts = [triple.text for triple in triples]
        content = f"gpt|{query}|{'|'.join(triple_texts)}"
        return hashlib.md5(content.encode()).hexdigest()[:16]
    
    def _create_fallback_evaluations(self, triples: List[RetrievedItem], reason: str) -> List[Dict[str, Any]]:
        """T·∫°o fallback evaluations cho GPT errors"""
        logger.warning(f"üîÑ T·∫°o GPT fallback evaluations (reason: {reason})...")
        
        fallback_evaluations = []
        for i, triple in enumerate(triples, 1):
            base_score = getattr(triple, 'hybrid_score', 0.5)
            fallback_score = 0.35 + (base_score * 0.25)  # Conservative scoring
            
            fallback_eval = {
                'triple_id': i,
                'relevance_score': fallback_score,
                'relevance_level': 'moderately_relevant',
                'explanation': f'GPT fallback evaluation due to {reason}',
                'confidence': 0.4
            }
            fallback_evaluations.append(fallback_eval)
        
        return fallback_evaluations

# ==================== MAIN TRIPLE FILTER ====================

class LLMTripleFilter:
    """
    Main LLM Triple Filter v·ªõi primary/backup system
    
    Class ch√≠nh ƒëi·ªÅu ph·ªëi vi·ªác filtering triples s·ª≠ d·ª•ng multiple LLM providers
    v·ªõi robust error handling v√† fallback mechanisms.
    """
    
    def __init__(self, config: Optional[TripleFilterConfig] = None):
        """
        Kh·ªüi t·∫°o LLM Triple Filter system
        
        Args:
            config (Optional[TripleFilterConfig]): C·∫•u h√¨nh filtering
        """
        self.config = config or TripleFilterConfig()
        
        logger.info("üöÄ Kh·ªüi t·∫°o LLMTripleFilter system...")
        logger.info(f"   üß† Primary LLM: {self.config.primary_llm.value}")
        logger.info(f"   üîÑ Backup LLM: {self.config.backup_llm.value}")
        logger.info(f"   üìä Strategy: {self.config.filtering_strategy.value}")
        logger.info(f"   üéØ Threshold: {self.config.relevance_threshold}")
        
        # Initialize LLM providers
        self.primary_filter = None
        self.backup_filter = None
        self.errors = []
        
        self._initialize_providers()
    
    def _initialize_providers(self):
        """Kh·ªüi t·∫°o c√°c LLM providers"""
        try:
            # Initialize primary LLM
            if self.config.primary_llm == LLMProvider.QWEN:
                logger.info("   üß† Kh·ªüi t·∫°o Qwen primary filter...")
                self.primary_filter = QwenTripleFilter(self.config)
            elif self.config.primary_llm == LLMProvider.GPT3_5:
                logger.info("   ü§ñ Kh·ªüi t·∫°o GPT-3.5 primary filter...")
                self.primary_filter = GPTTripleFilter(self.config)
            
            # Initialize backup LLM if enabled
            if self.config.enable_backup:
                if self.config.backup_llm == LLMProvider.GPT3_5 and self.config.primary_llm != LLMProvider.GPT3_5:
                    logger.info("   üîÑ Kh·ªüi t·∫°o GPT-3.5 backup filter...")
                    self.backup_filter = GPTTripleFilter(self.config)
                elif self.config.backup_llm == LLMProvider.QWEN and self.config.primary_llm != LLMProvider.QWEN:
                    logger.info("   üîÑ Kh·ªüi t·∫°o Qwen backup filter...")
                    self.backup_filter = QwenTripleFilter(self.config)
            
            logger.info("‚úÖ LLM providers ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói kh·ªüi t·∫°o LLM providers: {e}")
            self.errors.append({
                'timestamp': time.time(),
                'error': 'provider_initialization_failed',
                'message': str(e)
            })
            raise
    
    def filter_triples(self, query: str, raw_triples: List[RetrievedItem]) -> FilteringResult:
        """
        Main method ƒë·ªÉ filter triples s·ª≠ d·ª•ng LLM evaluation
        
        Args:
            query (str): User query
            raw_triples (List[RetrievedItem]): Raw triples t·ª´ Module 1
            
        Returns:
            FilteringResult: K·∫øt qu·∫£ filtering v·ªõi filtered triples
        """
        logger.info("=" * 60)
        logger.info("ü§ñ B·∫ÆT ƒê·∫¶U LLM TRIPLE FILTERING")
        logger.info("=" * 60)
        logger.info(f"üìù Query: '{query}'")
        logger.info(f"üìä Raw triples: {len(raw_triples)}")
        logger.info(f"üéØ Target threshold: {self.config.relevance_threshold}")
        
        start_time = time.time()
        
        # Validate input
        if not validate_query(query):
            logger.error("‚ùå Invalid query")
            return self._create_empty_result(query, 0, "invalid_query")
        
        if not raw_triples:
            logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ raw triples ƒë·ªÉ filter")
            return self._create_empty_result(query, 0, "no_input_triples")
        
        # Process triples in batches
        logger.info(f"\nüîÑ PROCESSING TRIPLES IN BATCHES")
        logger.info("-" * 40)
        
        all_evaluations = []
        batch_size = self.config.max_triples_per_batch
        total_batches = (len(raw_triples) + batch_size - 1) // batch_size
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(raw_triples))
            batch_triples = raw_triples[start_idx:end_idx]
            
            logger.info(f"üì¶ Batch {batch_idx + 1}/{total_batches}: triples {start_idx + 1}-{end_idx}")
            
            # Filter batch v·ªõi primary LLM
            batch_evaluations = self._filter_batch_with_fallback(query, batch_triples)
            all_evaluations.extend(batch_evaluations)
            
            logger.info(f"   ‚úÖ Ho√†n th√†nh batch {batch_idx + 1}, evaluations: {len(batch_evaluations)}")
        
        # Convert evaluations th√†nh FilteredTriple objects
        logger.info(f"\nüîß CONVERTING TO FILTERED TRIPLES")
        logger.info("-" * 40)
        
        filtered_triples = self._convert_to_filtered_triples(query, raw_triples, all_evaluations)
        
        # Apply filtering strategy
        logger.info(f"\nüéØ APPLYING FILTERING STRATEGY")
        logger.info("-" * 40)
        
        final_filtered_triples = self._apply_filtering_strategy(filtered_triples)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # T·∫°o statistics
        statistics = self._create_statistics(query, raw_triples, final_filtered_triples, total_time)
        
        # T·∫°o final result
        result = FilteringResult(
            filtered_triples=final_filtered_triples,
            original_count=len(raw_triples),
            filtered_count=len(final_filtered_triples),
            filtering_time=total_time,
            query=query,
            statistics=statistics,
            errors=self.errors.copy()
        )
        
        # Log final summary
        self._log_filtering_summary(result)
        
        return result
    
    def _filter_batch_with_fallback(self, query: str, batch_triples: List[RetrievedItem]) -> List[Dict[str, Any]]:
        """
        Filter batch v·ªõi primary LLM v√† fallback n·∫øu c·∫ßn
        
        Args:
            query (str): User query
            batch_triples (List[RetrievedItem]): Batch triples
            
        Returns:
            List[Dict[str, Any]]: Batch evaluations
        """
        # Try primary LLM first
        try:
            logger.info(f"   üß† S·ª≠ d·ª•ng {self.config.primary_llm.value} primary filter...")
            evaluations = self.primary_filter.filter_triples_batch(query, batch_triples)
            
            # Validate evaluations quality
            if self._validate_evaluations_quality(evaluations, batch_triples):
                logger.info("   ‚úÖ Primary LLM filtering th√†nh c√¥ng")
                return evaluations
            else:
                logger.warning("   ‚ö†Ô∏è Primary LLM evaluations ch·∫•t l∆∞·ª£ng th·∫•p, th·ª≠ backup...")
                raise ValueError("Low quality evaluations from primary LLM")
        
        except Exception as e:
            logger.error(f"   ‚ùå Primary LLM failed: {e}")
            self.errors.append({
                'timestamp': time.time(),
                'error': 'primary_llm_failed',
                'message': str(e),
                'batch_size': len(batch_triples)
            })
        
        # Try backup LLM
        if self.backup_filter:
            try:
                logger.info(f"   üîÑ S·ª≠ d·ª•ng {self.config.backup_llm.value} backup filter...")
                evaluations = self.backup_filter.filter_triples_batch(query, batch_triples)
                logger.info("   ‚úÖ Backup LLM filtering th√†nh c√¥ng")
                return evaluations
                
            except Exception as e:
                logger.error(f"   ‚ùå Backup LLM c≈©ng failed: {e}")
                self.errors.append({
                    'timestamp': time.time(),
                    'error': 'backup_llm_failed',
                    'message': str(e),
                    'batch_size': len(batch_triples)
                })
        
        # Final fallback: create conservative evaluations
        logger.warning("   üîÑ S·ª≠ d·ª•ng final conservative fallback...")
        return self._create_conservative_evaluations(batch_triples)
    
    def _validate_evaluations_quality(self, evaluations: List[Dict], triples: List[RetrievedItem]) -> bool:
        """
        Validate ch·∫•t l∆∞·ª£ng evaluations t·ª´ LLM
        
        Args:
            evaluations (List[Dict]): Evaluations t·ª´ LLM
            triples (List[RetrievedItem]): Original triples
            
        Returns:
            bool: True n·∫øu ch·∫•t l∆∞·ª£ng acceptable
        """
        # Check basic requirements
        if len(evaluations) != len(triples):
            logger.warning(f"   ‚ö†Ô∏è Evaluation count mismatch: {len(evaluations)} vs {len(triples)}")
            return False
        
        # Check for valid scores
        valid_scores = 0
        for eval_data in evaluations:
            score = eval_data.get('relevance_score', 0)
            if isinstance(score, (int, float)) and 0 <= score <= 1:
                valid_scores += 1
        
        quality_ratio = valid_scores / len(evaluations)
        logger.info(f"   üìä Evaluation quality: {quality_ratio:.2f} ({valid_scores}/{len(evaluations)} valid)")
        
        return quality_ratio >= 0.8  # Require 80% valid evaluations
    
    def _create_conservative_evaluations(self, triples: List[RetrievedItem]) -> List[Dict[str, Any]]:
        """
        T·∫°o conservative evaluations khi c·∫£ primary v√† backup fail
        
        Args:
            triples (List[RetrievedItem]): Triples c·∫ßn evaluate
            
        Returns:
            List[Dict[str, Any]]: Conservative evaluations
        """
        logger.warning("   üõ°Ô∏è T·∫°o conservative evaluations...")
        
        conservative_evaluations = []
        for i, triple in enumerate(triples, 1):
            # Use retrieval score as base, but be conservative
            base_score = getattr(triple, 'hybrid_score', 0.5)
            conservative_score = 0.25 + (base_score * 0.35)  # Scale to 0.25-0.6
            
            eval_data = {
                'triple_id': i,
                'relevance_score': conservative_score,
                'relevance_level': 'moderately_relevant',
                'explanation': 'Conservative evaluation - c·∫£ primary v√† backup LLM ƒë·ªÅu fail',
                'confidence': 0.3
            }
            conservative_evaluations.append(eval_data)
        
        logger.warning(f"   üìä T·∫°o {len(conservative_evaluations)} conservative evaluations")
        return conservative_evaluations
    
    def _convert_to_filtered_triples(self, query: str, raw_triples: List[RetrievedItem], 
                                   evaluations: List[Dict[str, Any]]) -> List[FilteredTriple]:
        """
        Convert evaluations th√†nh FilteredTriple objects
        
        Args:
            query (str): User query
            raw_triples (List[RetrievedItem]): Original triples
            evaluations (List[Dict]): LLM evaluations
            
        Returns:
            List[FilteredTriple]: Converted filtered triples
        """
        logger.info(f"üîß Converting {len(evaluations)} evaluations th√†nh FilteredTriple objects...")
        
        filtered_triples = []
        
        for i, (raw_triple, eval_data) in enumerate(zip(raw_triples, evaluations)):
            try:
                # Extract evaluation data
                relevance_score = eval_data.get('relevance_score', 0.5)
                relevance_level_str = eval_data.get('relevance_level', 'moderately_relevant')
                llm_explanation = eval_data.get('explanation', 'Kh√¥ng c√≥ gi·∫£i th√≠ch')
                eval_confidence = eval_data.get('confidence', 0.5)
                
                # Convert relevance level string to enum
                try:
                    relevance_level = RelevanceLevel(relevance_level_str)
                except ValueError:
                    logger.warning(f"   ‚ö†Ô∏è Invalid relevance level '{relevance_level_str}', using MODERATELY_RELEVANT")
                    relevance_level = RelevanceLevel.MODERATELY_RELEVANT
                
                # Extract triple metadata
                subject = raw_triple.metadata.get('subject', '')
                predicate = raw_triple.metadata.get('predicate', '')
                obj = raw_triple.metadata.get('object', '')
                source_passage_id = raw_triple.metadata.get('source_passage_id', '')
                original_confidence = raw_triple.metadata.get('confidence', 1.0)
                
                # Create filtering metadata
                filtering_metadata = {
                    'llm_provider': self.config.primary_llm.value,
                    'filtering_strategy': self.config.filtering_strategy.value,
                    'original_hybrid_score': raw_triple.hybrid_score,
                    'original_bm25_score': raw_triple.bm25_score,
                    'original_embedding_score': raw_triple.embedding_score,
                    'eval_confidence': eval_confidence,
                    'processing_timestamp': time.time()
                }
                
                # Create FilteredTriple object
                filtered_triple = FilteredTriple(
                    triple_id=raw_triple.item_id,
                    subject=subject,
                    predicate=predicate,
                    object=obj,
                    original_text=raw_triple.text,
                    query_relevance_score=relevance_score,
                    relevance_level=relevance_level,
                    confidence_score=original_confidence,
                    llm_explanation=llm_explanation,
                    source_passage_id=source_passage_id,
                    original_hybrid_retrieval_score=raw_triple.hybrid_score,
                    filtering_metadata=filtering_metadata
                )
                
                filtered_triples.append(filtered_triple)
                
                if (i + 1) % 10 == 0 or (i + 1) == len(evaluations):
                    logger.info(f"   üì¶ Converted {i + 1}/{len(evaluations)} triples")
                
            except Exception as e:
                logger.error(f"   ‚ùå L·ªói convert triple {i + 1}: {e}")
                # Skip invalid triple
                continue
        
        logger.info(f"‚úÖ Ho√†n th√†nh conversion: {len(filtered_triples)} FilteredTriple objects")
        return filtered_triples
    
    def _apply_filtering_strategy(self, filtered_triples: List[FilteredTriple]) -> List[FilteredTriple]:
        """
        √Åp d·ª•ng filtering strategy ƒë·ªÉ l·ªçc final triples
        
        Args:
            filtered_triples (List[FilteredTriple]): All filtered triples
            
        Returns:
            List[FilteredTriple]: Final filtered triples after strategy
        """
        logger.info(f"üéØ √Åp d·ª•ng filtering strategy: {self.config.filtering_strategy.value}")
        
        if self.config.filtering_strategy == FilteringStrategy.ADAPTIVE:
            # Adaptive strategy: ƒëi·ªÅu ch·ªânh threshold d·ª±a tr√™n distribution
            threshold = self._calculate_adaptive_threshold(filtered_triples)
            logger.info(f"   üß† Adaptive threshold: {threshold:.3f}")
        else:
            threshold = self.config.filtering_strategy.get_threshold()
            logger.info(f"   üéØ Fixed threshold: {threshold:.3f}")
        
        # Filter based on threshold
        final_triples = [
            triple for triple in filtered_triples 
            if triple.query_relevance_score >= threshold
        ]
        
        # Sort by quality score (combination of relevance and confidence)
        final_triples.sort(key=lambda t: t.get_quality_score(), reverse=True)
        
        # Log filtering results by relevance level
        distribution = {}
        for level in RelevanceLevel:
            count = sum(1 for triple in final_triples if triple.relevance_level == level)
            distribution[level.value] = count
            logger.info(f"   üìä {level.value}: {count} triples")
        
        logger.info(f"‚úÖ Filtering strategy applied: {len(filtered_triples)} ‚Üí {len(final_triples)} triples")
        return final_triples
    
    def _calculate_adaptive_threshold(self, filtered_triples: List[FilteredTriple]) -> float:
        """
        T√≠nh adaptive threshold d·ª±a tr√™n score distribution
        
        Args:
            filtered_triples (List[FilteredTriple]): All filtered triples
            
        Returns:
            float: Calculated adaptive threshold
        """
        if not filtered_triples:
            return 0.3  # Default threshold
        
        scores = [triple.query_relevance_score for triple in filtered_triples]
        scores.sort(reverse=True)
        
        # Calculate statistics
        avg_score = sum(scores) / len(scores)
        median_score = scores[len(scores) // 2] if scores else 0.3
        
        # Count high quality triples (‚â• 0.7)
        high_quality_count = sum(1 for score in scores if score >= 0.7)
        high_quality_ratio = high_quality_count / len(scores)
        
        logger.info(f"   üìä Score stats: avg={avg_score:.3f}, median={median_score:.3f}")
        logger.info(f"   üìà High quality ratio: {high_quality_ratio:.3f} ({high_quality_count}/{len(scores)})")
        
        # Adaptive logic
        if high_quality_ratio >= 0.3:  # Nhi·ªÅu triples ch·∫•t l∆∞·ª£ng cao
            threshold = 0.6  # Threshold cao ƒë·ªÉ l·ªçc ch·∫∑t
        elif high_quality_ratio >= 0.1:  # C√≥ m·ªôt s·ªë triples t·ªët
            threshold = 0.4  # Threshold v·ª´a ph·∫£i
        else:  # √çt triples ch·∫•t l∆∞·ª£ng cao
            threshold = 0.25  # Threshold th·∫•p ƒë·ªÉ gi·ªØ nhi·ªÅu triples
        
        # Kh√¥ng ƒë·ªÉ threshold qu√° th·∫•p ho·∫∑c qu√° cao
        threshold = max(0.2, min(0.8, threshold))
        
        return threshold
    
    def _create_statistics(self, query: str, raw_triples: List[RetrievedItem], 
                          filtered_triples: List[FilteredTriple], 
                          processing_time: float) -> Dict[str, Any]:
        """
        T·∫°o comprehensive statistics cho filtering process
        
        Args:
            query (str): Original query
            raw_triples (List[RetrievedItem]): Input triples
            filtered_triples (List[FilteredTriple]): Output triples
            processing_time (float): Total processing time
            
        Returns:
            Dict[str, Any]: Comprehensive statistics
        """
        logger.info("üìä T·∫°o filtering statistics...")
        
        # Basic counts
        original_count = len(raw_triples)
        filtered_count = len(filtered_triples)
        
        # Relevance distribution
        relevance_distribution = {}
        for level in RelevanceLevel:
            count = sum(1 for triple in filtered_triples if triple.relevance_level == level)
            relevance_distribution[level.value] = count
        
        # Score statistics
        if filtered_triples:
            relevance_scores = [t.query_relevance_score for t in filtered_triples]
            quality_scores = [t.get_quality_score() for t in filtered_triples]
            
            score_stats = {
                'relevance': {
                    'min': min(relevance_scores),
                    'max': max(relevance_scores),
                    'avg': sum(relevance_scores) / len(relevance_scores),
                    'median': sorted(relevance_scores)[len(relevance_scores) // 2]
                },
                'quality': {
                    'min': min(quality_scores),
                    'max': max(quality_scores),
                    'avg': sum(quality_scores) / len(quality_scores),
                    'median': sorted(quality_scores)[len(quality_scores) // 2]
                }
            }
        else:
            score_stats = {
                'relevance': {'min': 0, 'max': 0, 'avg': 0, 'median': 0},
                'quality': {'min': 0, 'max': 0, 'avg': 0, 'median': 0}
            }
        
        # Processing efficiency
        efficiency_stats = {
            'filtering_rate': filtered_count / original_count if original_count > 0 else 0,
            'triples_per_second': original_count / processing_time if processing_time > 0 else 0,
            'avg_time_per_triple': processing_time / original_count if original_count > 0 else 0
        }
        
        # LLM usage statistics
        llm_stats = {
            'primary_llm': self.config.primary_llm.value,
            'backup_llm': self.config.backup_llm.value if self.config.enable_backup else None,
            'total_errors': len(self.errors),
            'error_rate': len(self.errors) / original_count if original_count > 0 else 0
        }
        
        # Configuration used
        config_stats = {
            'filtering_strategy': self.config.filtering_strategy.value,
            'relevance_threshold': self.config.relevance_threshold,
            'max_batch_size': self.config.max_triples_per_batch,
            'temperature': self.config.temperature,
            'max_tokens': self.config.max_tokens
        }
        
        return {
            'query_info': {
                'original_query': query,
                'query_length': len(query),
                'query_words': len(query.split())
            },
            'processing_counts': {
                'original_triples': original_count,
                'filtered_triples': filtered_count,
                'removed_triples': original_count - filtered_count
            },
            'relevance_distribution': relevance_distribution,
            'score_statistics': score_stats,
            'efficiency_metrics': efficiency_stats,
            'llm_usage': llm_stats,
            'configuration': config_stats,
            'performance': {
                'total_time_seconds': processing_time,
                'timestamp': time.time()
            }
        }
    
    def _create_empty_result(self, query: str, original_count: int, reason: str) -> FilteringResult:
        """
        T·∫°o empty FilteringResult khi c√≥ l·ªói ho·∫∑c kh√¥ng c√≥ input
        
        Args:
            query (str): Original query
            original_count (int): Number of original triples
            reason (str): Reason for empty result
            
        Returns:
            FilteringResult: Empty result v·ªõi error info
        """
        logger.warning(f"üîÑ T·∫°o empty filtering result (reason: {reason})")
        
        error_info = {
            'timestamp': time.time(),
            'error': 'empty_result',
            'reason': reason,
            'original_count': original_count
        }
        
        return FilteringResult(
            filtered_triples=[],
            original_count=original_count,
            filtered_count=0,
            filtering_time=0.0,
            query=query,
            statistics={
                'query_info': {'original_query': query},
                'processing_counts': {'original_triples': original_count, 'filtered_triples': 0},
                'error_info': error_info
            },
            errors=[error_info]
        )
    
    def _log_filtering_summary(self, result: FilteringResult):
        """
        Log comprehensive summary c·ªßa filtering process
        
        Args:
            result (FilteringResult): Final filtering result
        """
        logger.info("=" * 60)
        logger.info("üéâ LLM TRIPLE FILTERING HO√ÄN TH√ÄNH")
        logger.info("=" * 60)
        
        # Basic metrics
        logger.info(f"üìä K·∫æT QU·∫¢ T·ªîNG QUAN:")
        logger.info(f"   üìù Query: '{result.query}'")
        logger.info(f"   ‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω: {result.filtering_time:.2f} gi√¢y")
        logger.info(f"   üìà Triples: {result.original_count} ‚Üí {result.filtered_count}")
        logger.info(f"   üìä T·ª∑ l·ªá gi·ªØ l·∫°i: {result.get_filtering_efficiency():.2%}")
        
        # Relevance distribution
        distribution = result.get_relevance_distribution()
        logger.info(f"\nüìä PH√ÇN B·ªê RELEVANCE:")
        for level, count in distribution.items():
            percentage = (count / result.filtered_count * 100) if result.filtered_count > 0 else 0
            level_desc = RelevanceLevel(level).get_description()
            logger.info(f"   {level}: {count} triples ({percentage:.1f}%) - {level_desc}")
        
        # Top quality triples
        if result.filtered_triples:
            logger.info(f"\nüèÜ TOP 5 FILTERED TRIPLES:")
            top_triples = sorted(result.filtered_triples, key=lambda t: t.get_quality_score(), reverse=True)[:5]
            for i, triple in enumerate(top_triples, 1):
                logger.info(f"   {i}. {triple.get_summary()}")
                logger.info(f"      üí≠ Explanation: {triple.llm_explanation[:100]}...")
        
        # Performance metrics
        stats = result.statistics
        if 'efficiency_metrics' in stats:
            efficiency = stats['efficiency_metrics']
            logger.info(f"\n‚ö° PERFORMANCE METRICS:")
            logger.info(f"   üèÉ Triples/gi√¢y: {efficiency['triples_per_second']:.1f}")
            logger.info(f"   ‚è±Ô∏è Avg time/triple: {efficiency['avg_time_per_triple']:.3f}s")
        
        # Error summary
        if result.errors:
            logger.info(f"\n‚ö†Ô∏è ERRORS ENCOUNTERED: {len(result.errors)}")
            for error in result.errors[-3:]:  # Show last 3 errors
                logger.info(f"   ‚ùå {error.get('error', 'unknown')}: {error.get('message', 'no message')}")
        else:
            logger.info(f"\n‚úÖ KH√îNG C√ì L·ªñI TRONG QU√Å TR√åNH FILTERING")
        
        logger.info("=" * 60)

# ==================== UTILITY FUNCTIONS ====================

def create_default_config() -> TripleFilterConfig:
    """
    T·∫°o c·∫•u h√¨nh m·∫∑c ƒë·ªãnh cho Triple Filter
    
    Returns:
        TripleFilterConfig: Default configuration
    """
    logger.info("‚öôÔ∏è T·∫°o default TripleFilterConfig...")
    
    config = TripleFilterConfig(
        primary_llm=LLMProvider.QWEN,
        backup_llm=LLMProvider.GPT3_5,
        max_triples_per_batch=8,
        relevance_threshold=0.3,
        filtering_strategy=FilteringStrategy.MODERATE,
        enable_backup=True,
        enable_caching=True,
        temperature=0.1,
        max_tokens=800
    )
    
    logger.info("‚úÖ Default config ƒë√£ ƒë∆∞·ª£c t·∫°o")
    return config

def quick_filter_triples(query: str, raw_triples: List[RetrievedItem], 
                        strategy: str = "moderate") -> FilteringResult:
    """
    Quick utility function ƒë·ªÉ filter triples v·ªõi c·∫•u h√¨nh ƒë∆°n gi·∫£n
    
    Args:
        query (str): User query
        raw_triples (List[RetrievedItem]): Raw triples t·ª´ Module 1
        strategy (str): Filtering strategy ("strict", "moderate", "lenient")
        
    Returns:
        FilteringResult: Filtering result
    """
    logger.info(f"üöÄ Quick filtering v·ªõi strategy: {strategy}")
    
    # Create config based on strategy
    config = create_default_config()
    config.filtering_strategy = FilteringStrategy(strategy)
    
    # Initialize filter v√† process
    filter_system = LLMTripleFilter(config)
    result = filter_system.filter_triples(query, raw_triples)
    
    return result

# ==================== TEST FUNCTIONS ====================

def test_triple_filter_with_mock_data():
    """Test Triple Filter v·ªõi mock data"""
    print("üß™ B·∫ÆT ƒê·∫¶U TEST TRIPLE FILTER")
    print("=" * 50)
    
    # Mock RetrievedItem objects
    mock_triples = [
        RetrievedItem(
            item_id="triple_001",
            item_type="triple",
            text="t√°o ch·ª©a vitamin C",
            bm25_score=0.8,
            embedding_score=0.9,
            hybrid_score=0.85,
            metadata={
                'subject': 't√°o',
                'predicate': 'ch·ª©a',
                'object': 'vitamin C',
                'confidence': 0.9,
                'source_passage_id': 'passage_001'
            }
        ),
        RetrievedItem(
            item_id="triple_002",
            item_type="triple",
            text="vitamin C t·ªët cho s·ª©c kh·ªèe",
            bm25_score=0.6,
            embedding_score=0.7,
            hybrid_score=0.65,
            metadata={
                'subject': 'vitamin C',
                'predicate': 't·ªët cho',
                'object': 's·ª©c kh·ªèe',
                'confidence': 0.8,
                'source_passage_id': 'passage_002'
            }
        ),
        RetrievedItem(
            item_id="triple_003",
            item_type="triple",
            text="t√°o c√≥ m√†u ƒë·ªè",
            bm25_score=0.3,
            embedding_score=0.2,
            hybrid_score=0.25,
            metadata={
                'subject': 't√°o',
                'predicate': 'c√≥ m√†u',
                'object': 'ƒë·ªè',
                'confidence': 0.7,
                'source_passage_id': 'passage_003'
            }
        )
    ]
    
    test_query = "L·ª£i √≠ch c·ªßa t√°o cho s·ª©c kh·ªèe l√† g√¨?"
    
    print(f"üìù Test query: '{test_query}'")
    print(f"üìä Mock triples: {len(mock_triples)}")
    
    try:
        # Test v·ªõi different strategies
        strategies = ["lenient", "moderate", "strict"]
        
        for strategy in strategies:
            print(f"\nüéØ Testing v·ªõi strategy: {strategy}")
            print("-" * 30)
            
            result = quick_filter_triples(test_query, mock_triples, strategy)
            
            print(f"   üìä K·∫øt qu·∫£: {result.original_count} ‚Üí {result.filtered_count} triples")
            print(f"   ‚è±Ô∏è Th·ªùi gian: {result.filtering_time:.2f}s")
            print(f"   üìà Efficiency: {result.get_filtering_efficiency():.2%}")
            
            # Show top filtered triples
            for i, triple in enumerate(result.filtered_triples[:2], 1):
                print(f"   {i}. {triple.get_summary()}")
    
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        logger.exception("Chi ti·∫øt l·ªói test:")

def test_individual_components():
    """Test c√°c components ri√™ng l·∫ª"""
    print("\nüß™ TEST INDIVIDUAL COMPONENTS")
    print("=" * 50)
    
    # Test RelevanceLevel enum
    print("üìä Test RelevanceLevel enum:")
    for level in RelevanceLevel:
        min_score, max_score = level.get_score_range()
        description = level.get_description()
        print(f"   {level.value}: [{min_score}-{max_score}] - {description}")
    
    # Test FilteringStrategy enum
    print("\nüéØ Test FilteringStrategy enum:")
    for strategy in FilteringStrategy:
        threshold = strategy.get_threshold()
        print(f"   {strategy.value}: threshold = {threshold}")
    
    # Test TripleFilterConfig
    print("\n‚öôÔ∏è Test TripleFilterConfig:")
    config = create_default_config()
    print(f"   Primary LLM: {config.primary_llm.value}")
    print(f"   Backup LLM: {config.backup_llm.value}")
    print(f"   Strategy: {config.filtering_strategy.value}")
    print(f"   Threshold: {config.relevance_threshold}")
    print(f"   Batch size: {config.max_triples_per_batch}")

if __name__ == "__main__":
    print("üöÄ B·∫ÆT ƒê·∫¶U CH·∫†Y T·∫§T C·∫¢ TESTS CHO MODULE 2")
    print("=" * 60)
    
    # Test 1: Individual components
    test_individual_components()
    
    print("\n" + "=" * 60)
    
    # Test 2: Mock data filtering (kh√¥ng c·∫ßn real LLM)
    print("‚ö†Ô∏è L∆ØU √ù: Test ti·∫øp theo s·∫Ω t·∫°o mock results thay v√¨ g·ªçi real LLM")
    test_triple_filter_with_mock_data()
    
    print("\nüéâ HO√ÄN TH√ÄNH T·∫§T C·∫¢ TESTS CHO MODULE 2!")
    print("=" * 60)
