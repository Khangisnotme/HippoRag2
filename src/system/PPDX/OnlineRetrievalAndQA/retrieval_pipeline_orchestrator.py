"""
OnlineRetrievalAndQA/retrieval_pipeline_orchestrator.py
Retrieval Pipeline Orchestrator - ƒêi·ªÅu ph·ªëi to√†n b·ªô qu√° tr√¨nh retrieval
K·∫øt h·ª£p Module 1, 2, 3, 4 ƒë·ªÉ t·∫°o ra complete retrieval pipeline

M√¥ t·∫£ chi ti·∫øt:
    Orchestrator n√†y ƒëi·ªÅu ph·ªëi vi·ªác th·ª±c thi c√°c modules theo th·ª© t·ª±:
    Module 1 (Dual Retrieval) ‚Üí Module 2 (Triple Filtering) ‚Üí 
    Module 3 (Passage Ranking) ‚Üí Module 4 (Context Expansion - Optional)
    
Workflow:
    Query ‚Üí Dual Retrieval ‚Üí Triple Filtering ‚Üí Passage Ranking ‚Üí [Context Expansion] ‚Üí Final Result

Output Format cho So s√°nh Baseline:
    Tr·∫£ ra standardized format ƒë·ªÉ so s√°nh v·ªõi baseline RAG:
    {
        "query_id": "Q001",
        "query": "L·ª£i √≠ch c·ªßa t√°o cho s·ª©c kh·ªèe",
        "top_passages": [
            {"passage_id": "P001", "rank": 1, "score": 0.85, "text": "..."},
            {"passage_id": "P015", "rank": 2, "score": 0.82, "text": "..."},
            {"passage_id": "P007", "rank": 3, "score": 0.78, "text": "..."},
            # ... ƒë·∫øn rank 10
        ],
        "method": "proposed_hipporag",
        "processing_time": 12.5,
        "statistics": {...}
    }

Evaluation Metrics:
    So s√°nh v·ªõi baseline RAG s·ª≠ d·ª•ng:
    - Precision@K: Trong top-K passages, c√≥ bao nhi√™u passages relevant?
    - Recall@K: C√≥ √≠t nh·∫•t 1 relevant passage trong top-K kh√¥ng?
    - MRR: Mean Reciprocal Rank c·ªßa relevant passage ƒë·∫ßu ti√™n
    
    V√≠ d·ª• so s√°nh:
    Baseline RAG output:     ["P015", "P032", "P001", "P088", "P007", ...]
    Proposed method output:  ["P001", "P007", "P015", "P025", "P044", ...]
    
    V·ªõi ground truth: ["P001", "P007", "P025"]
    Precision@3: Baseline=1/3=0.33, Proposed=3/3=1.00 (+200% improvement)
    Recall@5: Baseline=2/3=0.67, Proposed=3/3=1.00 (+49% improvement)
"""

from pathlib import Path
from typing import List, Dict, Any, Optional, Union
import logging
import time
import json
from dataclasses import dataclass, asdict
from datetime import datetime
import traceback
import pandas as pd
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils.dataframe import dataframe_to_rows

# Module imports
from module1_dual_retrieval import DualRetriever, RetrievalConfig, RetrievalResult
from module2_triple_filter import LLMTripleFilter, TripleFilterConfig, FilteringResult
from module3_passage_ranker import PassageRanker, PassageRankerConfig, RankingResult
from module4_context_expander import ContextExpander, ContextExpansionConfig, ExpansionResult

# Shared utilities
from utils.utils_shared_general import (
    setup_logger,
    log_performance,
    validate_query,
    save_json,
    load_json,
    create_query_metadata,
    PerformanceStats
)

# Create log directory if it doesn't exist
log_dir = Path("outputs/log")
log_dir.mkdir(parents=True, exist_ok=True)

# Setup logger with file output
log_file = log_dir / f"retrieval_pipeline_orchestrator_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logger = setup_logger(__name__, log_file=log_file)

# ==================== DATA CLASSES ====================

@dataclass
class RetrievalPipelineConfig:
    """
    C·∫•u h√¨nh to√†n di·ªán cho Retrieval Pipeline
    
    Attributes:
        # Module configurations
        dual_retrieval_config (RetrievalConfig): C·∫•u h√¨nh Module 1 - Dual Retrieval
        triple_filter_config (TripleFilterConfig): C·∫•u h√¨nh Module 2 - Triple Filtering
        passage_ranking_config (PassageRankerConfig): C·∫•u h√¨nh Module 3 - Passage Ranking
        context_expansion_config (ContextExpansionConfig): C·∫•u h√¨nh Module 4 - Context Expansion
        
        # Pipeline control
        enable_context_expansion (bool): C√≥ ch·∫°y Module 4 kh√¥ng
        max_final_passages (int): S·ªë l∆∞·ª£ng passages t·ªëi ƒëa trong k·∫øt qu·∫£ cu·ªëi
        enable_intermediate_saving (bool): C√≥ l∆∞u k·∫øt qu·∫£ trung gian kh√¥ng
        
        # Output formatting
        output_format (str): Format output ("standard", "evaluation", "detailed")
        include_metadata (bool): C√≥ include metadata chi ti·∫øt kh√¥ng
        include_intermediate_results (bool): C√≥ include k·∫øt qu·∫£ trung gian kh√¥ng
        
        # Performance settings
        timeout_seconds (int): Timeout cho to√†n b·ªô pipeline
        enable_performance_tracking (bool): C√≥ track performance kh√¥ng
    """
    # Module configurations
    dual_retrieval_config: RetrievalConfig = None
    triple_filter_config: TripleFilterConfig = None
    passage_ranking_config: PassageRankerConfig = None
    context_expansion_config: ContextExpansionConfig = None
    
    # Pipeline control
    enable_context_expansion: bool = False
    max_final_passages: int = 10
    enable_intermediate_saving: bool = False
    
    # Output formatting for evaluation
    output_format: str = "evaluation"  # "standard", "evaluation", "detailed"
    include_metadata: bool = True
    include_intermediate_results: bool = False
    
    # Performance settings
    timeout_seconds: int = 300  # 5 minutes timeout
    enable_performance_tracking: bool = True
    
    def __post_init__(self):
        """Initialize default configs n·∫øu kh√¥ng ƒë∆∞·ª£c provide"""
        if self.dual_retrieval_config is None:
            self.dual_retrieval_config = RetrievalConfig()
        if self.triple_filter_config is None:
            self.triple_filter_config = TripleFilterConfig()
        if self.passage_ranking_config is None:
            self.passage_ranking_config = PassageRankerConfig()
        if self.context_expansion_config is None:
            self.context_expansion_config = ContextExpansionConfig()

@dataclass
class RetrievalPipelineResult:
    """
    K·∫øt qu·∫£ ho√†n ch·ªânh c·ªßa Retrieval Pipeline
    
    Attributes:
        query_id (str): ID unique c·ªßa query
        query (str): Query g·ªëc c·ªßa user
        top_passages (List[Dict]): Top passages ƒë∆∞·ª£c ranked (format chu·∫©n cho evaluation)
        method (str): T√™n method ("proposed_hipporag")
        processing_time (float): T·ªïng th·ªùi gian x·ª≠ l√Ω
        
        # Intermediate results (optional)
        raw_retrieval_result (RetrievalResult): K·∫øt qu·∫£ Module 1
        filtering_result (FilteringResult): K·∫øt qu·∫£ Module 2
        ranking_result (RankingResult): K·∫øt qu·∫£ Module 3
        expansion_result (ExpansionResult): K·∫øt qu·∫£ Module 4 (n·∫øu c√≥)
        
        # Statistics v√† metadata
        statistics (Dict[str, Any]): Th·ªëng k√™ chi ti·∫øt
        config_used (RetrievalPipelineConfig): Config ƒë∆∞·ª£c s·ª≠ d·ª•ng
        errors (List[Dict]): Danh s√°ch errors n·∫øu c√≥
    """
    query_id: str
    query: str
    top_passages: List[Dict[str, Any]]
    method: str
    processing_time: float
    
    # Intermediate results
    raw_retrieval_result: Optional[RetrievalResult] = None
    filtering_result: Optional[FilteringResult] = None
    ranking_result: Optional[RankingResult] = None
    expansion_result: Optional[ExpansionResult] = None
    
    # Statistics
    statistics: Dict[str, Any] = None
    config_used: RetrievalPipelineConfig = None
    errors: List[Dict] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert th√†nh dictionary cho serialization"""
        result = {
            'query_id': self.query_id,
            'query': self.query,
            'top_passages': self.top_passages,
            'method': self.method,
            'processing_time': self.processing_time,
            'statistics': self.statistics or {},
            'errors': self.errors or []
        }
        
        # Add intermediate results n·∫øu available
        if self.raw_retrieval_result:
            result['raw_retrieval_result'] = self.raw_retrieval_result.to_dict()
        if self.filtering_result:
            result['filtering_result'] = self.filtering_result.to_dict()
        if self.ranking_result:
            result['ranking_result'] = self.ranking_result.to_dict()
        if self.expansion_result:
            result['expansion_result'] = self.expansion_result.to_dict()
        if self.config_used:
            result['config_used'] = asdict(self.config_used)
        
        return result
    
    def save_to_file(self, filepath: Path):
        """L∆∞u k·∫øt qu·∫£ v√†o file JSON"""
        save_json(self.to_dict(), filepath, indent=2)
        logger.info(f"üíæ ƒê√£ l∆∞u pipeline result v√†o: {filepath}")
    
    def get_evaluation_format(self) -> Dict[str, Any]:
        """
        Tr·∫£ v·ªÅ format chu·∫©n cho evaluation v·ªõi baseline
        
        Returns:
            Dict: Format chu·∫©n ƒë·ªÉ so s√°nh metrics
        """
        return {
            "query_id": self.query_id,
            "query": self.query,
            "method": self.method,
            "top_passages": self.top_passages,
            "processing_time": self.processing_time,
            "metadata": {
                "total_passages_retrieved": len(self.raw_retrieval_result.raw_passages) if self.raw_retrieval_result else 0,
                "filtered_triples_count": len(self.filtering_result.filtered_triples) if self.filtering_result else 0,
                "final_passages_count": len(self.top_passages)
            }
        }

    def to_excel_summary_row(self) -> Dict[str, Any]:
        """
        Convert result th√†nh row cho Excel summary sheet
        
        Returns:
            Dict: Row data cho summary sheet
        """
        top_scores = [p['score'] for p in self.top_passages] if self.top_passages else [0]
        
        return {
            'Query ID': self.query_id,
            'Query Text': self.query[:100] + ('...' if len(self.query) > 100 else ''),
            'Method': self.method,
            'Processing Time (s)': round(self.processing_time, 2),
            'Top Passages Count': len(self.top_passages),
            'Top Score': round(max(top_scores), 3) if top_scores else 0,
            'Average Score': round(sum(top_scores) / len(top_scores), 3) if top_scores else 0,
            'Min Score': round(min(top_scores), 3) if top_scores else 0,
            'Score Range': round(max(top_scores) - min(top_scores), 3) if len(top_scores) > 1 else 0
        }

    def to_excel_detailed_rows(self) -> List[Dict[str, Any]]:
        """
        Convert result th√†nh rows cho Excel detailed sheet
        
        Returns:
            List[Dict]: Rows data cho detailed sheet
        """
        rows = []
        for passage in self.top_passages:
            metadata = passage.get('metadata', {})
            
            row = {
                'Query ID': self.query_id,
                'Query Text': self.query[:50] + ('...' if len(self.query) > 50 else ''),
                'Rank': passage.get('rank', 0),
                'Passage ID': passage.get('passage_id', ''),
                'Final Score': round(passage.get('score', 0), 3),
                'Retrieval Score': round(metadata.get('hybrid_retrieval_score', 0), 3),
                'Support Score': round(metadata.get('support_score', 0), 3),
                'Supporting Triples': metadata.get('supporting_triples_count', 0),
                'Text Preview': passage.get('text', '')[:150] + ('...' if len(passage.get('text', '')) > 150 else ''),
                'Text Length': len(passage.get('text', '')),
                'Score Breakdown': str(metadata.get('score_breakdown', {}))
            }
            rows.append(row)
        
        return rows

# ==================== PIPELINE ORCHESTRATOR ====================

class RetrievalPipelineOrchestrator:
    """
    Main Orchestrator cho Retrieval Pipeline
    
    ƒêi·ªÅu ph·ªëi vi·ªác th·ª±c thi tu·∫ßn t·ª± c√°c modules v√† t·∫°o ra output chu·∫©n
    ƒë·ªÉ so s√°nh v·ªõi baseline RAG systems.
    """
    
    def __init__(self, config: Optional[RetrievalPipelineConfig] = None,
                 neo4j_uri: str = "bolt://localhost:7687",
                 neo4j_user: str = "neo4j", 
                 neo4j_password: str = "graphrag123"):
        """
        Kh·ªüi t·∫°o Pipeline Orchestrator
        
        Args:
            config (Optional[RetrievalPipelineConfig]): C·∫•u h√¨nh pipeline
            neo4j_uri (str): URI Neo4j database
            neo4j_user (str): Neo4j username
            neo4j_password (str): Neo4j password
        """
        self.config = config or RetrievalPipelineConfig()
        self.neo4j_uri = neo4j_uri
        self.neo4j_user = neo4j_user
        self.neo4j_password = neo4j_password
        
        # Initialize modules
        self.dual_retriever = None
        self.triple_filter = None
        self.passage_ranker = None
        self.context_expander = None
        
        # Performance tracking
        self.module_times = {}
        self.errors = []
        
        logger.info("üöÄ Kh·ªüi t·∫°o RetrievalPipelineOrchestrator...")
        logger.info(f"   üîß Context expansion: {'B·∫≠t' if self.config.enable_context_expansion else 'T·∫Øt'}")
        logger.info(f"   üìä Output format: {self.config.output_format}")
        logger.info(f"   üìã Max final passages: {self.config.max_final_passages}")
        
        self._initialize_modules()
    
    def _initialize_modules(self):
        """Kh·ªüi t·∫°o t·∫•t c·∫£ c√°c modules"""
        try:
            logger.info("üîß ƒêang kh·ªüi t·∫°o c√°c modules...")
            
            # Module 1: Dual Retriever
            logger.info("   üìä Kh·ªüi t·∫°o Module 1 - Dual Retrieval...")
            self.dual_retriever = DualRetriever(
                self.neo4j_uri, self.neo4j_user, self.neo4j_password,
                self.config.dual_retrieval_config
            )
            
            # Module 2: Triple Filter
            logger.info("   ü§ñ Kh·ªüi t·∫°o Module 2 - LLM Triple Filter...")
            self.triple_filter = LLMTripleFilter(self.config.triple_filter_config)
            
            # Module 3: Passage Ranker
            logger.info("   üèÜ Kh·ªüi t·∫°o Module 3 - Passage Ranker...")
            self.passage_ranker = PassageRanker(self.config.passage_ranking_config)
            
            # Module 4: Context Expander (n·∫øu enabled)
            if self.config.enable_context_expansion:
                logger.info("   üîó Kh·ªüi t·∫°o Module 4 - Context Expander...")
                self.context_expander = ContextExpander(self.config.context_expansion_config)
            
            logger.info("‚úÖ T·∫•t c·∫£ modules ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói kh·ªüi t·∫°o modules: {e}")
            self.errors.append({
                'stage': 'module_initialization',
                'error': str(e),
                'timestamp': time.time()
            })
            raise
    
    def process_single_query(self, query: str, query_id: Optional[str] = None) -> RetrievalPipelineResult:
        """
        X·ª≠ l√Ω m·ªôt query duy nh·∫•t qua to√†n b·ªô pipeline
        
        Args:
            query (str): Query c·ªßa user
            query_id (Optional[str]): ID c·ªßa query (auto-generate n·∫øu None)
            
        Returns:
            RetrievalPipelineResult: K·∫øt qu·∫£ ho√†n ch·ªânh
        """
        if query_id is None:
            query_id = f"Q_{int(time.time() * 1000)}"
        
        logger.info("=" * 80)
        logger.info("üöÄ B·∫ÆT ƒê·∫¶U RETRIEVAL PIPELINE")
        logger.info("=" * 80)
        logger.info(f"üìù Query ID: {query_id}")
        logger.info(f"üìù Query: '{query}'")
        
        start_time = time.time()
        
        # Validate query
        if not validate_query(query):
            error_msg = f"Invalid query: {query}"
            logger.error(f"‚ùå {error_msg}")
            return self._create_error_result(query_id, query, error_msg)
        
        try:
            # Module 1: Dual Retrieval
            logger.info(f"\nüìä MODULE 1: DUAL RETRIEVAL")
            logger.info("-" * 60)
            module1_start = time.time()
            
            retrieval_result = self.dual_retriever.retrieve_dual(
                query,
                top_k_passages=self.config.dual_retrieval_config.max_passages,
                top_n_triples=10  # Retrieve nhi·ªÅu ƒë·ªÉ filter
            )
            
            module1_time = time.time() - module1_start
            self.module_times['dual_retrieval'] = module1_time
            logger.info(f"‚úÖ Module 1 ho√†n th√†nh trong {module1_time:.2f}s")
            
            # Module 2: Triple Filtering
            logger.info(f"\nü§ñ MODULE 2: LLM TRIPLE FILTERING")
            logger.info("-" * 60)
            module2_start = time.time()
            
            filtering_result = self.triple_filter.filter_triples(
                query, retrieval_result.raw_triples
            )
            
            module2_time = time.time() - module2_start
            self.module_times['triple_filtering'] = module2_time
            logger.info(f"‚úÖ Module 2 ho√†n th√†nh trong {module2_time:.2f}s")
            
            # Module 3: Passage Ranking
            logger.info(f"\nüèÜ MODULE 3: PASSAGE RANKING")
            logger.info("-" * 60)
            module3_start = time.time()
            
            ranking_result = self.passage_ranker.rank_passages(
                retrieval_result.raw_passages,
                filtering_result.filtered_triples,
                query
            )
            
            module3_time = time.time() - module3_start
            self.module_times['passage_ranking'] = module3_time
            logger.info(f"‚úÖ Module 3 ho√†n th√†nh trong {module3_time:.2f}s")
            
            # Module 4: Context Expansion (Optional)
            expansion_result = None
            if self.config.enable_context_expansion:
                logger.info(f"\nüîó MODULE 4: CONTEXT EXPANSION")
                logger.info("-" * 60)
                module4_start = time.time()
                
                expansion_result = self.context_expander.expand_context(
                    filtering_result.filtered_triples, query
                )
                
                module4_time = time.time() - module4_start
                self.module_times['context_expansion'] = module4_time
                logger.info(f"‚úÖ Module 4 ho√†n th√†nh trong {module4_time:.2f}s")
            
            # Generate final result
            total_time = time.time() - start_time
            
            final_result = self._create_final_result(
                query_id, query, total_time,
                retrieval_result, filtering_result, ranking_result, expansion_result
            )
            
            # Log final summary
            self._log_pipeline_summary(final_result)
            
            return final_result
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói trong pipeline: {e}")
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            
            error_info = {
                'stage': 'pipeline_execution',
                'error': str(e),
                'traceback': traceback.format_exc(),
                'timestamp': time.time()
            }
            self.errors.append(error_info)
            
            return self._create_error_result(query_id, query, str(e))
    
    def process_multiple_queries(self, queries: List[Dict[str, str]]) -> List[RetrievalPipelineResult]:
        """
        X·ª≠ l√Ω multiple queries cho evaluation
        
        Args:
            queries (List[Dict]): List of {"query_id": "Q001", "query": "text"}
            
        Returns:
            List[RetrievalPipelineResult]: Danh s√°ch k·∫øt qu·∫£
        """
        logger.info(f"üîÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(queries)} queries...")
        
        results = []
        for i, query_data in enumerate(queries, 1):
            query_id = query_data.get('query_id', f'Q_{i:03d}')
            query_text = query_data.get('query', '')
            
            logger.info(f"\nüìä PROCESSING QUERY {i}/{len(queries)}: {query_id}")
            
            try:
                result = self.process_single_query(query_text, query_id)
                results.append(result)
                
                logger.info(f"‚úÖ Query {query_id} ho√†n th√†nh")
                
                # Save intermediate n·∫øu enabled
                if self.config.enable_intermediate_saving:
                    output_path = Path(f"outputs/intermediate/query_{query_id}_result.json")
                    result.save_to_file(output_path)
                
            except Exception as e:
                logger.error(f"‚ùå L·ªói x·ª≠ l√Ω query {query_id}: {e}")
                error_result = self._create_error_result(query_id, query_text, str(e))
                results.append(error_result)
        
        logger.info(f"üéâ Ho√†n th√†nh x·ª≠ l√Ω {len(results)} queries")
        return results
    
    def _create_final_result(self, query_id: str, query: str, total_time: float,
                           retrieval_result: RetrievalResult,
                           filtering_result: FilteringResult,
                           ranking_result: RankingResult,
                           expansion_result: Optional[ExpansionResult]) -> RetrievalPipelineResult:
        """T·∫°o final result object"""
        
        # Format top passages cho evaluation
        top_passages = self._format_passages_for_evaluation(ranking_result.ranked_passages)
        
        # Create statistics
        statistics = self._create_pipeline_statistics(
            retrieval_result, filtering_result, ranking_result, expansion_result, total_time
        )
        
        return RetrievalPipelineResult(
            query_id=query_id,
            query=query,
            top_passages=top_passages,
            method="proposed_hipporag",
            processing_time=total_time,
            raw_retrieval_result=retrieval_result if self.config.include_intermediate_results else None,
            filtering_result=filtering_result if self.config.include_intermediate_results else None,
            ranking_result=ranking_result if self.config.include_intermediate_results else None,
            expansion_result=expansion_result if self.config.include_intermediate_results else None,
            statistics=statistics,
            config_used=self.config,
            errors=self.errors.copy()
        )
    
    def _format_passages_for_evaluation(self, ranked_passages) -> List[Dict[str, Any]]:
        """
        Format passages th√†nh format chu·∫©n cho evaluation
        
        Args:
            ranked_passages: List of RankedPassage objects
            
        Returns:
            List[Dict]: Standardized format cho so s√°nh v·ªõi baseline
        """
        formatted = []
        
        for passage in ranked_passages[:self.config.max_final_passages]:
            passage_dict = {
                "passage_id": passage.passage_id,
                "rank": passage.rank,
                "score": passage.final_score,
                "text": passage.original_text
            }
            
            # Add metadata n·∫øu enabled
            if self.config.include_metadata:
                passage_dict["metadata"] = {
                    "hybrid_retrieval_score": passage.hybrid_retrieval_score,
                    "support_score": passage.support_score,
                    "supporting_triples_count": passage.supporting_triples_count,
                    "score_breakdown": passage.score_breakdown
                }
            
            formatted.append(passage_dict)
        
        return formatted
    
    def _create_pipeline_statistics(self, retrieval_result, filtering_result, 
                                  ranking_result, expansion_result, total_time) -> Dict[str, Any]:
        """T·∫°o comprehensive statistics"""
        
        stats = {
            'pipeline_overview': {
                'total_time_seconds': total_time,
                'modules_executed': len(self.module_times),
                'context_expansion_enabled': self.config.enable_context_expansion
            },
            'module_times': self.module_times.copy(),
            'data_flow': {
                'raw_passages': len(retrieval_result.raw_passages) if retrieval_result else 0,
                'raw_triples': len(retrieval_result.raw_triples) if retrieval_result else 0,
                'filtered_triples': len(filtering_result.filtered_triples) if filtering_result else 0,
                'final_passages': len(ranking_result.ranked_passages) if ranking_result else 0,
                'expanded_contexts': len(expansion_result.expanded_contexts) if expansion_result else 0
            },
            'efficiency_metrics': {
                'triple_filtering_rate': (
                    len(filtering_result.filtered_triples) / len(retrieval_result.raw_triples)
                    if filtering_result and retrieval_result and len(retrieval_result.raw_triples) > 0
                    else 0
                ),
                'passage_reranking_impact': (
                    len(ranking_result.ranked_passages) / len(retrieval_result.raw_passages)
                    if ranking_result and retrieval_result and len(retrieval_result.raw_passages) > 0
                    else 0
                )
            },
            'errors_count': len(self.errors)
        }
        
        return stats
    
    def _create_error_result(self, query_id: str, query: str, error_msg: str) -> RetrievalPipelineResult:
        """T·∫°o error result khi c√≥ l·ªói"""
        return RetrievalPipelineResult(
            query_id=query_id,
            query=query,
            top_passages=[],
            method="proposed_hipporag",
            processing_time=0.0,
            statistics={'error': error_msg},
            errors=[{'error': error_msg, 'timestamp': time.time()}]
        )
    
    def _log_pipeline_summary(self, result: RetrievalPipelineResult):
        """Log t·ªïng k·∫øt pipeline execution"""
        logger.info("=" * 80)
        logger.info("üéâ RETRIEVAL PIPELINE HO√ÄN TH√ÄNH")
        logger.info("=" * 80)
        logger.info(f"üìù Query: '{result.query}'")
        logger.info(f"‚è±Ô∏è T·ªïng th·ªùi gian: {result.processing_time:.2f}s")
        logger.info(f"üìä Top passages: {len(result.top_passages)}")
        
        # Module times breakdown
        logger.info(f"\nüìà TH·ªúI GIAN T·ª™NG MODULE:")
        for module, time_taken in self.module_times.items():
            percentage = (time_taken / result.processing_time * 100) if result.processing_time > 0 else 0
            logger.info(f"   {module}: {time_taken:.2f}s ({percentage:.1f}%)")
        
        # Top passages preview
        if result.top_passages:
            logger.info(f"\nüèÜ TOP 3 PASSAGES:")
            for i, passage in enumerate(result.top_passages[:3], 1):
                logger.info(f"   {i}. {passage['passage_id']}: {passage['score']:.3f}")
                logger.info(f"      üìù {passage['text'][:100]}...")
        
        # Data flow
        if result.statistics and 'data_flow' in result.statistics:
            flow = result.statistics['data_flow']
            logger.info(f"\nüìä DATA FLOW:")
            logger.info(f"   Raw passages: {flow['raw_passages']}")
            logger.info(f"   Raw triples: {flow['raw_triples']}")
            logger.info(f"   Filtered triples: {flow['filtered_triples']}")
            logger.info(f"   Final passages: {flow['final_passages']}")
            if flow['expanded_contexts'] > 0:
                logger.info(f"   Expanded contexts: {flow['expanded_contexts']}")
        
        logger.info("=" * 80)
    
    def close(self):
        """ƒê√≥ng t·∫•t c·∫£ connections v√† cleanup"""
        logger.info("üîê ƒêang ƒë√≥ng Pipeline Orchestrator...")
        
        if self.dual_retriever:
            self.dual_retriever.close()
        if self.context_expander:
            self.context_expander.close()
        
        logger.info("‚úÖ Pipeline Orchestrator ƒë√£ ƒë∆∞·ª£c ƒë√≥ng")

# ==================== UTILITY FUNCTIONS ====================

def create_default_pipeline_config() -> RetrievalPipelineConfig:
    """T·∫°o c·∫•u h√¨nh m·∫∑c ƒë·ªãnh cho pipeline"""
    return RetrievalPipelineConfig(
        enable_context_expansion=False,  # T·∫Øt context expansion ƒë·ªÉ focus v√†o retrieval
        max_final_passages=10,
        enable_intermediate_saving=False,
        output_format="evaluation",
        include_metadata=True,
        include_intermediate_results=False,
        enable_performance_tracking=True
    )

def load_queries_from_file(filepath: Path) -> List[Dict[str, str]]:
    """
    Load queries t·ª´ file JSON
    
    Args:
        filepath (Path): ƒê∆∞·ªùng d·∫´n file ch·ª©a queries
        
    Returns:
        List[Dict]: List of {"query_id": "Q001", "query": "text"}
    """
    if not filepath.exists():
        raise FileNotFoundError(f"Query file not found: {filepath}")
    
    data = load_json(filepath)
    
    # Support multiple formats
    if isinstance(data, list):
        # Format: [{"query_id": "Q001", "query": "text"}, ...]
        return data
    elif isinstance(data, dict):
        if 'queries' in data:
            # Format: {"queries": [...]}
            return data['queries']
        else:
            # Format: {"Q001": "text", "Q002": "text", ...}
            return [{"query_id": qid, "query": text} for qid, text in data.items()]
    
    raise ValueError("Invalid query file format")

def save_evaluation_results(results: List[RetrievalPipelineResult], filepath: Path):
    """
    L∆∞u results theo format evaluation chu·∫©n
    
    Args:
        results (List[RetrievalPipelineResult]): Pipeline results
        filepath (Path): Output file path
    """
    evaluation_data = {
        "method": "proposed_hipporag",
        "total_queries": len(results),
        "timestamp": datetime.now().isoformat(),
        "results": [result.get_evaluation_format() for result in results]
    }
    
    save_json(evaluation_data, filepath, indent=2)
    logger.info(f"üíæ ƒê√£ l∆∞u evaluation results v√†o: {filepath}")
    logger.info(f"   üìä Total queries: {len(results)}")

def save_results_to_excel(results: List[RetrievalPipelineResult], 
                         filepath: Path,
                         include_charts: bool = True) -> None:
    """
    Save pipeline results th√†nh Excel file v·ªõi multiple sheets v√† formatting
    
    Args:
        results (List[RetrievalPipelineResult]): Pipeline results
        filepath (Path): Excel output file path
        include_charts (bool): C√≥ t·∫°o charts kh√¥ng
    """
    logger.info(f"üìä Saving {len(results)} results to Excel: {filepath}")
    
    # Ensure output directory exists
    filepath.parent.mkdir(parents=True, exist_ok=True)
    
    with pd.ExcelWriter(filepath, engine='openpyxl') as writer:
        # Sheet 1: Summary
        logger.info("   üìã Creating Summary sheet...")
        summary_data = [result.to_excel_summary_row() for result in results]
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_excel(writer, sheet_name='Summary', index=False)
        
        # Sheet 2: Detailed Results
        logger.info("   üìÑ Creating Detailed Results sheet...")
        detailed_data = []
        for result in results:
            detailed_data.extend(result.to_excel_detailed_rows())
        
        if detailed_data:
            detailed_df = pd.DataFrame(detailed_data)
            detailed_df.to_excel(writer, sheet_name='Detailed Results', index=False)
        
        # Sheet 3: Performance Statistics
        logger.info("   üìà Creating Performance Stats sheet...")
        perf_stats = _calculate_performance_statistics(results)
        perf_df = pd.DataFrame(list(perf_stats.items()), columns=['Metric', 'Value'])
        perf_df.to_excel(writer, sheet_name='Performance Stats', index=False)
        
        # Sheet 4: Module Breakdown
        logger.info("   üîß Creating Module Breakdown sheet...")
        module_data = _create_module_breakdown_data(results)
        if module_data:
            module_df = pd.DataFrame(module_data)
            module_df.to_excel(writer, sheet_name='Module Breakdown', index=False)
        
        # Sheet 5: Score Distribution
        logger.info("   üìä Creating Score Distribution sheet...")
        score_data = _create_score_distribution_data(results)
        if score_data:
            score_df = pd.DataFrame(score_data)
            score_df.to_excel(writer, sheet_name='Score Distribution', index=False)
    
    # Apply formatting
    _format_excel_workbook(filepath, include_charts)
    
    logger.info(f"‚úÖ Excel file saved successfully: {filepath}")
    logger.info(f"   üìä Sheets: Summary, Detailed Results, Performance Stats, Module Breakdown, Score Distribution")

def _calculate_performance_statistics(results: List[RetrievalPipelineResult]) -> Dict[str, Any]:
    """Calculate comprehensive performance statistics"""
    if not results:
        return {}
    
    # Basic stats
    total_queries = len(results)
    processing_times = [r.processing_time for r in results]
    passage_counts = [len(r.top_passages) for r in results]
    
    # Extract module times
    module_times = {}
    for result in results:
        if result.statistics and 'module_times' in result.statistics:
            for module, time_val in result.statistics['module_times'].items():
                if module not in module_times:
                    module_times[module] = []
                module_times[module].append(time_val)
    
    # Calculate stats
    stats = {
        'Total Queries': total_queries,
        'Successful Queries': sum(1 for r in results if len(r.top_passages) > 0),
        'Failed Queries': sum(1 for r in results if len(r.top_passages) == 0),
        'Success Rate (%)': round((sum(1 for r in results if len(r.top_passages) > 0) / total_queries * 100), 2),
        
        'Avg Processing Time (s)': round(sum(processing_times) / len(processing_times), 2),
        'Min Processing Time (s)': round(min(processing_times), 2),
        'Max Processing Time (s)': round(max(processing_times), 2),
        'Total Processing Time (s)': round(sum(processing_times), 2),
        
        'Avg Passages per Query': round(sum(passage_counts) / len(passage_counts), 1),
        'Min Passages per Query': min(passage_counts),
        'Max Passages per Query': max(passage_counts),
        'Total Passages Retrieved': sum(passage_counts)
    }
    
    # Add module-specific stats
    for module, times in module_times.items():
        if times:
            stats[f'Avg {module.title()} Time (s)'] = round(sum(times) / len(times), 2)
            stats[f'{module.title()} % of Total'] = round((sum(times) / sum(processing_times) * 100), 1)
    
    return stats

def _create_module_breakdown_data(results: List[RetrievalPipelineResult]) -> List[Dict[str, Any]]:
    """Create data cho module breakdown analysis"""
    module_data = []
    
    for result in results:
        if result.statistics and 'module_times' in result.statistics:
            module_times = result.statistics['module_times']
            
            row = {
                'Query ID': result.query_id,
                'Total Time': round(result.processing_time, 2)
            }
            
            # Add individual module times
            for module, time_val in module_times.items():
                row[f'{module.title()} Time'] = round(time_val, 2)
                row[f'{module.title()} %'] = round((time_val / result.processing_time * 100), 1) if result.processing_time > 0 else 0
            
            module_data.append(row)
    
    return module_data

def _create_score_distribution_data(results: List[RetrievalPipelineResult]) -> List[Dict[str, Any]]:
    """Create data cho score distribution analysis"""
    score_data = []
    
    for result in results:
        for passage in result.top_passages:
            metadata = passage.get('metadata', {})
            
            score_data.append({
                'Query ID': result.query_id,
                'Passage ID': passage.get('passage_id', ''),
                'Rank': passage.get('rank', 0),
                'Final Score': passage.get('score', 0),
                'Retrieval Score': metadata.get('hybrid_retrieval_score', 0),
                'Support Score': metadata.get('support_score', 0),
                'Supporting Triples Count': metadata.get('supporting_triples_count', 0),
                'Score Difference': passage.get('score', 0) - metadata.get('hybrid_retrieval_score', 0)
            })
    
    return score_data

def _format_excel_workbook(filepath: Path, include_charts: bool = True):
    """Apply formatting to Excel workbook"""
    try:
        from openpyxl import load_workbook
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
        from openpyxl.formatting.rule import ColorScaleRule
        
        wb = load_workbook(filepath)
        
        # Header formatting
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
        center_alignment = Alignment(horizontal="center", vertical="center")
        
        # Format each sheet
        for sheet_name in wb.sheetnames:
            ws = wb[sheet_name]
            
            # Header row formatting
            for cell in ws[1]:
                cell.font = header_font
                cell.fill = header_fill
                cell.alignment = center_alignment
            
            # Auto-adjust column widths
            for column in ws.columns:
                max_length = 0
                column_letter = column[0].column_letter
                
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                
                adjusted_width = min(max_length + 2, 50)  # Max width 50
                ws.column_dimensions[column_letter].width = adjusted_width
            
            # Add color scales for score columns
            if sheet_name == "Detailed Results":
                # Find score columns
                score_columns = []
                for col_idx, cell in enumerate(ws[1], 1):
                    if cell.value and 'score' in str(cell.value).lower():
                        score_columns.append(col_idx)
                
                # Apply color scale to score columns
                for col_idx in score_columns:
                    col_letter = ws.cell(row=1, column=col_idx).column_letter
                    color_rule = ColorScaleRule(
                        start_type='min', start_color='F8696B',
                        mid_type='percentile', mid_value=50, mid_color='FFEB9C',
                        end_type='max', end_color='63BE7B'
                    )
                    ws.conditional_formatting.add(f'{col_letter}2:{col_letter}{ws.max_row}', color_rule)
        
        wb.save(filepath)
        logger.info("   ‚úÖ Excel formatting applied successfully")
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not apply Excel formatting: {e}")

# Add method to RetrievalPipelineOrchestrator class
def save_results_to_excel(self, results: List[RetrievalPipelineResult], 
                         filepath: Path, include_charts: bool = True):
    """
    Save pipeline results to formatted Excel file
    
    Args:
        results (List[RetrievalPipelineResult]): Results to save
        filepath (Path): Excel output path
        include_charts (bool): Include charts and advanced formatting
    """
    save_results_to_excel(results, filepath, include_charts)

# ==================== TEST FUNCTIONS ====================

def test_pipeline_orchestrator():
    """Test Pipeline Orchestrator v·ªõi sample query"""
    print("üß™ TESTING PIPELINE ORCHESTRATOR")
    print("=" * 50)
    
    try:
        # Create config
        config = create_default_pipeline_config()
        config.max_final_passages = 5  # Reduce cho test
        
        # Initialize orchestrator
        orchestrator = RetrievalPipelineOrchestrator(config)
        
        # Test single query
        test_query = "L·ª£i √≠ch c·ªßa t√°o cho s·ª©c kh·ªèe"
        result = orchestrator.process_single_query(test_query, "TEST_001")
        
        print(f"‚úÖ Test successful!")
        print(f"   Query: {result.query}")
        print(f"   Passages found: {len(result.top_passages)}")
        print(f"   Processing time: {result.processing_time:.2f}s")
        print(f"   Method: {result.method}")
        
        # Show top passages
        print(f"\nüèÜ Top passages:")
        for passage in result.top_passages[:3]:
            print(f"   - {passage['passage_id']}: {passage['score']:.3f}")
            print(f"     {passage['text'][:80]}...")
        
        # Test evaluation format
        eval_format = result.get_evaluation_format()
        print(f"\nüìä Evaluation format:")
        print(f"   Query ID: {eval_format['query_id']}")
        print(f"   Method: {eval_format['method']}")
        print(f"   Passages count: {len(eval_format['top_passages'])}")
        
        # Close orchestrator
        orchestrator.close()
        
        return result
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        logger.exception("Test error:")
        return None

if __name__ == "__main__":
    test_pipeline_orchestrator()
