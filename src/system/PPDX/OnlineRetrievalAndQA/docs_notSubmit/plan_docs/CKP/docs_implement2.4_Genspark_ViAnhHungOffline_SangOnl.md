# docs_implement2.4_Genspark_ViAnhHungOffline_SangOnl.md: Ko cÃ²n nhiá»u time vÃ  cÃ¡c láº§n thá»­ Cursor, Manus tháº¥t báº¡i quÃ¡ -> mÃ¬nh Ä‘Ã£ cháº¡y Claude Opus rá»“i nhÆ°ng táº¡m bá» -> qua genspark trÆ°á»›c vÃ¬ tá»‰ lá»‡ thÃ nh cÃ´ng á»Ÿ nÃ³ siÃªu cao. CHECK XEM NHá»š RÃ• PHA ONLINE VÃ€ OFFLINE KO? ----- QUÃŠN Máº¤T THÃ€NH CÃ”NG Cá»¦A GENSPARK LÃ€ Tá»ª PDF REPORT RA LUÃ”N THáº²NG Tá»ªNG FILE CODE Má»–I FILE 400-500 DÃ’NG, Háº¾T Láº I CONTINUE --- Äi Ä‘Æ°á»ng vÃ²ng nÃ o lÃ  genspark vÃ  cursor ra CLASS, DEF, DOCS --- xong láº¡i code tiáº¿p báº±ng cursor, manus, claude, ... VáºªN BUG --- QUAY Láº I Vá»šI MANUS 1 PHÃT THáº²NG RA CODE LUÃ”N KO Cáº¦N QUA CLASS, DEF, DOCS TRÆ¯á»šC. 

# 2. ğŸ—ï¸ **Nhá»› rÃµ pha Offline! ÄÃ¢y lÃ  luá»“ng 4 modules:**

## ğŸ¯ **TÃ³m táº¯t Offline Phase (4 Modules):**

### **ğŸ“‹ Workflow Overview:**
```
Documents â†’ Module 1 â†’ Module 2 â†’ Module 3 â†’ Module 4 â†’ Knowledge Graph
```

---

## ğŸ”„ **Chi tiáº¿t tá»«ng bÆ°á»›c:**

### **ğŸ“„ Module 1: Document Chunking**
**ğŸ¯ Má»¥c Ä‘Ã­ch:** Chia documents thÃ nh passages cÃ³ nghÄ©a
- **Input:** Raw documents (Excel files)
- **Process:** Simple chunking - giá»¯ nguyÃªn paragraphs tá»« Excel
- **Output:** Document chunks vá»›i metadata
- **Implementation:** Keep as paragraph (1 document = 1 chunk)

### **ğŸ§  Module 2: Triple Extraction (OpenIE)**
**ğŸ¯ Má»¥c Ä‘Ã­ch:** Extract structured knowledge tá»« text
- **Input:** Document chunks
- **Process:** 
  - **Primary:** Qwen2.5-7B extract triples `(subject, predicate, object)`
  - **Backup:** GPT-3.5-Turbo khi Qwen fail
- **Output:** Extracted triples vá»›i metadata
- **Schema:** Schema-less open KG (khÃ´ng giá»›i háº¡n relation types)

### **ğŸ”— Module 3: Synonym Detection**
**ğŸ¯ Má»¥c Ä‘Ã­ch:** TÃ¬m cÃ¡c phrases Ä‘á»“ng nghÄ©a
- **Input:** Extracted triples
- **Process:**
  - Extract unique phrases tá»« subjects/objects
  - Generate embeddings (paraphrase-multilingual-mpnet-base-v2)
  - Calculate similarity (cosine similarity)
  - Threshold = 0.8 (HippoRAG 2 style)
- **Output:** Synonym pairs vá»›i similarity scores

### **ğŸ—ï¸ Module 4: Graph Builder (KG Integration)**
**ğŸ¯ Má»¥c Ä‘Ã­ch:** XÃ¢y dá»±ng complete Knowledge Graph
- **Input:** Chunks + Triples + Synonym pairs
- **Process:**
  - Create **Passage nodes** (documents)
  - Create **Phrase nodes** (entities/concepts) 
  - Create **RELATION edges** (subjectâ†’object)
  - Create **SYNONYM edges** (bidirectional)
  - Create **CONTAINS edges** (passageâ†’phrases)
- **Output:** Complete KG trong Neo4j

---

## ğŸ—ï¸ **HippoRAG 2 Style Implementation:**

### **ğŸ”‘ Key Design Decisions:**

#### **âœ… NO Canonical Mapping:**
- **Preserve all surface forms** - khÃ´ng merge synonyms thÃ nh 1 node
- **Individual nodes** cho má»—i phrase variant
- **SYNONYM edges** connect similar phrases

#### **âœ… Synonym Edges:**
- **Threshold 0.8** (HippoRAG 2 standard)
- **Bidirectional edges** vá»›i similarity scores
- **Enable semantic connectivity** giá»¯a phrase variants

#### **âœ… Robust Extraction:**
- **Primary:** Qwen2.5-7B-Instruct
- **Backup:** GPT-3.5-Turbo automatic fallback
- **Zero failure rate** vá»›i backup system

---

## ğŸ“Š **Graph Architecture:**

### **ğŸ”· Nodes:**
- **Passage Nodes:** Represent document chunks
- **Phrase Nodes:** Represent entities/concepts (ALL surface forms preserved)

### **ğŸ”— Edges:**
- **RELATION:** `(subject) -[predicate]-> (object)`
- **SYNONYM:** `(phrase1) <-[similarity]-> (phrase2)`
- **CONTAINS:** `(passage) -[contains]-> (phrase)`

### **ğŸ“ˆ Example Graph:**
```
Passage_1 -[CONTAINS]-> "tÃ¡o"
Passage_1 -[CONTAINS]-> "apple" 
"tÃ¡o" -[SYNONYM]-> "apple" (similarity: 0.96)
"tÃ¡o" -[RELATION]-> "cháº¥t xÆ¡" (predicate: "chá»©a")
```

---

## ğŸ“‚ **Architecture Mapping:**

| Module | File | Key Function | Output |
|--------|------|--------------|---------|
| **1** | `module1_chunking.py` | `process_documents()` | Document chunks |
| **2** | `module2_triple_extractor.py` | `extract_triples()` | Extracted triples |
| **3** | `module3_synonym_detector.py` | `detect_synonyms()` | Synonym pairs |
| **4** | `module4_graph_builder.py` | `build_graph()` | Complete KG |

### **ğŸ¯ Orchestration:**
- **`pipeline_orchestrator.py`** - Coordinates all modules
- **`run_offline_pipeline.py`** - Main entry point

---

## ğŸ§ª **Execution Results (From Previous Run):**

### **ğŸ“Š Performance Stats:**
```
âœ… Pipeline completed successfully in 42.21 seconds
ğŸ“Š Documents: 10 â†’ Chunks: 10 â†’ Triples: 87 â†’ Synonyms: 22
ğŸ—ï¸ Graph: 111 nodes, 234 edges
ğŸ¤– LLM Usage: Qwen 71 triples + GPT 16 triples (backup rescue)
```

### **ğŸ”¢ Graph Statistics:**
- **Passage Nodes:** 10
- **Phrase Nodes:** 101 (all surface forms preserved)
- **RELATION Edges:** 87
- **SYNONYM Edges:** 44 (22 pairs Ã— 2 bidirectional)
- **CONTAINS Edges:** 103

---

## ğŸ¯ **HippoRAG 2 Compliance:**

| Feature | HippoRAG 2 Standard | Our Implementation | Status |
|---------|-------------------|-------------------|--------|
| **Surface Form Preservation** | Required | âœ… All preserved | âœ… |
| **Synonym Edges** | High volume | 44 edges | âœ… |
| **Bidirectional Synonyms** | Required | âœ… Implemented | âœ… |
| **No Node Merging** | Required | âœ… Confirmed | âœ… |
| **Threshold 0.8** | Standard | âœ… Used | âœ… |
| **Semantic Embeddings** | Required | âœ… All nodes | âœ… |

---

## ğŸš€ **Integration vá»›i Online:**

### **ğŸ“‹ Data Flow:**
```
Offline: Excel â†’ KG in Neo4j
Online: Query â†’ KG retrieval â†’ Answer
```

### **ğŸ”— Connection Point:**
- **Neo4j Database** - Bridge giá»¯a Offline vÃ  Online
- **Same connection**: `bolt://localhost:7687`
- **Ready for retrieval** - Graph sáºµn sÃ ng cho Online queries

---

## ğŸ“ **File Structure (Implemented):**

```bash
ğŸ“ OfflineIndexing/
â”œâ”€â”€ ğŸ“„ module1_chunking.py              âœ… DONE
â”œâ”€â”€ ğŸ§  module2_triple_extractor.py      âœ… DONE  
â”œâ”€â”€ ğŸ”— module3_synonym_detector.py      âœ… DONE
â”œâ”€â”€ ğŸ—ï¸ module4_graph_builder.py         âœ… DONE
â”œâ”€â”€ ğŸ¯ pipeline_orchestrator.py         âœ… DONE
â”œâ”€â”€ ğŸš€ run_offline_pipeline.py          âœ… DONE
â””â”€â”€ ğŸ“ utils/ (Neo4j, Excel, General)   âœ… DONE
```

---

## âœ… **Confirmed - Offline Phase Complete!**

**ğŸ‰ HoÃ n toÃ n nhá»› rÃµ Offline Phase! ÄÃ£ Ä‘Æ°á»£c implement vÃ  tested thÃ nh cÃ´ng.**

**ğŸ”— Ready to connect vá»›i Online Phase:**
- KG Ä‘Ã£ sáºµn sÃ ng trong Neo4j
- Architecture HippoRAG 2 compliant
- Robust extraction vá»›i backup system
- All surface forms preserved vá»›i synonym edges

**BÃ¢y giá» implement Online Phase Ä‘á»ƒ complete full system! ğŸš€**



# 3. ğŸŒ **Nhá»› rÃµ pha Online! ÄÃ¢y lÃ  luá»“ng 5 bÆ°á»›c:**

## ğŸ¯ **TÃ³m táº¯t Online Phase (5 Modules):**

### **ğŸ“‹ Workflow Overview:**
```
Query â†’ Module 1 â†’ Module 2 â†’ Module 3 â†’ Module 4 (Optional) â†’ Module 5 â†’ Answer
```

---

## ğŸ”„ **Chi tiáº¿t tá»«ng bÆ°á»›c:**

### **ğŸ” Module 1: Dual/Hybrid Retrieval**
**ğŸ¯ Má»¥c Ä‘Ã­ch:** Táº­n dá»¥ng Hybrid Search
- **Input:** User query
- **Process:** 
  - **Passages:** BM25 + Embedding â†’ Top-K passages (20-50)
  - **Triples:** BM25 + Embedding â†’ Top-N triples (20-50)
- **Output:** Raw Passages + Raw Triples

### **ğŸ¤– Module 2: LLM Triple Filtering**
**ğŸ¯ Má»¥c Ä‘Ã­ch:** Giá»¯ láº¡i triples phÃ¹ há»£p vá»›i query
- **Input:** Raw Triples + Original Query
- **Process:** Qwen2.5-7B (+ GPT-3.5 backup) Ä‘Ã¡nh giÃ¡ vÃ  chá»n facts cá»‘t lÃµi
- **Output:** Filtered Triples (Top-M, ~10-20 triples cháº¥t lÆ°á»£ng cao)

### **ğŸ“Š Module 3: Passage Ranking based on Triples**
**ğŸ¯ Má»¥c Ä‘Ã­ch:** Lá»c passages nhiá»…u báº±ng cÃ¡ch tÃ­nh support score
- **Input:** Raw Passages + Filtered Triples
- **Process:**
  - TÃ­nh **Support Score:** Ä‘áº¿m triples mÃ  passage há»— trá»£
  - **Final Score = Î± Ã— hybrid_retrieval_score + (1-Î±) Ã— Support_Score**
  - Chá»n Top-P passages (5-10 passages tá»‘t nháº¥t)
- **Output:** Ranked Passages

### **ğŸ¯ Module 4: Context Expansion (Optional)**
**ğŸ¯ Má»¥c Ä‘Ã­ch:** Má»Ÿ rá»™ng 1-hop cho Filtered Triples
- **Input:** Filtered Triples
- **Process:** 1-hop traversal trÃªn Knowledge Graph (khÃ´ng lan rá»™ng nhÆ° PPR)
- **Output:** Expanded Context

### **ğŸ—£ï¸ Module 5: Answer Generation**
**ğŸ¯ Má»¥c Ä‘Ã­ch:** Táº¡o cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng
- **Input:** Query + Ranked Passages + Filtered Triples + Expanded Context
- **Process:**
  - Format thÃ nh structured prompt
  - Qwen2.5-7B (+ GPT-3.5 backup) generate answer
- **Output:** Final Answer vá»›i citations

---

## ğŸ§  **Core Innovation:**

### **ğŸ”‘ Key Insight:**
**Sá»­ dá»¥ng Filtered Triples lÃ m "cáº§u ná»‘i" Ä‘á»ƒ cáº£i thiá»‡n Passage Ranking**

### **ğŸ’¡ Workflow Logic:**
1. **Retrieve** cáº£ passages vÃ  triples
2. **Filter triples** Ä‘á»ƒ cÃ³ core facts 
3. **Use filtered triples** Ä‘á»ƒ re-rank passages
4. **Expand context** náº¿u cáº§n (1-hop)
5. **Generate answer** tá»« táº¥t cáº£ context

### **âš¡ Advantages:**
- **Bá»• sung thÃ´ng tin:** Passages (context) + Triples (facts)
- **Loáº¡i bá» nhiá»…u:** Filtered Triples â†’ Better Passage Selection
- **Má»Ÿ rá»™ng ngá»¯ cáº£nh:** 1-hop expansion cho more info
- **Robust LLM:** Qwen primary + GPT backup = zero failures

---

## ğŸ“Š **Architecture Mapping:**

| Module | File | Key Function | Output |
|--------|------|--------------|---------|
| **1** | `module1_dual_retrieval.py` | `retrieve_dual()` | Raw Passages + Triples |
| **2** | `module2_triple_filter.py` | `filter_triples()` | Filtered Triples |
| **3** | `module3_passage_ranker.py` | `rank_passages()` | Ranked Passages |
| **4** | `module4_context_expander.py` | `expand_context()` | Expanded Context |
| **5** | `module5_answer_generator.py` | `generate_answer()` | Final Answer |

---

## ğŸš€ **Execution Modes:**

### **ğŸ” Retrieval-Only (`run_retrieval_pipeline.py`):**
```bash
python run_retrieval_pipeline.py --query "..." --enable_expansion True/False
# Output: Retrieved + Filtered + Ranked content (no answer)
```

### **ğŸŒ Full Pipeline (`run_retrieval_and_qa_pipeline.py`):**
```bash
python run_retrieval_and_qa_pipeline.py --query "..." --enable_expansion True/False  
# Output: Complete answer vá»›i all intermediate results
```

---

## ğŸ¯ **So vá»›i HippoRAG 2:**

| Aspect | HippoRAG 2 | Our Approach |
|--------|------------|--------------|
| **Graph Usage** | PPR (complex) | 1-hop expansion (simple) |
| **Triple Role** | Node selection | Passage filtering |
| **LLM Integration** | Limited | Robust (primary + backup) |
| **Focus** | Graph algorithms | Hybrid retrieval + LLM |

---

## âœ… **Confirmed - Ready to implement!**

**ğŸ‰ HoÃ n toÃ n nhá»› rÃµ Online Phase! Architecture Ä‘Ã£ Ä‘Æ°á»£c finalize vÃ  sáºµn sÃ ng coding.**

Báº¡n muá»‘n báº¯t Ä‘áº§u implement module nÃ o trÆ°á»›c? ğŸš€


---

# 2.4.1 Code: utils/utils_shared_general.py ğŸš€ Báº¯t Ä‘áº§u code pha Online!

**TÃ´i sáº½ bÃ¡m sÃ¡t style vÃ  patterns tá»« pha Offline Ä‘Ã£ implement.**

## ğŸ¯ **Thá»© tá»± implement:**

1. âœ… **utils/utils_shared_general.py** (foundation)
2. ğŸ” **module1_dual_retrieval.py**
3. ğŸ¤– **module2_triple_filter.py** 
4. ğŸ“Š **module3_passage_ranker.py**
5. ğŸ¯ **module4_context_expander.py**
6. ğŸ—£ï¸ **module5_answer_generator.py**
7. ğŸ¯ **online_pipeline_orchestrator.py**
8. ğŸš€ **run files**

---

## ğŸ“ **File Ä‘áº§u tiÃªn: utils/utils_shared_general.py**

```python
"""
Shared utilities cho OnlineRetrievalAndQA
Minimal utilities thá»±c sá»± Ä‘Æ°á»£c share across modules
"""

from pathlib import Path
from typing import List, Dict, Any, Optional, Union
import logging
import json
import time
import os
from datetime import datetime
import re
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# ==================== TEXT PROCESSING ====================

def normalize_text(text: str) -> str:
    """
    Normalize text cho consistent processing
    
    Args:
        text (str): Input text cáº§n normalize
        
    Returns:
        str: Normalized text
    """
    if not text:
        return ""
    
    # Basic normalization
    text = text.strip()
    text = re.sub(r'\s+', ' ', text)  # Multiple spaces -> single space
    text = re.sub(r'\n+', ' ', text)  # Multiple newlines -> space
    
    return text

def clean_text(text: str) -> str:
    """
    Clean and preprocess text cho retrieval
    
    Args:
        text (str): Input text
        
    Returns:
        str: Cleaned text
    """
    if not text:
        return ""
    
    # Remove extra whitespace
    text = normalize_text(text)
    
    # Remove special characters nhÆ°ng giá»¯ Vietnamese
    text = re.sub(r'[^\w\s\u00C0-\u024F\u1E00-\u1EFF]', ' ', text)
    
    # Remove extra spaces again
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def validate_query(query: str) -> bool:
    """
    Validate user query
    
    Args:
        query (str): User query
        
    Returns:
        bool: True náº¿u query há»£p lá»‡
    """
    if not query or not isinstance(query, str):
        return False
    
    query = query.strip()
    
    # Check minimum length
    if len(query) < 3:
        return False
    
    # Check maximum length
    if len(query) > 1000:
        return False
    
    # Check if not only special characters
    if not re.search(r'[a-zA-Z\u00C0-\u024F\u1E00-\u1EFF]', query):
        return False
    
    return True

def extract_keywords(text: str, max_keywords: int = 10) -> List[str]:
    """
    Extract keywords tá»« text cho BM25
    
    Args:
        text (str): Input text
        max_keywords (int): Maximum keywords to extract
        
    Returns:
        List[str]: Extracted keywords
    """
    if not text:
        return []
    
    # Clean text
    text = clean_text(text.lower())
    
    # Split into words
    words = text.split()
    
    # Filter words (minimum 2 characters)
    keywords = [word for word in words if len(word) >= 2]
    
    # Remove duplicates while preserving order
    seen = set()
    unique_keywords = []
    for word in keywords:
        if word not in seen:
            seen.add(word)
            unique_keywords.append(word)
    
    return unique_keywords[:max_keywords]

# ==================== FILE I/O ====================

def save_json(data: Dict[str, Any], filepath: Path, indent: int = 2):
    """
    Save data to JSON file
    
    Args:
        data (Dict): Data to save
        filepath (Path): File path
        indent (int): JSON indentation
    """
    filepath = Path(filepath)
    filepath.parent.mkdir(parents=True, exist_ok=True)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=indent)
    
    logger.debug(f"Saved JSON to {filepath}")

def load_json(filepath: Path) -> Dict[str, Any]:
    """
    Load data from JSON file
    
    Args:
        filepath (Path): File path
        
    Returns:
        Dict[str, Any]: Loaded data
    """
    filepath = Path(filepath)
    
    if not filepath.exists():
        raise FileNotFoundError(f"File not found: {filepath}")
    
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    logger.debug(f"Loaded JSON from {filepath}")
    return data

def save_text(text: str, filepath: Path):
    """
    Save text to file
    
    Args:
        text (str): Text to save
        filepath (Path): File path
    """
    filepath = Path(filepath)
    filepath.parent.mkdir(parents=True, exist_ok=True)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(text)
    
    logger.debug(f"Saved text to {filepath}")

def load_text(filepath: Path) -> str:
    """
    Load text from file
    
    Args:
        filepath (Path): File path
        
    Returns:
        str: Loaded text
    """
    filepath = Path(filepath)
    
    if not filepath.exists():
        raise FileNotFoundError(f"File not found: {filepath}")
    
    with open(filepath, 'r', encoding='utf-8') as f:
        text = f.read()
    
    logger.debug(f"Loaded text from {filepath}")
    return text

# ==================== LOGGING ====================

def setup_logger(name: str, level: str = "INFO", log_file: Optional[Path] = None) -> logging.Logger:
    """
    Setup logger vá»›i consistent format
    
    Args:
        name (str): Logger name
        level (str): Logging level
        log_file (Optional[Path]): Optional log file
        
    Returns:
        logging.Logger: Configured logger
    """
    logger = logging.getLogger(name)
    logger.setLevel(getattr(logging, level.upper()))
    
    # Avoid duplicate handlers
    if logger.handlers:
        return logger
    
    # Create formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # File handler if specified
    if log_file:
        log_file = Path(log_file)
        log_file.parent.mkdir(parents=True, exist_ok=True)
        
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

def log_performance(func_name: str, start_time: float, end_time: float, 
                   extra_info: Optional[Dict[str, Any]] = None):
    """
    Log performance metrics
    
    Args:
        func_name (str): Function name
        start_time (float): Start timestamp
        end_time (float): End timestamp
        extra_info (Optional[Dict]): Additional information
    """
    duration = end_time - start_time
    
    log_msg = f"{func_name} completed in {duration:.2f} seconds"
    
    if extra_info:
        info_str = ", ".join([f"{k}={v}" for k, v in extra_info.items()])
        log_msg += f" ({info_str})"
    
    logger.info(log_msg)

# ==================== VALIDATION ====================

def validate_pipeline_input(data: Dict[str, Any], required_keys: List[str]) -> bool:
    """
    Validate pipeline input data
    
    Args:
        data (Dict): Input data
        required_keys (List[str]): Required keys
        
    Returns:
        bool: True náº¿u valid
    """
    if not isinstance(data, dict):
        return False
    
    for key in required_keys:
        if key not in data:
            logger.error(f"Missing required key: {key}")
            return False
    
    return True

def validate_module_output(data: Any, module_name: str, expected_type: type = dict) -> bool:
    """
    Validate module output format
    
    Args:
        data (Any): Module output
        module_name (str): Module name for logging
        expected_type (type): Expected data type
        
    Returns:
        bool: True náº¿u valid
    """
    if not isinstance(data, expected_type):
        logger.error(f"{module_name} output has invalid type: {type(data)}, expected: {expected_type}")
        return False
    
    if expected_type == list and len(data) == 0:
        logger.warning(f"{module_name} returned empty list")
    
    return True

def validate_passages(passages: List[Dict[str, Any]]) -> bool:
    """
    Validate passages format
    
    Args:
        passages (List[Dict]): Passages to validate
        
    Returns:
        bool: True náº¿u valid
    """
    if not isinstance(passages, list):
        return False
    
    required_keys = ['passage_id', 'text']
    
    for passage in passages:
        if not isinstance(passage, dict):
            return False
        
        for key in required_keys:
            if key not in passage:
                logger.error(f"Passage missing key: {key}")
                return False
    
    return True

def validate_triples(triples: List[Dict[str, Any]]) -> bool:
    """
    Validate triples format
    
    Args:
        triples (List[Dict]): Triples to validate
        
    Returns:
        bool: True náº¿u valid
    """
    if not isinstance(triples, list):
        return False
    
    required_keys = ['subject', 'predicate', 'object']
    
    for triple in triples:
        if not isinstance(triple, dict):
            return False
        
        for key in required_keys:
            if key not in triple:
                logger.error(f"Triple missing key: {key}")
                return False
    
    return True

# ==================== ENVIRONMENT ====================

def load_environment_variables(env_file: Optional[Path] = None) -> Dict[str, str]:
    """
    Load environment variables tá»« .env file
    
    Args:
        env_file (Optional[Path]): Path to .env file
        
    Returns:
        Dict[str, str]: Environment variables
    """
    env_vars = {}
    
    # Load from system environment
    env_vars.update(dict(os.environ))
    
    # Load from .env file if exists
    if env_file is None:
        env_file = Path('.env')
    
    env_file = Path(env_file)
    if env_file.exists():
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    env_vars[key.strip()] = value.strip().strip('"\'')
    
    return env_vars

def get_api_key(service: str, env_vars: Optional[Dict[str, str]] = None) -> Optional[str]:
    """
    Get API key cho specific service
    
    Args:
        service (str): Service name (huggingface, openai, etc.)
        env_vars (Optional[Dict]): Environment variables
        
    Returns:
        Optional[str]: API key náº¿u found
    """
    if env_vars is None:
        env_vars = load_environment_variables()
    
    # Common API key patterns
    key_patterns = {
        'huggingface': ['HUGGINGFACE_API_KEY', 'HF_API_KEY', 'HUGGINGFACE_TOKEN'],
        'openai': ['OPENAI_API_KEY', 'OPENAI_TOKEN'],
        'anthropic': ['ANTHROPIC_API_KEY', 'ANTHROPIC_TOKEN']
    }
    
    service = service.lower()
    if service in key_patterns:
        for pattern in key_patterns[service]:
            if pattern in env_vars:
                return env_vars[pattern]
    
    # Try direct service name
    service_key = f"{service.upper()}_API_KEY"
    if service_key in env_vars:
        return env_vars[service_key]
    
    logger.warning(f"API key not found for service: {service}")
    return None

# ==================== DATA CLASSES ====================

@dataclass
class PerformanceStats:
    """
    Data class cho performance statistics
    """
    start_time: float
    end_time: float
    duration: float
    operation: str
    success: bool = True
    error_message: Optional[str] = None
    extra_info: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            'start_time': self.start_time,
            'end_time': self.end_time,
            'duration': self.duration,
            'operation': self.operation,
            'success': self.success,
            'error_message': self.error_message,
            'extra_info': self.extra_info or {}
        }

@dataclass
class QueryMetadata:
    """
    Data class cho query metadata
    """
    query: str
    query_id: str
    timestamp: str
    normalized_query: str
    keywords: List[str]
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            'query': self.query,
            'query_id': self.query_id,
            'timestamp': self.timestamp,
            'normalized_query': self.normalized_query,
            'keywords': self.keywords
        }

# ==================== UTILITY FUNCTIONS ====================

def generate_id(prefix: str = "") -> str:
    """
    Generate unique ID vá»›i timestamp
    
    Args:
        prefix (str): Optional prefix
        
    Returns:
        str: Unique ID
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
    if prefix:
        return f"{prefix}_{timestamp}"
    return timestamp

def create_query_metadata(query: str) -> QueryMetadata:
    """
    Create query metadata object
    
    Args:
        query (str): User query
        
    Returns:
        QueryMetadata: Query metadata
    """
    query_id = generate_id("query")
    timestamp = datetime.now().isoformat()
    normalized_query = normalize_text(query)
    keywords = extract_keywords(query)
    
    return QueryMetadata(
        query=query,
        query_id=query_id,
        timestamp=timestamp,
        normalized_query=normalized_query,
        keywords=keywords
    )

def merge_scores(scores: List[float], weights: List[float] = None) -> float:
    """
    Merge multiple scores vá»›i optional weights
    
    Args:
        scores (List[float]): Scores to merge
        weights (List[float]): Optional weights
        
    Returns:
        float: Merged score
    """
    if not scores:
        return 0.0
    
    if weights is None:
        weights = [1.0] * len(scores)
    
    if len(scores) != len(weights):
        raise ValueError("Scores and weights must have same length")
    
    weighted_sum = sum(score * weight for score, weight in zip(scores, weights))
    total_weight = sum(weights)
    
    return weighted_sum / total_weight if total_weight > 0 else 0.0

# ==================== TEST FUNCTIONS ====================

def test_utilities():
    """Test basic utilities"""
    print("ğŸ§ª Testing shared utilities...")
    
    # Test text processing
    test_text = "  ÄÃ¢y lÃ    text\n\n  cÃ³ nhiá»u khoáº£ng tráº¯ng  "
    normalized = normalize_text(test_text)
    print(f"Normalized: '{normalized}'")
    
    # Test query validation
    valid_query = "Lá»£i Ã­ch cá»§a tÃ¡o lÃ  gÃ¬?"
    invalid_query = "x"
    print(f"Valid query: {validate_query(valid_query)}")
    print(f"Invalid query: {validate_query(invalid_query)}")
    
    # Test keywords extraction
    keywords = extract_keywords("Lá»£i Ã­ch cá»§a tÃ¡o Ä‘á»‘i vá»›i sá»©c khá»e con ngÆ°á»i")
    print(f"Keywords: {keywords}")
    
    # Test query metadata
    metadata = create_query_metadata("Test query")
    print(f"Query metadata: {metadata.to_dict()}")
    
    # Test score merging
    merged_score = merge_scores([0.8, 0.6, 0.9], [0.5, 0.3, 0.2])
    print(f"Merged score: {merged_score}")
    
    print("âœ… Utilities test completed!")

if __name__ == "__main__":
    test_utilities()
```

## âœ… **File Ä‘áº§u tiÃªn hoÃ n thÃ nh!**

**Key features:**
- **Text processing** cho Vietnamese + English
- **File I/O** vá»›i proper error handling  
- **Logging** setup consistent vá»›i Offline phase
- **Validation** cho pipeline data
- **Environment** management
- **Performance** tracking utilities
- **Data classes** cho structured data

**ğŸ¯ Style consistency vá»›i Offline:**
- Same logging patterns
- Same error handling approach  
- Same validation philosophy
- Same documentation style

**Tiáº¿p theo: Module 1 - Dual Retrieval! ğŸ”**



```bash
(.venv) PS D:\GIT\ResearchProject_Memory-AugmentedAIAgents_GraduationProject\src\system\PPDX\OnlineRetrievalAndQA> python .\utils\utils_shared_general.py
ğŸ§ª Testing shared utilities...
Normalized: 'ÄÃ¢y lÃ  text cÃ³ nhiá»u khoáº£ng tráº¯ng'
Valid query: True
Invalid query: False
Keywords: ['lá»£i', 'Ã­ch', 'cá»§a', 'tÃ¡o', 'Ä‘á»‘i', 'vá»›i', 'sá»©c', 'khá»e', 'con', 'ngÆ°á»i']
Query metadata: {'query': 'Test query', 'query_id': 'query_20250531_033111_862', 'timestamp': '2025-05-31T03:31:11.863474', 'normalized_query': 'Test query', 'keywords': ['test', 'query']}
Merged score: 0.76
âœ… Utilities test completed!
(.venv) PS D:\GIT\ResearchProject_Memory-AugmentedAIAgents_GraduationProject\src\system\PPDX\OnlineRetrievalAndQA>
```


# âœ… **Excellent! Utilities hoáº¡t Ä‘á»™ng perfect!**

## ğŸ¯ **Analysis cá»§a test results:**

### **ğŸ”¥ All functions work correctly:**

1. **âœ… Text Normalization:**
   ```
   Input:  "  ÄÃ¢y lÃ    text\n\n  cÃ³ nhiá»u khoáº£ng tráº¯ng  "
   Output: 'ÄÃ¢y lÃ  text cÃ³ nhiá»u khoáº£ng tráº¯ng'
   ```
   - Perfect cleanup cá»§a extra spaces vÃ  newlines

2. **âœ… Query Validation:**
   ```
   Valid query: True    # "Lá»£i Ã­ch cá»§a tÃ¡o lÃ  gÃ¬?" - há»£p lá»‡
   Invalid query: False # "x" - quÃ¡ ngáº¯n
   ```
   - Logic validation chÃ­nh xÃ¡c

3. **âœ… Keywords Extraction:**
   ```
   Input: "Lá»£i Ã­ch cá»§a tÃ¡o Ä‘á»‘i vá»›i sá»©c khá»e con ngÆ°á»i"
   Output: ['lá»£i', 'Ã­ch', 'cá»§a', 'tÃ¡o', 'Ä‘á»‘i', 'vá»›i', 'sá»©c', 'khá»e', 'con', 'ngÆ°á»i']
   ```
   - Vietnamese text processing hoáº¡t Ä‘á»™ng tá»‘t
   - Properly lowercase vÃ  split

4. **âœ… Query Metadata Generation:**
   ```
   {
     'query': 'Test query',
     'query_id': 'query_20250531_033111_862',  # Unique timestamp-based ID
     'timestamp': '2025-05-31T03:31:11.863474', # ISO format
     'normalized_query': 'Test query',
     'keywords': ['test', 'query']
   }
   ```
   - Perfect metadata structure for tracking

5. **âœ… Score Merging:**
   ```
   Scores: [0.8, 0.6, 0.9]
   Weights: [0.5, 0.3, 0.2]
   Result: 0.76 = (0.8Ã—0.5 + 0.6Ã—0.3 + 0.9Ã—0.2) / (0.5+0.3+0.2)
   ```
   - Weighted average calculation chÃ­nh xÃ¡c

---

## ğŸš€ **Ready for Module 1!**

**Foundation utilities Ä‘Ã£ solid:**
- âœ… Vietnamese text handling
- âœ… Data validation
- âœ… Performance tracking
- âœ… Metadata generation
- âœ… Score calculations

**ğŸ” Tiáº¿p theo: Module 1 - Dual Retrieval**

Báº¯t Ä‘áº§u implement `module1_dual_retrieval.py` vá»›i:
- BM25 + Embedding hybrid search
- Neo4j connection (reuse tá»« Offline)
- Passage vÃ  Triple retrieval
- Score combination

**Sáºµn sÃ ng code Module 1! ğŸš€**