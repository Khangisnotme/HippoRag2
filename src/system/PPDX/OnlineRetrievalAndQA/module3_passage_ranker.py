"""
OnlineRetrievalAndQA/module3_passage_ranker.py
Module 3: Passage Ranking based on Filtered Triples (Xáº¿p háº¡ng Passage dá»±a trÃªn Triple Ä‘Ã£ lá»c)
Sá»­ dá»¥ng filtered triples tá»« Module 2 Ä‘á»ƒ tÃ­nh support score vÃ  re-rank passages tá»« Module 1

MÃ´ táº£ chi tiáº¿t:
    Module nÃ y nháº­n raw passages tá»« Module 1 vÃ  filtered triples tá»« Module 2, 
    sau Ä‘Ã³ tÃ­nh toÃ¡n support score cho má»—i passage dá»±a trÃªn sá»‘ lÆ°á»£ng filtered triples
    mÃ  passage Ä‘Ã³ há»— trá»£, cuá»‘i cÃ¹ng combine vá»›i retrieval score Ä‘á»ƒ táº¡o final ranking.

Kiáº¿n trÃºc chÃ­nh:
    - PassageRankerConfig: Cáº¥u hÃ¬nh ranking parameters
    - SupportScoreCalculator: TÃ­nh toÃ¡n support score dá»±a trÃªn triples
    - PassageRanker: Core ranking logic vá»›i multiple scoring methods
    - RankedPassage: Output data structure vá»›i comprehensive scoring
    - RankingResult: Container cho toÃ n bá»™ káº¿t quáº£ ranking

Workflow:
    Raw Passages + Filtered Triples â†’ Support Score Calculation â†’ Hybrid Ranking â†’ Ranked Passages
"""

from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Tuple, Set
import logging
import time
import json
from dataclasses import dataclass, asdict
from enum import Enum
import math
from collections import Counter, defaultdict
import datetime

# Shared utilities
from utils.utils_shared_general import (
    setup_logger,
    log_performance,
    validate_query,
    clean_text,
    save_json,
    load_json,
    merge_scores,
    PerformanceStats
)

# Module imports
from module1_dual_retrieval import RetrievedItem, RetrievalResult
from module2_triple_filter import FilteredTriple, FilteringResult, RelevanceLevel

# Setup logger with file output
log_file = Path("outputs/log/module3_passage_ranker_{}.log".format(datetime.datetime.now().strftime("%Y%m%d_%H%M%S")))
logger = setup_logger(__name__, log_file=log_file)

# ==================== ENUMS AND CONSTANTS ====================

class RankingStrategy(Enum):
    """
    CÃ¡c chiáº¿n lÆ°á»£c ranking khÃ¡c nhau
    
    Äá»‹nh nghÄ©a cÃ¡ch thá»©c combine retrieval score vá»›i support score:
    - RETRIEVAL_ONLY: Chá»‰ dÃ¹ng retrieval score (baseline)
    - SUPPORT_ONLY: Chá»‰ dÃ¹ng support score tá»« triples  
    - HYBRID_BALANCED: CÃ¢n báº±ng 50-50 giá»¯a retrieval vÃ  support
    - HYBRID_RETRIEVAL_FOCUSED: ThiÃªn vá» retrieval score (70-30)
    - HYBRID_SUPPORT_FOCUSED: ThiÃªn vá» support score (30-70)
    - ADAPTIVE: Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh weights dá»±a trÃªn data quality
    """
    RETRIEVAL_ONLY = "retrieval_only"
    SUPPORT_ONLY = "support_only"
    HYBRID_BALANCED = "hybrid_balanced"
    HYBRID_RETRIEVAL_FOCUSED = "hybrid_retrieval_focused"
    HYBRID_SUPPORT_FOCUSED = "hybrid_support_focused"
    ADAPTIVE = "adaptive"

    def get_weights(self) -> Tuple[float, float]:
        """
        Láº¥y weights cho (hybrid_retrieval_score, support_score)
        
        Returns:
            Tuple[float, float]: (alpha_retrieval, alpha_support)
        """
        weights = {
            RankingStrategy.RETRIEVAL_ONLY: (1.0, 0.0),
            RankingStrategy.SUPPORT_ONLY: (0.0, 1.0),
            RankingStrategy.HYBRID_BALANCED: (0.5, 0.5),
            RankingStrategy.HYBRID_RETRIEVAL_FOCUSED: (0.7, 0.3),
            RankingStrategy.HYBRID_SUPPORT_FOCUSED: (0.3, 0.7),
            RankingStrategy.ADAPTIVE: (0.6, 0.4)  # Default, sáº½ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh
        }
        return weights[self]

class SupportCalculationMethod(Enum):
    """
    CÃ¡c phÆ°Æ¡ng phÃ¡p tÃ­nh support score
    
    Äá»‹nh nghÄ©a cÃ¡ch tÃ­nh support score tá»« filtered triples:
    - COUNT_BASED: Äáº¿m sá»‘ lÆ°á»£ng triples há»— trá»£ passage
    - WEIGHTED_RELEVANCE: TÃ­nh theo trá»ng sá»‘ relevance score cá»§a triples
    - QUALITY_WEIGHTED: TÃ­nh theo quality score (relevance + confidence)
    - LOG_SCALED: Ãp dá»¥ng log scaling Ä‘á»ƒ giáº£m impact cá»§a outliers
    """
    COUNT_BASED = "count_based"
    WEIGHTED_RELEVANCE = "weighted_relevance"
    QUALITY_WEIGHTED = "quality_weighted"
    LOG_SCALED = "log_scaled"

# ==================== DATA CLASSES ====================

@dataclass
class PassageRankerConfig:
    """
    Cáº¥u hÃ¬nh toÃ n diá»‡n cho há»‡ thá»‘ng ranking passages
    
    Attributes:
        ranking_strategy (RankingStrategy): Chiáº¿n lÆ°á»£c ranking chÃ­nh
        support_calculation_method (SupportCalculationMethod): PhÆ°Æ¡ng phÃ¡p tÃ­nh support score
        relevance_threshold (float): NgÆ°á»¡ng relevance Ä‘á»ƒ tÃ­nh support (0.0-1.0)
        max_passages_output (int): Sá»‘ lÆ°á»£ng passages tá»‘i Ä‘a trong output
        enable_score_normalization (bool): CÃ³ normalize scores khÃ´ng
        boost_factor_high_support (float): Há»‡ sá»‘ boost cho passages cÃ³ support cao
        penalty_factor_no_support (float): Há»‡ sá»‘ penalty cho passages khÃ´ng cÃ³ support
        enable_diversity_ranking (bool): CÃ³ Ã¡p dá»¥ng diversity ranking khÃ´ng
        diversity_threshold (float): NgÆ°á»¡ng similarity Ä‘á»ƒ coi lÃ  duplicate (0.0-1.0)
        min_support_threshold (float): Support tá»‘i thiá»ƒu Ä‘á»ƒ khÃ´ng bá»‹ penalty
        adaptive_weight_adjustment (bool): CÃ³ tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh weights khÃ´ng
    """
    # Core ranking parameters
    ranking_strategy: RankingStrategy = RankingStrategy.HYBRID_BALANCED
    support_calculation_method: SupportCalculationMethod = SupportCalculationMethod.WEIGHTED_RELEVANCE
    
    # Filtering and threshold parameters
    relevance_threshold: float = 0.3  # Chá»‰ count triples cÃ³ relevance >= threshold
    max_passages_output: int = 10
    
    # Score processing parameters
    enable_score_normalization: bool = True
    boost_factor_high_support: float = 1.2  # Boost cho passages cÃ³ nhiá»u support
    penalty_factor_no_support: float = 0.8  # Penalty cho passages khÃ´ng cÃ³ support
    
    # Diversity parameters
    enable_diversity_ranking: bool = False  # Advanced feature
    diversity_threshold: float = 0.85  # Similarity threshold cho diversity
    
    # Advanced parameters
    min_support_threshold: float = 0.1  # Minimum support Ä‘á»ƒ avoid penalty
    adaptive_weight_adjustment: bool = True  # Auto-adjust weights based on data

@dataclass
class RankedPassage:
    """
    Passage Ä‘Ã£ Ä‘Æ°á»£c ranked vá»›i comprehensive scoring information
    
    Attributes:
        passage_id (str): ID unique cá»§a passage
        original_text (str): Ná»™i dung gá»‘c cá»§a passage
        hybrid_retrieval_score (float): Äiá»ƒm tá»« Module 1 (hybrid score)
        support_score (float): Äiá»ƒm support tá»« triples (0.0-1.0)
        final_score (float): Äiá»ƒm cuá»‘i cÃ¹ng sau khi combine
        rank (int): Thá»© háº¡ng final (1, 2, 3, ...)
        supporting_triples_count (int): Sá»‘ lÆ°á»£ng triples há»— trá»£ passage nÃ y
        supporting_triples (List[str]): Danh sÃ¡ch IDs cá»§a supporting triples
        score_breakdown (Dict): Chi tiáº¿t breakdown cÃ¡c scores
        ranking_metadata (Dict): Metadata vá» quÃ¡ trÃ¬nh ranking
    """
    passage_id: str
    original_text: str
    hybrid_retrieval_score: float
    support_score: float
    final_score: float
    rank: int
    supporting_triples_count: int
    supporting_triples: List[str]
    score_breakdown: Dict[str, float]
    ranking_metadata: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Chuyá»ƒn Ä‘á»•i RankedPassage object thÃ nh dictionary
        
        Returns:
            Dict[str, Any]: Dictionary representation
        """
        return asdict(self)
    
    def get_support_ratio(self) -> float:
        """
        TÃ­nh tá»· lá»‡ support so vá»›i retrieval score
        
        Returns:
            float: Support ratio (support_score / hybrid_retrieval_score)
        """
        if self.hybrid_retrieval_score == 0:
            return float('inf') if self.support_score > 0 else 0
        return self.support_score / self.hybrid_retrieval_score
    
    def has_strong_support(self, threshold: int = 2) -> bool:
        """
        Kiá»ƒm tra passage cÃ³ support máº¡nh khÃ´ng
        
        Args:
            threshold (int): NgÆ°á»¡ng sá»‘ triples Ä‘á»ƒ coi lÃ  strong support
            
        Returns:
            bool: True náº¿u cÃ³ strong support
        """
        return self.supporting_triples_count >= threshold
    
    def get_summary(self) -> str:
        """
        Táº¡o summary ngáº¯n gá»n vá» ranked passage
        
        Returns:
            str: Summary string
        """
        return (f"Rank {self.rank}: {self.passage_id} | "
                f"Final: {self.final_score:.3f} "
                f"(Ret: {self.hybrid_retrieval_score:.3f}, Sup: {self.support_score:.3f}) | "
                f"Triples: {self.supporting_triples_count}")

@dataclass
class RankingResult:
    """
    Káº¿t quáº£ hoÃ n chá»‰nh cá»§a quÃ¡ trÃ¬nh ranking vá»›i comprehensive statistics
    
    Attributes:
        ranked_passages (List[RankedPassage]): Passages Ä‘Ã£ Ä‘Æ°á»£c ranked
        original_passage_count (int): Sá»‘ lÆ°á»£ng passages ban Ä‘áº§u
        final_passage_count (int): Sá»‘ lÆ°á»£ng passages sau ranking
        ranking_time (float): Thá»i gian thá»±c hiá»‡n ranking (giÃ¢y)
        query (str): Query gá»‘c
        statistics (Dict): Thá»‘ng kÃª chi tiáº¿t vá» quÃ¡ trÃ¬nh ranking
        ranking_config (PassageRankerConfig): Config Ä‘Æ°á»£c sá»­ dá»¥ng
    """
    ranked_passages: List[RankedPassage]
    original_passage_count: int
    final_passage_count: int
    ranking_time: float
    query: str
    statistics: Dict[str, Any]
    ranking_config: PassageRankerConfig
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Chuyá»ƒn Ä‘á»•i RankingResult thÃ nh dictionary
        
        Returns:
            Dict[str, Any]: Dictionary representation
        """
        result = asdict(self)
        result['ranking_config'] = asdict(self.ranking_config)
        result['ranking_config']['ranking_strategy'] = self.ranking_config.ranking_strategy.value
        result['ranking_config']['support_calculation_method'] = self.ranking_config.support_calculation_method.value
        return result
    
    def save_to_file(self, filepath: Path):
        """
        LÆ°u káº¿t quáº£ ranking vÃ o file JSON
        
        Args:
            filepath (Path): ÄÆ°á»ng dáº«n file
        """
        save_json(self.to_dict(), filepath, indent=2)
        logger.info(f"ğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£ ranking vÃ o file: {filepath}")
        logger.info(f"   ğŸ“Š Passages: {self.original_passage_count} â†’ {self.final_passage_count}")
        logger.info(f"   â±ï¸ Thá»i gian: {self.ranking_time:.2f} giÃ¢y")
    
    def get_ranking_efficiency(self) -> float:
        """
        TÃ­nh hiá»‡u suáº¥t ranking
        
        Returns:
            float: Efficiency ratio
        """
        if self.original_passage_count == 0:
            return 0.0
        return self.final_passage_count / self.original_passage_count
    
    def get_support_distribution(self) -> Dict[str, int]:
        """
        Thá»‘ng kÃª phÃ¢n bá»‘ support levels
        
        Returns:
            Dict[str, int]: Support distribution
        """
        distribution = {
            'no_support': 0,
            'low_support': 0,    # 1-2 triples
            'medium_support': 0, # 3-5 triples
            'high_support': 0    # 6+ triples
        }
        
        for passage in self.ranked_passages:
            count = passage.supporting_triples_count
            if count == 0:
                distribution['no_support'] += 1
            elif count <= 2:
                distribution['low_support'] += 1
            elif count <= 5:
                distribution['medium_support'] += 1
            else:
                distribution['high_support'] += 1
        
        return distribution

# ==================== CORE COMPONENTS ====================

class SupportScoreCalculator:
    """
    Calculator cho support scores dá»±a trÃªn filtered triples
    
    Class nÃ y implement cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhau Ä‘á»ƒ tÃ­nh support score
    cho passages dá»±a trÃªn cÃ¡c filtered triples tá»« Module 2.
    """
    
    def __init__(self, config: PassageRankerConfig):
        """
        Khá»Ÿi táº¡o SupportScoreCalculator
        
        Args:
            config (PassageRankerConfig): Cáº¥u hÃ¬nh ranking
        """
        self.config = config
        self.passage_triple_mapping = defaultdict(list)  # passage_id -> [triple_ids]
        self.triple_lookup = {}  # triple_id -> FilteredTriple
        
        logger.info("ğŸ“Š Khá»Ÿi táº¡o SupportScoreCalculator...")
        logger.info(f"   ğŸ§® Method: {config.support_calculation_method.value}")
        logger.info(f"   ğŸ¯ Relevance threshold: {config.relevance_threshold}")
    
    def build_support_mapping(self, filtered_triples: List[FilteredTriple]):
        """
        XÃ¢y dá»±ng mapping giá»¯a passages vÃ  supporting triples
        
        Args:
            filtered_triples (List[FilteredTriple]): Filtered triples tá»« Module 2
        """
        logger.info(f"ğŸ”— Äang xÃ¢y dá»±ng support mapping cho {len(filtered_triples)} filtered triples...")
        
        # Clear previous mappings
        self.passage_triple_mapping.clear()
        self.triple_lookup.clear()
        
        relevant_triples_count = 0
        
        for triple in filtered_triples:
            # Store triple lookup
            self.triple_lookup[triple.triple_id] = triple
            
            # Only consider triples above relevance threshold
            if triple.query_relevance_score >= self.config.relevance_threshold:
                passage_id = triple.source_passage_id
                if passage_id:  # Ensure passage_id exists
                    self.passage_triple_mapping[passage_id].append(triple.triple_id)
                    relevant_triples_count += 1
        
        passages_with_support = len(self.passage_triple_mapping)
        logger.info(f"âœ… Support mapping completed:")
        logger.info(f"   ğŸ“Š Relevant triples: {relevant_triples_count}/{len(filtered_triples)}")
        logger.info(f"   ğŸ“„ Passages with support: {passages_with_support}")
        
        # Log top supported passages
        if self.passage_triple_mapping:
            sorted_passages = sorted(
                self.passage_triple_mapping.items(), 
                key=lambda x: len(x[1]), 
                reverse=True
            )
            logger.info(f"   ğŸ† Top supported passages:")
            for i, (passage_id, triple_ids) in enumerate(sorted_passages[:3], 1):
                logger.info(f"      {i}. {passage_id}: {len(triple_ids)} triples")
    
    def calculate_support_score(self, passage_id: str) -> Tuple[float, int, List[str]]:
        """
        TÃ­nh support score cho má»™t passage
        
        Args:
            passage_id (str): ID cá»§a passage
            
        Returns:
            Tuple[float, int, List[str]]: (support_score, count, supporting_triple_ids)
        """
        supporting_triple_ids = self.passage_triple_mapping.get(passage_id, [])
        
        if not supporting_triple_ids:
            return 0.0, 0, []
        
        # Get supporting triples
        supporting_triples = [
            self.triple_lookup[triple_id] 
            for triple_id in supporting_triple_ids 
            if triple_id in self.triple_lookup
        ]
        
        # Calculate score based on method
        if self.config.support_calculation_method == SupportCalculationMethod.COUNT_BASED:
            score = self._calculate_count_based_score(supporting_triples)
        elif self.config.support_calculation_method == SupportCalculationMethod.WEIGHTED_RELEVANCE:
            score = self._calculate_weighted_relevance_score(supporting_triples)
        elif self.config.support_calculation_method == SupportCalculationMethod.QUALITY_WEIGHTED:
            score = self._calculate_quality_weighted_score(supporting_triples)
        elif self.config.support_calculation_method == SupportCalculationMethod.LOG_SCALED:
            score = self._calculate_log_scaled_score(supporting_triples)
        else:
            score = self._calculate_weighted_relevance_score(supporting_triples)  # Default
        
        return score, len(supporting_triples), supporting_triple_ids
    
    def _calculate_count_based_score(self, supporting_triples: List[FilteredTriple]) -> float:
        """
        TÃ­nh support score dá»±a trÃªn sá»‘ lÆ°á»£ng triples
        
        Args:
            supporting_triples (List[FilteredTriple]): Supporting triples
            
        Returns:
            float: Count-based support score
        """
        count = len(supporting_triples)
        # Normalize using log scaling to prevent very high scores
        if count == 0:
            return 0.0
        elif count == 1:
            return 0.3
        elif count == 2:
            return 0.5
        elif count <= 5:
            return 0.7
        else:
            return min(0.9, 0.7 + 0.05 * (count - 5))  # Cap at 0.9
    
    def _calculate_weighted_relevance_score(self, supporting_triples: List[FilteredTriple]) -> float:
        """
        TÃ­nh support score dá»±a trÃªn tá»•ng relevance scores
        
        Args:
            supporting_triples (List[FilteredTriple]): Supporting triples
            
        Returns:
            float: Weighted relevance support score
        """
        if not supporting_triples:
            return 0.0
        
        # Sum of all relevance scores
        total_relevance = sum(triple.query_relevance_score for triple in supporting_triples)
        
        # Average relevance
        avg_relevance = total_relevance / len(supporting_triples)
        
        # Apply count boost (more triples = higher score)
        count_boost = min(1.0, len(supporting_triples) / 5.0)  # Max boost at 5 triples
        
        # Combine average relevance with count boost
        final_score = (avg_relevance * 0.7) + (count_boost * 0.3)
        
        return min(final_score, 1.0)  # Cap at 1.0
    
    def _calculate_quality_weighted_score(self, supporting_triples: List[FilteredTriple]) -> float:
        """
        TÃ­nh support score dá»±a trÃªn quality scores (relevance + confidence)
        
        Args:
            supporting_triples (List[FilteredTriple]): Supporting triples
            
        Returns:
            float: Quality weighted support score
        """
        if not supporting_triples:
            return 0.0
        
        quality_scores = [triple.get_quality_score() for triple in supporting_triples]
        avg_quality = sum(quality_scores) / len(quality_scores)
        
        # Apply count boost
        count_boost = min(1.0, len(supporting_triples) / 5.0)
        
        # Combine quality with count
        final_score = (avg_quality * 0.8) + (count_boost * 0.2)
        
        return min(final_score, 1.0)
    
    def _calculate_log_scaled_score(self, supporting_triples: List[FilteredTriple]) -> float:
        """
        TÃ­nh support score vá»›i log scaling Ä‘á»ƒ handle outliers
        
        Args:
            supporting_triples (List[FilteredTriple]): Supporting triples
            
        Returns:
            float: Log scaled support score
        """
        if not supporting_triples:
            return 0.0
        
        # Base score from relevance
        total_relevance = sum(triple.query_relevance_score for triple in supporting_triples)
        
        # Log scaling to prevent extreme values
        log_scaled_count = math.log(len(supporting_triples) + 1) / math.log(6)  # Normalize to ~1.0 at 5 triples
        log_scaled_relevance = math.log(total_relevance + 1) / math.log(6)
        
        # Combine
        final_score = (log_scaled_relevance * 0.6) + (log_scaled_count * 0.4)
        
        return min(final_score, 1.0)
    
    def get_support_statistics(self) -> Dict[str, Any]:
        """
        Láº¥y thá»‘ng kÃª vá» support mapping
        
        Returns:
            Dict[str, Any]: Support statistics
        """
        if not self.passage_triple_mapping:
            return {
                'total_passages_with_support': 0,
                'total_supporting_triples': 0,
                'avg_triples_per_passage': 0,
                'max_triples_per_passage': 0
            }
        
        triple_counts = [len(triple_ids) for triple_ids in self.passage_triple_mapping.values()]
        
        return {
            'total_passages_with_support': len(self.passage_triple_mapping),
            'total_supporting_triples': sum(triple_counts),
            'avg_triples_per_passage': sum(triple_counts) / len(triple_counts),
            'max_triples_per_passage': max(triple_counts),
            'min_triples_per_passage': min(triple_counts),
            'support_distribution': {
                '1_triple': sum(1 for count in triple_counts if count == 1),
                '2-3_triples': sum(1 for count in triple_counts if 2 <= count <= 3),
                '4-5_triples': sum(1 for count in triple_counts if 4 <= count <= 5),
                '6+_triples': sum(1 for count in triple_counts if count >= 6)
            }
        }

class PassageRanker:
    """
    Main class cho ranking passages dá»±a trÃªn retrieval scores vÃ  support scores
    
    Class nÃ y implement logic chÃ­nh Ä‘á»ƒ combine retrieval scores tá»« Module 1
    vá»›i support scores tÃ­nh tá»« filtered triples cá»§a Module 2.
    """
    
    def __init__(self, config: Optional[PassageRankerConfig] = None):
        """
        Khá»Ÿi táº¡o PassageRanker
        
        Args:
            config (Optional[PassageRankerConfig]): Cáº¥u hÃ¬nh ranking
        """
        self.config = config or PassageRankerConfig()
        self.support_calculator = SupportScoreCalculator(self.config)
        
        logger.info("ğŸ† Khá»Ÿi táº¡o PassageRanker...")
        logger.info(f"   ğŸ“Š Strategy: {self.config.ranking_strategy.value}")
        logger.info(f"   ğŸ§® Support method: {self.config.support_calculation_method.value}")
        logger.info(f"   ğŸ“ Max output: {self.config.max_passages_output}")
        logger.info(f"   âš–ï¸ Score normalization: {'Báº­t' if self.config.enable_score_normalization else 'Táº¯t'}")
    
    def rank_passages(self, raw_passages: List[RetrievedItem], 
                     filtered_triples: List[FilteredTriple],
                     query: str) -> RankingResult:
        """
        Main method Ä‘á»ƒ rank passages dá»±a trÃªn retrieval scores vÃ  support scores
        
        Args:
            raw_passages (List[RetrievedItem]): Raw passages tá»« Module 1
            filtered_triples (List[FilteredTriple]): Filtered triples tá»« Module 2
            query (str): Query gá»‘c
            
        Returns:
            RankingResult: Káº¿t quáº£ ranking hoÃ n chá»‰nh
        """
        logger.info("=" * 60)
        logger.info("ğŸ† Báº®T Äáº¦U PASSAGE RANKING")
        logger.info("=" * 60)
        logger.info(f"ğŸ“ Query: '{query}'")
        logger.info(f"ğŸ“„ Raw passages: {len(raw_passages)}")
        logger.info(f"ğŸ”— Filtered triples: {len(filtered_triples)}")
        logger.info(f"ğŸ¯ Strategy: {self.config.ranking_strategy.value}")
        
        # Log raw passages vá»›i thá»© tá»± ban Ä‘áº§u
        logger.info("\nğŸ“„ RAW PASSAGES (Sáº®P Xáº¾P THEO EMBEDDING SCORE):")
        logger.info("-" * 60)
        # Sort raw passages by embedding score for clear comparison
        sorted_raw = sorted(raw_passages, key=lambda x: x.embedding_score, reverse=True)
        for i, passage in enumerate(sorted_raw, 1):
            logger.info(f"   {i}. ID: {passage.item_id}")
            logger.info(f"      Text: {passage.text[:100]}...")
            logger.info(f"      Scores: BM25={passage.bm25_score:.3f}, Embed={passage.embedding_score:.3f}, Hybrid={passage.hybrid_score:.3f}")
            logger.info(f"      Metadata: {passage.metadata}")
        
        start_time = time.time()
        
        # Validate inputs
        if not raw_passages:
            logger.warning("âš ï¸ KhÃ´ng cÃ³ raw passages Ä‘á»ƒ rank")
            return self._create_empty_result(query, 0, "no_passages")
        
        if not validate_query(query):
            logger.error("âŒ Invalid query")
            return self._create_empty_result(query, len(raw_passages), "invalid_query")
        
        # Step 1: Build support mapping
        logger.info(f"\nğŸ“Š BÆ¯á»šC 1: XÃ‚Y Dá»°NG SUPPORT MAPPING")
        logger.info("-" * 40)
        self.support_calculator.build_support_mapping(filtered_triples)
        
        # Step 2: Calculate scores for all passages
        logger.info(f"\nğŸ§® BÆ¯á»šC 2: TÃNH TOÃN SCORES")
        logger.info("-" * 40)
        scored_passages = self._calculate_all_scores(raw_passages)
        
        # Step 3: Apply ranking strategy
        logger.info(f"\nğŸ† BÆ¯á»šC 3: ÃP Dá»¤NG RANKING STRATEGY")
        logger.info("-" * 40)
        ranked_passages = self._apply_ranking_strategy(scored_passages, query)
        
        # Step 4: Apply final processing
        logger.info(f"\nğŸ”§ BÆ¯á»šC 4: Xá»¬ LÃ CUá»I CÃ™NG")
        logger.info("-" * 40)
        final_passages = self._apply_final_processing(ranked_passages)
        
        # Log final passages vá»›i thá»© tá»± má»›i vÃ  giáº£i thÃ­ch
        logger.info("\nğŸ† FINAL RANKED PASSAGES (SAU KHI TÃNH SUPPORT SCORE):")
        logger.info("-" * 60)
        logger.info("   Giáº£i thÃ­ch thá»© tá»± má»›i:")
        logger.info("   1. Final score = (alpha_retrieval * hybrid_retrieval_score) + (alpha_support * support_score)")
        logger.info("   2. Support score Ä‘Æ°á»£c tÃ­nh tá»« sá»‘ lÆ°á»£ng vÃ  cháº¥t lÆ°á»£ng cá»§a supporting triples")
        logger.info("   3. Passages cÃ³ nhiá»u supporting triples sáº½ Ä‘Æ°á»£c boost lÃªn")
        logger.info("   4. Passages khÃ´ng cÃ³ support sáº½ bá»‹ penalty")
        logger.info("-" * 60)
        
        for passage in final_passages:
            logger.info(f"   Rank {passage.rank}. ID: {passage.passage_id}")
            logger.info(f"      Text: {passage.original_text[:100]}...")
            logger.info(f"      Scores:")
            logger.info(f"         - Final score: {passage.final_score:.3f}")
            logger.info(f"         - Retrieval score: {passage.hybrid_retrieval_score:.3f}")
            logger.info(f"         - Support score: {passage.support_score:.3f}")
            logger.info(f"      Supporting triples: {passage.supporting_triples_count}")
            if passage.supporting_triples:
                logger.info(f"      Triple IDs: {', '.join(passage.supporting_triples[:3])}...")
            logger.info(f"      Score breakdown: {passage.score_breakdown}")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # Step 5: Create result vá»›i statistics
        logger.info(f"\nğŸ“ˆ BÆ¯á»šC 5: Táº O Káº¾T QUáº¢ VÃ€ THá»NG KÃŠ")
        logger.info("-" * 40)
        statistics = self._create_ranking_statistics(
            raw_passages, final_passages, filtered_triples, total_time
        )
        
        result = RankingResult(
            ranked_passages=final_passages,
            original_passage_count=len(raw_passages),
            final_passage_count=len(final_passages),
            ranking_time=total_time,
            query=query,
            statistics=statistics,
            ranking_config=self.config
        )
        
        # Log final summary
        self._log_ranking_summary(result)
        
        return result
    
    def _calculate_all_scores(self, raw_passages: List[RetrievedItem]) -> List[Dict[str, Any]]:
        """
        TÃ­nh toÃ¡n scores cho táº¥t cáº£ passages
        
        Args:
            raw_passages (List[RetrievedItem]): Raw passages
            
        Returns:
            List[Dict[str, Any]]: Scored passages vá»›i Ä‘áº§y Ä‘á»§ thÃ´ng tin
        """
        logger.info(f"ğŸ§® Äang tÃ­nh scores cho {len(raw_passages)} passages...")
        
        scored_passages = []
        hybrid_retrieval_scores = []
        support_scores = []
        
        for i, passage in enumerate(raw_passages, 1):
            # Get retrieval score tá»« Module 1
            hybrid_retrieval_score = passage.hybrid_score
            hybrid_retrieval_scores.append(hybrid_retrieval_score)
            
            # Calculate support score
            support_score, support_count, supporting_triple_ids = \
                self.support_calculator.calculate_support_score(passage.item_id)
            support_scores.append(support_score)
            
            # Create scored passage info
            scored_passage = {
                'passage_id': passage.item_id,
                'original_text': passage.text,
                'hybrid_retrieval_score': hybrid_retrieval_score,
                'support_score': support_score,
                'supporting_triples_count': support_count,
                'supporting_triples': supporting_triple_ids,
                'original_passage': passage,
                'score_breakdown': {
                    'bm25_score': passage.bm25_score,
                    'embedding_score': passage.embedding_score,
                    'hybrid_score': passage.hybrid_score,
                    'support_score': support_score,
                    'support_count': support_count
                }
            }
            
            scored_passages.append(scored_passage)
            
            if i % 5 == 0 or i == len(raw_passages):
                logger.info(f"   ğŸ“Š Processed {i}/{len(raw_passages)} passages")
        
        # Log score distributions
        if hybrid_retrieval_scores and support_scores:
            logger.info(f"ğŸ“ˆ Score distributions:")
            logger.info(f"   ğŸ“Š Retrieval scores: min={min(hybrid_retrieval_scores):.3f}, max={max(hybrid_retrieval_scores):.3f}, avg={sum(hybrid_retrieval_scores)/len(hybrid_retrieval_scores):.3f}")
            logger.info(f"   ğŸ”— Support scores: min={min(support_scores):.3f}, max={max(support_scores):.3f}, avg={sum(support_scores)/len(support_scores):.3f}")
            
            # Count passages by support level
            no_support = sum(1 for score in support_scores if score == 0)
            low_support = sum(1 for score in support_scores if 0 < score <= 0.3)
            medium_support = sum(1 for score in support_scores if 0.3 < score <= 0.7)
            high_support = sum(1 for score in support_scores if score > 0.7)
            
            logger.info(f"   ğŸ“„ Support levels: no={no_support}, low={low_support}, medium={medium_support}, high={high_support}")
        
        logger.info(f"âœ… HoÃ n thÃ nh tÃ­nh scores cho {len(scored_passages)} passages")
        return scored_passages
    
    def _apply_ranking_strategy(self, scored_passages: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
        """
        Ãp dá»¥ng ranking strategy Ä‘á»ƒ tÃ­nh final scores
        
        Args:
            scored_passages (List[Dict]): Passages vá»›i scores
            query (str): Query gá»‘c
            
        Returns:
            List[Dict[str, Any]]: Passages vá»›i final scores
        """
        logger.info(f"ğŸ† Ãp dá»¥ng ranking strategy: {self.config.ranking_strategy.value}")
        
        # Get weights based on strategy
        if self.config.ranking_strategy == RankingStrategy.ADAPTIVE:
            alpha_retrieval, alpha_support = self._calculate_adaptive_weights(scored_passages)
            logger.info(f"   ğŸ§  Adaptive weights: retrieval={alpha_retrieval:.2f}, support={alpha_support:.2f}")
        else:
            alpha_retrieval, alpha_support = self.config.ranking_strategy.get_weights()
            logger.info(f"   âš–ï¸ Fixed weights: retrieval={alpha_retrieval:.2f}, support={alpha_support:.2f}")
        
        # Normalize scores if enabled
        if self.config.enable_score_normalization:
            scored_passages = self._normalize_scores(scored_passages)
            logger.info("   ğŸ“Š Scores Ä‘Ã£ Ä‘Æ°á»£c normalized")
        
        # Calculate final scores
        for passage in scored_passages:
            hybrid_retrieval_score = passage['hybrid_retrieval_score']
            support_score = passage['support_score']
            
            # Base final score
            final_score = (alpha_retrieval * hybrid_retrieval_score) + (alpha_support * support_score)
            
            # Apply boost/penalty factors
            if support_score > 0.7:  # High support boost
                final_score *= self.config.boost_factor_high_support
                logger.debug(f"   ğŸš€ Boost applied to {passage['passage_id']}: {final_score:.3f}")
            elif support_score < self.config.min_support_threshold:  # Low support penalty
                final_score *= self.config.penalty_factor_no_support
                logger.debug(f"   â¬‡ï¸ Penalty applied to {passage['passage_id']}: {final_score:.3f}")
            
            # Cap final score at 1.0
            final_score = min(final_score, 1.0)
            
            passage['final_score'] = final_score
            
            # Update score breakdown
            passage['score_breakdown'].update({
                'alpha_retrieval': alpha_retrieval,
                'alpha_support': alpha_support,
                'base_final_score': (alpha_retrieval * hybrid_retrieval_score) + (alpha_support * support_score),
                'final_score_after_modifiers': final_score
            })
        
        # Sort by final score descending
        scored_passages.sort(key=lambda x: x['final_score'], reverse=True)
        
        logger.info(f"âœ… Final scores calculated vÃ  sorted")
        if scored_passages:
            top_passage = scored_passages[0]
            logger.info(f"   ğŸ† Top passage: {top_passage['passage_id']} vá»›i score {top_passage['final_score']:.3f}")
        
        return scored_passages
    
    def _calculate_adaptive_weights(self, scored_passages: List[Dict[str, Any]]) -> Tuple[float, float]:
        """
        TÃ­nh adaptive weights dá»±a trÃªn cháº¥t lÆ°á»£ng data
        
        Args:
            scored_passages (List[Dict]): Scored passages
            
        Returns:
            Tuple[float, float]: (alpha_retrieval, alpha_support)
        """
        hybrid_retrieval_scores = [p['hybrid_retrieval_score'] for p in scored_passages]
        support_scores = [p['support_score'] for p in scored_passages]
        
        # Calculate quality metrics
        retrieval_variance = self._calculate_variance(hybrid_retrieval_scores)
        support_variance = self._calculate_variance(support_scores)
        
        # Calculate support coverage (percentage of passages with support > 0)
        passages_with_support = sum(1 for score in support_scores if score > 0)
        support_coverage = passages_with_support / len(support_scores) if support_scores else 0
        
        logger.info(f"   ğŸ“Š Adaptive metrics:")
        logger.info(f"      ğŸ“ˆ Retrieval variance: {retrieval_variance:.3f}")
        logger.info(f"      ğŸ”— Support variance: {support_variance:.3f}")
        logger.info(f"      ğŸ“„ Support coverage: {support_coverage:.2%}")
        
        # Adaptive logic
        if support_coverage < 0.3:  # Low support coverage
            alpha_retrieval, alpha_support = 0.8, 0.2
            logger.info("   ğŸ¯ Low support coverage â†’ Retrieval focused")
        elif support_coverage > 0.7 and support_variance > 0.1:  # High support with good variance
            alpha_retrieval, alpha_support = 0.4, 0.6
            logger.info("   ğŸ¯ High quality support â†’ Support focused")
        else:  # Balanced case
            alpha_retrieval, alpha_support = 0.6, 0.4
            logger.info("   ğŸ¯ Balanced case â†’ Moderate weights")
        
        return alpha_retrieval, alpha_support
    
    def _calculate_variance(self, scores: List[float]) -> float:
        """TÃ­nh variance cá»§a scores"""
        if len(scores) <= 1:
            return 0.0
        
        mean = sum(scores) / len(scores)
        variance = sum((score - mean) ** 2 for score in scores) / len(scores)
        return variance
    
    def _normalize_scores(self, scored_passages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Normalize retrieval vÃ  support scores vá» [0, 1] range
        
        Args:
            scored_passages (List[Dict]): Passages vá»›i scores
            
        Returns:
            List[Dict[str, Any]]: Passages vá»›i normalized scores
        """
        hybrid_retrieval_scores = [p['hybrid_retrieval_score'] for p in scored_passages]
        support_scores = [p['support_score'] for p in scored_passages]
        
        # Normalize retrieval scores
        if hybrid_retrieval_scores:
            min_ret, max_ret = min(hybrid_retrieval_scores), max(hybrid_retrieval_scores)
            if max_ret > min_ret:
                for passage in scored_passages:
                    old_score = passage['hybrid_retrieval_score']
                    new_score = (old_score - min_ret) / (max_ret - min_ret)
                    passage['hybrid_retrieval_score'] = new_score
                    passage['score_breakdown']['normalized_retrieval'] = new_score
        
        # Normalize support scores (usually already in [0,1] but ensure it)
        if support_scores:
            max_sup = max(support_scores)
            if max_sup > 1.0:  # Only normalize if exceeds 1.0
                for passage in scored_passages:
                    old_score = passage['support_score']
                    new_score = old_score / max_sup
                    passage['support_score'] = new_score
                    passage['score_breakdown']['normalized_support'] = new_score
        
        return scored_passages
    
    def _apply_final_processing(self, ranked_passages: List[Dict[str, Any]]) -> List[RankedPassage]:
        """
        Ãp dá»¥ng final processing vÃ  convert thÃ nh RankedPassage objects
        
        Args:
            ranked_passages (List[Dict]): Ranked passages
            
        Returns:
            List[RankedPassage]: Final ranked passages
        """
        logger.info(f"ğŸ”§ Ãp dá»¥ng final processing...")
        
        # Apply max output limit
        limited_passages = ranked_passages[:self.config.max_passages_output]
        logger.info(f"   ğŸ“Š Limited output: {len(ranked_passages)} â†’ {len(limited_passages)} passages")
        
        # Apply diversity ranking if enabled
        if self.config.enable_diversity_ranking:
            limited_passages = self._apply_diversity_ranking(limited_passages)
            logger.info(f"   ğŸŒˆ Diversity ranking applied")
        
        # Convert to RankedPassage objects
        final_passages = []
        for rank, passage_data in enumerate(limited_passages, 1):
            ranked_passage = RankedPassage(
                passage_id=passage_data['passage_id'],
                original_text=passage_data['original_text'],
                hybrid_retrieval_score=passage_data['hybrid_retrieval_score'],
                support_score=passage_data['support_score'],
                final_score=passage_data['final_score'],
                rank=rank,
                supporting_triples_count=passage_data['supporting_triples_count'],
                supporting_triples=passage_data['supporting_triples'],
                score_breakdown=passage_data['score_breakdown'],
                ranking_metadata={
                    'ranking_strategy': self.config.ranking_strategy.value,
                    'support_method': self.config.support_calculation_method.value,
                    'processing_timestamp': time.time(),
                    'original_rank_from_retrieval': None  # Could track original ranking
                }
            )
            final_passages.append(ranked_passage)
        
        logger.info(f"âœ… Final processing hoÃ n thÃ nh: {len(final_passages)} ranked passages")
        return final_passages
    
    def _apply_diversity_ranking(self, passages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Ãp dá»¥ng diversity ranking Ä‘á»ƒ trÃ¡nh duplicate/similar passages
        
        Args:
            passages (List[Dict]): Ranked passages
            
        Returns:
            List[Dict[str, Any]]: Diversified passages
        """
        # Simple diversity implementation based on text similarity
        # Advanced version would use embedding similarity
        
        diversified = []
        seen_texts = set()
        
        for passage in passages:
            text = passage['original_text'].lower()
            
            # Simple duplicate detection based on first 100 characters
            text_signature = text[:100]
            
            is_duplicate = False
            for seen_sig in seen_texts:
                # Simple similarity check
                common_chars = len(set(text_signature) & set(seen_sig))
                similarity = common_chars / max(len(text_signature), len(seen_sig), 1)
                
                if similarity > self.config.diversity_threshold:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                diversified.append(passage)
                seen_texts.add(text_signature)
            else:
                logger.debug(f"   ğŸ”„ Filtered duplicate: {passage['passage_id']}")
        
        logger.info(f"   ğŸŒˆ Diversity filtering: {len(passages)} â†’ {len(diversified)}")
        return diversified
    
    def _create_ranking_statistics(self, raw_passages: List[RetrievedItem], 
                                  final_passages: List[RankedPassage],
                                  filtered_triples: List[FilteredTriple],
                                  processing_time: float) -> Dict[str, Any]:
        """
        Táº¡o comprehensive statistics cho ranking process
        
        Args:
            raw_passages (List[RetrievedItem]): Original passages
            final_passages (List[RankedPassage]): Final ranked passages
            filtered_triples (List[FilteredTriple]): Input triples
            processing_time (float): Total processing time
            
        Returns:
            Dict[str, Any]: Comprehensive statistics
        """
        logger.info("ğŸ“ˆ Táº¡o ranking statistics...")
        
        # Basic counts
        original_count = len(raw_passages)
        final_count = len(final_passages)
        
        # Score statistics
        if final_passages:
            hybrid_retrieval_scores = [p.hybrid_retrieval_score for p in final_passages]
            support_scores = [p.support_score for p in final_passages]
            final_scores = [p.final_score for p in final_passages]
            
            score_stats = {
                'hybrid_retrieval_scores': {
                    'min': min(hybrid_retrieval_scores),
                    'max': max(hybrid_retrieval_scores),
                    'avg': sum(hybrid_retrieval_scores) / len(hybrid_retrieval_scores),
                    'median': sorted(hybrid_retrieval_scores)[len(hybrid_retrieval_scores) // 2]
                },
                'support_scores': {
                    'min': min(support_scores),
                    'max': max(support_scores),
                    'avg': sum(support_scores) / len(support_scores),
                    'median': sorted(support_scores)[len(support_scores) // 2]
                },
                'final_scores': {
                    'min': min(final_scores),
                    'max': max(final_scores),
                    'avg': sum(final_scores) / len(final_scores),
                    'median': sorted(final_scores)[len(final_scores) // 2]
                }
            }
        else:
            score_stats = {
                'hybrid_retrieval_scores': {'min': 0, 'max': 0, 'avg': 0, 'median': 0},
                'support_scores': {'min': 0, 'max': 0, 'avg': 0, 'median': 0},
                'final_scores': {'min': 0, 'max': 0, 'avg': 0, 'median': 0}
            }
        
        # Support statistics
        support_stats = self.support_calculator.get_support_statistics()
        
        # Ranking changes analysis
        ranking_changes = self._analyze_ranking_changes(raw_passages, final_passages)
        
        # Performance metrics
        performance_stats = {
            'total_time_seconds': processing_time,
            'passages_per_second': original_count / processing_time if processing_time > 0 else 0,
            'avg_time_per_passage': processing_time / original_count if original_count > 0 else 0
        }
        
        # Configuration info
        config_stats = {
            'ranking_strategy': self.config.ranking_strategy.value,
            'support_calculation_method': self.config.support_calculation_method.value,
            'relevance_threshold': self.config.relevance_threshold,
            'max_passages_output': self.config.max_passages_output,
            'score_normalization_enabled': self.config.enable_score_normalization,
            'diversity_ranking_enabled': self.config.enable_diversity_ranking
        }
        
        return {
            'processing_counts': {
                'original_passages': original_count,
                'final_passages': final_count,
                'filtered_triples_used': len(filtered_triples)
            },
            'score_statistics': score_stats,
            'support_statistics': support_stats,
            'ranking_changes': ranking_changes,
            'performance_metrics': performance_stats,
            'configuration': config_stats,
            'timestamp': time.time()
        }
    
    def _analyze_ranking_changes(self, raw_passages: List[RetrievedItem], 
                                final_passages: List[RankedPassage]) -> Dict[str, Any]:
        """
        PhÃ¢n tÃ­ch nhá»¯ng thay Ä‘á»•i trong ranking so vá»›i original retrieval order
        
        Args:
            raw_passages (List[RetrievedItem]): Original passages (sorted by hybrid score)
            final_passages (List[RankedPassage]): Final ranked passages
            
        Returns:
            Dict[str, Any]: Ranking change analysis
        """
        # Create mapping from passage_id to original rank
        original_ranks = {passage.item_id: i + 1 for i, passage in enumerate(raw_passages)}
        
        # Calculate rank changes
        rank_changes = []
        significant_changes = 0
        
        for final_passage in final_passages:
            original_rank = original_ranks.get(final_passage.passage_id, len(raw_passages) + 1)
            final_rank = final_passage.rank
            
            rank_change = original_rank - final_rank  # Positive = moved up
            rank_changes.append(rank_change)
            
            if abs(rank_change) >= 3:  # Significant change threshold
                significant_changes += 1
        
        # Statistics
        if rank_changes:
            avg_rank_change = sum(rank_changes) / len(rank_changes)
            max_improvement = max(rank_changes)
            max_decline = min(rank_changes)
        else:
            avg_rank_change = max_improvement = max_decline = 0
        
        return {
            'total_rank_changes': len(rank_changes),
            'significant_changes': significant_changes,
            'avg_rank_change': avg_rank_change,
            'max_improvement': max_improvement,
            'max_decline': max_decline,
            'passages_moved_up': sum(1 for change in rank_changes if change > 0),
            'passages_moved_down': sum(1 for change in rank_changes if change < 0),
            'passages_unchanged': sum(1 for change in rank_changes if change == 0)
        }
    
    def _create_empty_result(self, query: str, original_count: int, reason: str) -> RankingResult:
        """
        Táº¡o empty RankingResult khi cÃ³ lá»—i
        
        Args:
            query (str): Original query
            original_count (int): Number of original passages
            reason (str): Reason for empty result
            
        Returns:
            RankingResult: Empty result vá»›i error info
        """
        logger.warning(f"ğŸ”„ Táº¡o empty ranking result (reason: {reason})")
        
        return RankingResult(
            ranked_passages=[],
            original_passage_count=original_count,
            final_passage_count=0,
            ranking_time=0.0,
            query=query,
            statistics={
                'error_info': {
                    'reason': reason,
                    'timestamp': time.time()
                }
            },
            ranking_config=self.config
        )
    
    def _log_ranking_summary(self, result: RankingResult):
        """
        Log comprehensive summary cá»§a ranking process
        
        Args:
            result (RankingResult): Final ranking result
        """
        logger.info("=" * 60)
        logger.info("ğŸ‰ PASSAGE RANKING HOÃ€N THÃ€NH")
        logger.info("=" * 60)
        
        # Basic metrics
        logger.info(f"ğŸ“Š Káº¾T QUáº¢ Tá»”NG QUAN:")
        logger.info(f"   ğŸ“ Query: '{result.query}'")
        logger.info(f"   â±ï¸ Thá»i gian xá»­ lÃ½: {result.ranking_time:.2f} giÃ¢y")
        logger.info(f"   ğŸ“„ Passages: {result.original_passage_count} â†’ {result.final_passage_count}")
        logger.info(f"   ğŸ“ˆ Efficiency: {result.get_ranking_efficiency():.2%}")
        
        # Top ranked passages
        if result.ranked_passages:
            logger.info(f"\nğŸ† TOP 5 RANKED PASSAGES:")
            for i, passage in enumerate(result.ranked_passages[:5], 1):
                logger.info(f"   {i}. {passage.get_summary()}")
                logger.info(f"      ğŸ“ Text: {passage.original_text[:80]}...")
        
        # Support distribution
        support_dist = result.get_support_distribution()
        logger.info(f"\nğŸ“Š PHÃ‚N Bá» SUPPORT:")
        for level, count in support_dist.items():
            percentage = (count / result.final_passage_count * 100) if result.final_passage_count > 0 else 0
            logger.info(f"   {level}: {count} passages ({percentage:.1f}%)")
        
        # Ranking changes
        stats = result.statistics
        if 'ranking_changes' in stats:
            changes = stats['ranking_changes']
            logger.info(f"\nğŸ”„ THAY Äá»”I RANKING:")
            logger.info(f"   â¬†ï¸ Moved up: {changes['passages_moved_up']}")
            logger.info(f"   â¬‡ï¸ Moved down: {changes['passages_moved_down']}")
            logger.info(f"   â¡ï¸ Unchanged: {changes['passages_unchanged']}")
            logger.info(f"   ğŸ“ˆ Max improvement: {changes['max_improvement']} positions")
            logger.info(f"   ğŸ“‰ Max decline: {abs(changes['max_decline'])} positions")
        
        # Performance metrics
        if 'performance_metrics' in stats:
            perf = stats['performance_metrics']
            logger.info(f"\nâš¡ PERFORMANCE:")
            logger.info(f"   ğŸƒ Passages/giÃ¢y: {perf['passages_per_second']:.1f}")
            logger.info(f"   â±ï¸ Avg time/passage: {perf['avg_time_per_passage']:.3f}s")
        
        logger.info("=" * 60)

# ==================== UTILITY FUNCTIONS ====================

def create_default_ranking_config() -> PassageRankerConfig:
    """
    Táº¡o cáº¥u hÃ¬nh máº·c Ä‘á»‹nh cho Passage Ranker
    
    Returns:
        PassageRankerConfig: Default configuration
    """
    logger.info("âš™ï¸ Táº¡o default PassageRankerConfig...")
    
    config = PassageRankerConfig(
        ranking_strategy=RankingStrategy.HYBRID_BALANCED,
        support_calculation_method=SupportCalculationMethod.WEIGHTED_RELEVANCE,
        relevance_threshold=0.3,
        max_passages_output=10,
        enable_score_normalization=True,
        boost_factor_high_support=1.2,
        penalty_factor_no_support=0.8
    )
    
    logger.info("âœ… Default ranking config Ä‘Ã£ Ä‘Æ°á»£c táº¡o")
    return config

def quick_rank_passages(raw_passages: List[RetrievedItem], 
                       filtered_triples: List[FilteredTriple],
                       query: str,
                       strategy: str = "hybrid_balanced") -> RankingResult:
    """
    Quick utility function Ä‘á»ƒ rank passages vá»›i cáº¥u hÃ¬nh Ä‘Æ¡n giáº£n
    
    Args:
        raw_passages (List[RetrievedItem]): Raw passages tá»« Module 1
        filtered_triples (List[FilteredTriple]): Filtered triples tá»« Module 2
        query (str): User query
        strategy (str): Ranking strategy
        
    Returns:
        RankingResult: Ranking result
    """
    logger.info(f"ğŸš€ Quick ranking vá»›i strategy: {strategy}")
    
    # Create config based on strategy
    config = create_default_ranking_config()
    config.ranking_strategy = RankingStrategy(strategy)
    
    # Initialize ranker vÃ  process
    ranker = PassageRanker(config)
    result = ranker.rank_passages(raw_passages, filtered_triples, query)
    
    return result

# ==================== TEST FUNCTIONS ====================

def test_passage_ranking_with_mock_data():
    """Test Passage Ranking vá»›i mock data"""
    print("ğŸ§ª Báº®T Äáº¦U TEST PASSAGE RANKING")
    print("=" * 50)
    
    # Mock raw passages tá»« Module 1
    mock_passages = [
        RetrievedItem(
            item_id="passage_001",
            item_type="passage",
            text="TÃ¡o lÃ  loáº¡i trÃ¡i cÃ¢y ráº¥t tá»‘t cho sá»©c khá»e, chá»©a nhiá»u vitamin C vÃ  cháº¥t xÆ¡.",
            bm25_score=0.8,
            embedding_score=0.7,
            hybrid_score=0.75,
            metadata={'title': 'Lá»£i Ã­ch cá»§a tÃ¡o', 'doc_id': 'health_001'}
        ),
        RetrievedItem(
            item_id="passage_002", 
            item_type="passage",
            text="Vitamin C giÃºp tÄƒng cÆ°á»ng há»‡ miá»…n dá»‹ch vÃ  báº£o vá»‡ cÆ¡ thá»ƒ khá»i bá»‡nh táº­t.",
            bm25_score=0.6,
            embedding_score=0.8,
            hybrid_score=0.7,
            metadata={'title': 'Vitamin C vÃ  miá»…n dá»‹ch', 'doc_id': 'health_002'}
        ),
        RetrievedItem(
            item_id="passage_003",
            item_type="passage", 
            text="Cháº¥t xÆ¡ trong thá»±c pháº©m giÃºp cáº£i thiá»‡n tiÃªu hÃ³a vÃ  kiá»ƒm soÃ¡t cholesterol.",
            bm25_score=0.5,
            embedding_score=0.6,
            hybrid_score=0.55,
            metadata={'title': 'Cháº¥t xÆ¡ vÃ  tiÃªu hÃ³a', 'doc_id': 'health_003'}
        ),
        RetrievedItem(
            item_id="passage_004",
            item_type="passage",
            text="Thá»i tiáº¿t hÃ´m nay ráº¥t Ä‘áº¹p, trá»i náº¯ng vÃ  mÃ¡t máº».",
            bm25_score=0.2,
            embedding_score=0.1, 
            hybrid_score=0.15,
            metadata={'title': 'Thá»i tiáº¿t', 'doc_id': 'weather_001'}
        )
    ]
    
    # Mock filtered triples tá»« Module 2
    mock_filtered_triples = [
        FilteredTriple(
            triple_id="triple_001",
            subject="tÃ¡o",
            predicate="chá»©a",
            object="vitamin C",
            original_text="tÃ¡o chá»©a vitamin C",
            query_relevance_score=0.9,
            relevance_level=RelevanceLevel.HIGHLY_RELEVANT,
            confidence_score=0.85,
            llm_explanation="Triple tráº£ lá»i trá»±c tiáº¿p vá» thÃ nh pháº§n dinh dÆ°á»¡ng cá»§a tÃ¡o",
            source_passage_id="passage_001",
            original_hybrid_retrieval_score=0.75,
            filtering_metadata={}
        ),
        FilteredTriple(
            triple_id="triple_002",
            subject="vitamin C",
            predicate="tÄƒng cÆ°á»ng",
            object="há»‡ miá»…n dá»‹ch",
            original_text="vitamin C tÄƒng cÆ°á»ng há»‡ miá»…n dá»‹ch",
            query_relevance_score=0.8,
            relevance_level=RelevanceLevel.HIGHLY_RELEVANT,
            confidence_score=0.9,
            llm_explanation="Triple giáº£i thÃ­ch lá»£i Ã­ch sá»©c khá»e cá»§a vitamin C",
            source_passage_id="passage_002",
            original_hybrid_retrieval_score=0.7,
            filtering_metadata={}
        ),
        FilteredTriple(
            triple_id="triple_003",
            subject="tÃ¡o",
            predicate="chá»©a",
            object="cháº¥t xÆ¡",
            original_text="tÃ¡o chá»©a cháº¥t xÆ¡",
            query_relevance_score=0.7,
            relevance_level=RelevanceLevel.MODERATELY_RELEVANT,
            confidence_score=0.8,
            llm_explanation="Triple cung cáº¥p thÃ´ng tin bá»• sung vá» thÃ nh pháº§n cá»§a tÃ¡o",
            source_passage_id="passage_001",
            original_hybrid_retrieval_score=0.75,
            filtering_metadata={}
        ),
        FilteredTriple(
            triple_id="triple_004",
            subject="cháº¥t xÆ¡",
            predicate="cáº£i thiá»‡n",
            object="tiÃªu hÃ³a",
            original_text="cháº¥t xÆ¡ cáº£i thiá»‡n tiÃªu hÃ³a",
            query_relevance_score=0.5,
            relevance_level=RelevanceLevel.MODERATELY_RELEVANT,
            confidence_score=0.75,
            llm_explanation="Triple liÃªn quan giÃ¡n tiáº¿p Ä‘áº¿n lá»£i Ã­ch sá»©c khá»e",
            source_passage_id="passage_003",
            original_hybrid_retrieval_score=0.55,
            filtering_metadata={}
        )
    ]
    
    test_query = "Lá»£i Ã­ch cá»§a tÃ¡o cho sá»©c khá»e lÃ  gÃ¬?"
    
    print(f"ğŸ“ Test query: '{test_query}'")
    print(f"ğŸ“„ Mock passages: {len(mock_passages)}")
    print(f"ğŸ”— Mock filtered triples: {len(mock_filtered_triples)}")
    
    try:
        # Test vá»›i different strategies
        strategies = ["hybrid_balanced", "hybrid_support_focused", "retrieval_only", "adaptive"]
        
        for strategy in strategies:
            print(f"\nğŸ¯ Testing vá»›i strategy: {strategy}")
            print("-" * 40)
            
            result = quick_rank_passages(mock_passages, mock_filtered_triples, test_query, strategy)
            
            print(f"   ğŸ“Š Káº¿t quáº£: {result.original_passage_count} â†’ {result.final_passage_count} passages")
            print(f"   â±ï¸ Thá»i gian: {result.ranking_time:.2f}s")
            print(f"   ğŸ“ˆ Efficiency: {result.get_ranking_efficiency():.2%}")
            
            # Show top ranked passages
            print(f"   ğŸ† Top 3 passages:")
            for i, passage in enumerate(result.ranked_passages[:3], 1):
                print(f"      {i}. {passage.get_summary()}")
                print(f"         ğŸ“ Text: {passage.original_text[:60]}...")
    
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        logger.exception("Chi tiáº¿t lá»—i test:")

def test_individual_ranking_components():
    """Test cÃ¡c components riÃªng láº» cá»§a ranking system"""
    print("\nğŸ§ª TEST INDIVIDUAL RANKING COMPONENTS")
    print("=" * 50)
    
    # Test RankingStrategy enum
    print("ğŸ¯ Test RankingStrategy enum:")
    for strategy in RankingStrategy:
        alpha_ret, alpha_sup = strategy.get_weights()
        print(f"   {strategy.value}: retrieval={alpha_ret}, support={alpha_sup}")
    
    # Test SupportCalculationMethod enum
    print("\nğŸ§® Test SupportCalculationMethod enum:")
    for method in SupportCalculationMethod:
        print(f"   {method.value}")
    
    # Test PassageRankerConfig
    print("\nâš™ï¸ Test PassageRankerConfig:")
    config = create_default_ranking_config()
    print(f"   Ranking strategy: {config.ranking_strategy.value}")
    print(f"   Support method: {config.support_calculation_method.value}")
    print(f"   Relevance threshold: {config.relevance_threshold}")
    print(f"   Max output: {config.max_passages_output}")
    print(f"   Score normalization: {config.enable_score_normalization}")
    
    # Test SupportScoreCalculator
    print("\nğŸ“Š Test SupportScoreCalculator:")
    calculator = SupportScoreCalculator(config)
    print(f"   Calculator initialized with method: {config.support_calculation_method.value}")
    
    # Test vá»›i mock triples
    mock_triples = [
        FilteredTriple(
            triple_id="test_triple_1",
            subject="tÃ¡o",
            predicate="chá»©a", 
            object="vitamin C",
            original_text="tÃ¡o chá»©a vitamin C",
            query_relevance_score=0.9,
            relevance_level=RelevanceLevel.HIGHLY_RELEVANT,
            confidence_score=0.85,
            llm_explanation="Test triple",
            source_passage_id="test_passage_1",
            original_hybrid_retrieval_score=0.75,
            filtering_metadata={}
        )
    ]
    
    calculator.build_support_mapping(mock_triples)
    support_score, count, triple_ids = calculator.calculate_support_score("test_passage_1")
    print(f"   Support score calculation: score={support_score:.3f}, count={count}, triples={triple_ids}")
    
    stats = calculator.get_support_statistics()
    print(f"   Support stats: {stats}")

def test_ranking_strategies():
    """Test cÃ¡c ranking strategies vá»›i mock data"""
    print("\nğŸ† TEST RANKING STRATEGIES")
    print("=" * 40)
    
    # Create simple mock data
    mock_passages = [
        RetrievedItem(
            item_id="p1", item_type="passage", text="High retrieval, high support",
            bm25_score=0.9, embedding_score=0.8, hybrid_score=0.85,
            metadata={}
        ),
        RetrievedItem(
            item_id="p2", item_type="passage", text="High retrieval, no support", 
            bm25_score=0.8, embedding_score=0.7, hybrid_score=0.75,
            metadata={}
        ),
        RetrievedItem(
            item_id="p3", item_type="passage", text="Low retrieval, high support",
            bm25_score=0.3, embedding_score=0.4, hybrid_score=0.35,
            metadata={}
        )
    ]
    
    mock_triples = [
        FilteredTriple(
            triple_id="t1", subject="test", predicate="has", object="property",
            original_text="test", query_relevance_score=0.8,
            relevance_level=RelevanceLevel.HIGHLY_RELEVANT, confidence_score=0.9,
            llm_explanation="test", source_passage_id="p1",
            original_hybrid_retrieval_score=0.85, filtering_metadata={}
        ),
        FilteredTriple(
            triple_id="t2", subject="test", predicate="shows", object="benefit",
            original_text="test", query_relevance_score=0.7,
            relevance_level=RelevanceLevel.MODERATELY_RELEVANT, confidence_score=0.8,
            llm_explanation="test", source_passage_id="p3",
            original_hybrid_retrieval_score=0.35, filtering_metadata={}
        )
    ]
    
    strategies = [
        RankingStrategy.RETRIEVAL_ONLY,
        RankingStrategy.SUPPORT_ONLY, 
        RankingStrategy.HYBRID_BALANCED
    ]
    
    for strategy in strategies:
        print(f"\n   ğŸ¯ Strategy: {strategy.value}")
        config = PassageRankerConfig(ranking_strategy=strategy)
        ranker = PassageRanker(config)
        
        result = ranker.rank_passages(mock_passages, mock_triples, "test query")
        
        print(f"      Ranking results:")
        for passage in result.ranked_passages:
            print(f"         {passage.rank}. {passage.passage_id}: final={passage.final_score:.3f} "
                  f"(ret={passage.hybrid_retrieval_score:.3f}, sup={passage.support_score:.3f})")

if __name__ == "__main__":
    print("ğŸš€ Báº®T Äáº¦U CHáº Y Táº¤T Cáº¢ TESTS CHO MODULE 3")
    print("=" * 60)
    
    # Test 1: Individual components
    test_individual_ranking_components()
    
    print("\n" + "=" * 60)
    
    # Test 2: Ranking strategies
    test_ranking_strategies()
    
    print("\n" + "=" * 60)
    
    # Test 3: Full ranking vá»›i mock data
    print("ğŸ¯ Test tiáº¿p theo sá»­ dá»¥ng comprehensive mock data")
    test_passage_ranking_with_mock_data()
    
    print("\nğŸ‰ HOÃ€N THÃ€NH Táº¤T Cáº¢ TESTS CHO MODULE 3!")
    print("=" * 60)
