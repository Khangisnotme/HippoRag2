"""
OfflineIndexing/module3_sysnonym_detector.py
Module 3: Synonym Detector  
Sá»­ dá»¥ng sentence-transformers Ä‘á»ƒ detect synonyms giá»¯a cÃ¡c phrases
"""

from pathlib import Path
from typing import List, Dict, Tuple, Set, Any
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import logging
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass  
class SynonymPair:
    """
    Data class Ä‘á»ƒ lÆ°u trá»¯ thÃ´ng tin cá»§a má»™t cáº·p tá»« Ä‘á»“ng nghÄ©a
    
    Attributes:
        phrase1 (str): Cá»¥m tá»« thá»© nháº¥t
        phrase2 (str): Cá»¥m tá»« thá»© hai
        similarity_score (float): Äiá»ƒm tÆ°Æ¡ng tá»± giá»¯a hai cá»¥m tá»« (0-1)
    """
    phrase1: str
    phrase2: str
    similarity_score: float

class SynonymDetector:
    """
    Class chÃ­nh Ä‘á»ƒ detect synonyms sá»­ dá»¥ng embedding similarity
    
    Workflow:
    1. Initialize model sentence-transformers
    2. Extract unique phrases tá»« triples
    3. Generate embeddings cho táº¥t cáº£ phrases
    4. Compute similarity matrix
    5. Find synonym pairs dá»±a trÃªn threshold
    6. Create synonym mapping cho canonical forms
    """
    
    def __init__(self, model_name: str = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"):
        """
        Initialize SynonymDetector vá»›i multilingual sentence transformer
        
        Args:
            model_name (str): TÃªn model sentence-transformers Ä‘á»ƒ sá»­ dá»¥ng
                            Default: paraphrase-multilingual-mpnet-base-v2 
                            (há»— trá»£ Ä‘a ngÃ´n ngá»¯, tá»‘t cho tiáº¿ng Viá»‡t)
        
        Initializes:
            - self.model: SentenceTransformer model
            - self.synonym_pairs: List lÆ°u cÃ¡c cáº·p synonyms Ä‘Ã£ tÃ¬m Ä‘Æ°á»£c
            - self.phrase_embeddings: Dict cache embeddings Ä‘Ã£ tÃ­nh
        """
        logger.info(f"Initializing SynonymDetector with model: {model_name}")
        self.model_name = model_name
        # Load pre-trained sentence transformer model
        self.model = SentenceTransformer(model_name)
        # List Ä‘á»ƒ lÆ°u cÃ¡c synonym pairs Ä‘Ã£ detect Ä‘Æ°á»£c
        self.synonym_pairs = []
        # Dict Ä‘á»ƒ cache embeddings Ä‘Ã£ tÃ­nh (trÃ¡nh tÃ­nh láº¡i)
        self.phrase_embeddings = {}
        logger.info("Model loaded successfully")
        
    def detect_synonyms_from_triples(self, triples: List[Any], similarity_threshold: float = 0.85) -> List[SynonymPair]:
        """
        Main function Ä‘á»ƒ detect synonyms tá»« extracted triples
        
        Args:
            triples (List[Any]): List cÃ¡c triple objects cÃ³ attributes .subject vÃ  .object
            similarity_threshold (float): NgÆ°á»¡ng similarity Ä‘á»ƒ coi lÃ  synonym (0-1)
                                        Default: 0.85 (khÃ¡ strict)
        
        Returns:
            List[SynonymPair]: List cÃ¡c cáº·p synonyms Ä‘Ã£ tÃ¬m Ä‘Æ°á»£c
            
        Process:
            1. Extract all unique phrases tá»« subjects vÃ  objects
            2. Filter phrases (loáº¡i bá» phrases quÃ¡ ngáº¯n)
            3. Call detect_synonyms() Ä‘á»ƒ tÃ¬m synonym pairs
        """
        logger.info(f"Starting synonym detection from {len(triples)} triples")
        
        # STEP 1: Extract all unique phrases tá»« triples
        all_phrases = set()
        
        for triple in triples:
            # Láº¥y subject vÃ  object, strip whitespace vÃ  lowercase Ä‘á»ƒ normalize
            all_phrases.add(triple.subject.strip().lower())
            all_phrases.add(triple.object.strip().lower())
        
        all_phrases = list(all_phrases)
        logger.debug(f"Found {len(all_phrases)} unique phrases")
        
        # STEP 2: Filter phrases - loáº¡i bá» phrases quÃ¡ ngáº¯n (< 3 chars)
        # VÃ¬ phrases ngáº¯n thÆ°á»ng khÃ´ng cÃ³ Ã½ nghÄ©a rÃµ rÃ ng
        filtered_phrases = [phrase for phrase in all_phrases if len(phrase) > 2]
        logger.debug(f"Filtered to {len(filtered_phrases)} valid phrases (length > 2)")
        
        logger.info(f"Detecting synonyms among {len(filtered_phrases)} unique phrases")
        
        # STEP 3: Gá»i detect_synonyms Ä‘á»ƒ tÃ¬m synonym pairs
        synonym_pairs = self.detect_synonyms(filtered_phrases, similarity_threshold)
        
        # LÆ°u káº¿t quáº£ vÃ o instance variable
        self.synonym_pairs = synonym_pairs
        logger.info(f"Found {len(synonym_pairs)} synonym pairs")
        return synonym_pairs
    
    def detect_synonyms(self, phrases: List[str], similarity_threshold: float = 0.85) -> List[SynonymPair]:
        """
        Core function Ä‘á»ƒ detect synonyms trong list of phrases
        
        Args:
            phrases (List[str]): List cÃ¡c phrases cáº§n tÃ¬m synonyms
            similarity_threshold (float): NgÆ°á»¡ng similarity (0-1)
        
        Returns:
            List[SynonymPair]: List cÃ¡c synonym pairs
            
        Algorithm:
            1. Generate embeddings cho táº¥t cáº£ phrases
            2. Compute similarity matrix (cosine similarity)
            3. Find pairs cÃ³ similarity >= threshold
            4. Avoid duplicate pairs
        """
        # Kiá»ƒm tra input Ä‘á»§ Ä‘á»ƒ so sÃ¡nh
        if len(phrases) < 2:
            logger.warning("Not enough phrases for synonym detection")
            return []
        
        # STEP 1: Generate embeddings cho táº¥t cáº£ phrases
        logger.info("Generating embeddings for phrases...")
        # model.encode() convert text thÃ nh vector embeddings
        # show_progress_bar=True Ä‘á»ƒ hiá»ƒn thá»‹ progress
        embeddings = self.model.encode(phrases, show_progress_bar=True)
        logger.debug(f"Generated {len(embeddings)} embeddings")
        
        # STEP 2: Store embeddings vÃ o cache Ä‘á»ƒ reuse
        for phrase, embedding in zip(phrases, embeddings):
            self.phrase_embeddings[phrase] = embedding
        logger.debug(f"Stored embeddings for {len(self.phrase_embeddings)} phrases")
        
        # STEP 3: Compute similarity matrix
        logger.info("Computing similarity matrix...")
        # Cosine similarity giá»¯a táº¥t cáº£ cáº·p embeddings
        # Káº¿t quáº£: matrix NxN vá»›i similarity[i][j] = similarity giá»¯a phrase i vÃ  j
        similarity_matrix = cosine_similarity(embeddings)
        logger.debug(f"Similarity matrix shape: {similarity_matrix.shape}")
        
        # STEP 4: Find synonym pairs
        synonym_pairs = []
        processed_pairs = set()  # Äá»ƒ trÃ¡nh duplicate pairs
        
        logger.info("Finding synonym pairs...")
        # Nested loop Ä‘á»ƒ kiá»ƒm tra táº¥t cáº£ cáº·p phrases
        for i in range(len(phrases)):
            for j in range(i + 1, len(phrases)):  # j > i Ä‘á»ƒ trÃ¡nh duplicate vÃ  self-comparison
                similarity = similarity_matrix[i][j]
                
                # Náº¿u similarity >= threshold thÃ¬ coi lÃ  synonym
                if similarity >= similarity_threshold:
                    phrase1, phrase2 = phrases[i], phrases[j]
                    
                    # STEP 5: Avoid duplicate pairs
                    # Táº¡o pair_key sorted Ä‘á»ƒ trÃ¡nh (A,B) vÃ  (B,A) Ä‘á»u Ä‘Æ°á»£c thÃªm
                    pair_key = tuple(sorted([phrase1, phrase2]))
                    if pair_key not in processed_pairs:
                        # Táº¡o SynonymPair object
                        synonym_pair = SynonymPair(
                            phrase1=phrase1,
                            phrase2=phrase2,
                            similarity_score=float(similarity)
                        )
                        synonym_pairs.append(synonym_pair)
                        processed_pairs.add(pair_key)
                        logger.debug(f"Found synonym pair: {phrase1} â‰ˆ {phrase2} (score: {similarity:.3f})")
        
        logger.info(f"Found {len(synonym_pairs)} synonym pairs with threshold {similarity_threshold}")
        return synonym_pairs
    
    def create_synonym_mapping(self, synonym_pairs: List[SynonymPair] = None) -> Dict[str, str]:
        """
        Create mapping tá»« synonyms vá» canonical form (dáº¡ng chuáº©n)
        
        Args:
            synonym_pairs (List[SynonymPair]): List synonym pairs, náº¿u None thÃ¬ dÃ¹ng self.synonym_pairs
        
        Returns:
            Dict[str, str]: Mapping tá»« phrase -> canonical_phrase
            
        Algorithm:
            1. Build graph of synonyms (undirected graph)
            2. Find connected components (synonym groups)
            3. Choose canonical form cho má»—i group (shortest phrase)
            4. Create mapping dict
            
        Example:
            Input: [("diego costa", "costa"), ("costa", "diego da silva costa")]
            Output: {"diego costa": "costa", "diego da silva costa": "costa", "costa": "costa"}
        """
        logger.info("Creating synonym mapping...")
        if synonym_pairs is None:
            synonym_pairs = self.synonym_pairs
            
        # STEP 1: Build graph of synonyms
        # Graph represented as adjacency list: {phrase: set_of_connected_phrases}
        synonym_graph = {}
        
        for pair in synonym_pairs:
            # Initialize empty sets if phrases not in graph
            if pair.phrase1 not in synonym_graph:
                synonym_graph[pair.phrase1] = set()
            if pair.phrase2 not in synonym_graph:
                synonym_graph[pair.phrase2] = set()
            
            # Add bidirectional edges (undirected graph)
            synonym_graph[pair.phrase1].add(pair.phrase2)
            synonym_graph[pair.phrase2].add(pair.phrase1)
        
        logger.debug(f"Built synonym graph with {len(synonym_graph)} nodes")
        
        # STEP 2: Find connected components (synonym groups) using DFS
        visited = set()
        synonym_groups = []
        
        def dfs(node, current_group):
            """
            Depth-First Search Ä‘á»ƒ tÃ¬m connected component
            
            Args:
                node (str): Current node trong DFS
                current_group (set): Set lÆ°u táº¥t cáº£ nodes trong component hiá»‡n táº¡i
            """
            if node in visited:
                return
            visited.add(node)
            current_group.add(node)
            
            # Recursive DFS cho táº¥t cáº£ neighbors
            for neighbor in synonym_graph.get(node, set()):
                dfs(neighbor, current_group)
        
        # TÃ¬m táº¥t cáº£ connected components
        for phrase in synonym_graph:
            if phrase not in visited:
                group = set()
                dfs(phrase, group)
                # Chá»‰ thÃªm groups cÃ³ > 1 phrase (tá»©c cÃ³ synonyms)
                if len(group) > 1:
                    synonym_groups.append(group)
                    logger.debug(f"Found synonym group: {group}")
        
        # STEP 3: Create mapping to canonical forms
        synonym_mapping = {}
        
        for group in synonym_groups:
            # Choose shortest phrase as canonical (assumption: shorter = more common)
            canonical = min(group, key=len)
            
            # Map táº¥t cáº£ phrases trong group vá» canonical
            for phrase in group:
                synonym_mapping[phrase] = canonical
                logger.debug(f"Mapped {phrase} â†’ {canonical}")
        
        logger.info(f"Created {len(synonym_groups)} synonym groups with {len(synonym_mapping)} mappings")
        return synonym_mapping
    
    def get_phrase_embedding(self, phrase: str) -> np.ndarray:
        """
        Get embedding cho má»™t phrase, sá»­ dá»¥ng cache náº¿u cÃ³
        
        Args:
            phrase (str): Phrase cáº§n tÃ­nh embedding
            
        Returns:
            np.ndarray: Vector embedding cá»§a phrase
            
        Note:
            - Sá»­ dá»¥ng cache Ä‘á»ƒ trÃ¡nh tÃ­nh láº¡i embeddings Ä‘Ã£ cÃ³
            - Embedding Ä‘Æ°á»£c normalize bá»Ÿi sentence-transformers
        """
        # Kiá»ƒm tra cache trÆ°á»›c
        if phrase in self.phrase_embeddings:
            logger.debug(f"Retrieved cached embedding for phrase: {phrase}")
            return self.phrase_embeddings[phrase]
        else:
            # TÃ­nh embedding má»›i
            logger.debug(f"Computing new embedding for phrase: {phrase}")
            # model.encode() tráº£ vá» array, láº¥y element Ä‘áº§u tiÃªn
            embedding = self.model.encode([phrase])[0]
            # LÆ°u vÃ o cache
            self.phrase_embeddings[phrase] = embedding
            return embedding
    
    def find_similar_phrases(self, target_phrase: str, candidate_phrases: List[str], 
                           top_k: int = 5) -> List[Tuple[str, float]]:
        """
        Find top-k phrases most similar to target phrase
        
        Args:
            target_phrase (str): Phrase Ä‘á»ƒ tÃ¬m similar
            candidate_phrases (List[str]): List phrases Ä‘á»ƒ so sÃ¡nh
            top_k (int): Sá»‘ lÆ°á»£ng top results tráº£ vá»
            
        Returns:
            List[Tuple[str, float]]: List (phrase, similarity_score) sorted by similarity desc
            
        ğŸš¨ BUG ALERT: DÃ²ng similarity calculation cÃ³ bug!
        """
        logger.info(f"Finding top-{top_k} similar phrases to: {target_phrase}")
        target_embedding = self.get_phrase_embedding(target_phrase)
        
        similarities = []
        for phrase in candidate_phrases:
            if phrase != target_phrase:  # Skip self-comparison
                phrase_embedding = self.get_phrase_embedding(phrase)
                # ğŸš¨ BUG: [2][0] should be [0][0] 
                # cosine_similarity returns 1x1 matrix, not 3x1
                similarity = float(cosine_similarity([target_embedding], [phrase_embedding])[0][0])
                similarities.append((phrase, similarity))
                logger.debug(f"Similarity with {phrase}: {similarity:.3f}")
        
        # Sort by similarity descending (highest similarity first)
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        logger.info(f"Found {len(similarities)} similar phrases")
        return similarities[:top_k]
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        Get comprehensive statistics vá» synonym detection results
        
        Returns:
            Dict[str, Any]: Statistics dictionary with keys:
                - total_synonym_pairs: Tá»•ng sá»‘ synonym pairs
                - avg_similarity: Similarity trung bÃ¬nh
                - min_similarity: Similarity tháº¥p nháº¥t
                - max_similarity: Similarity cao nháº¥t  
                - unique_phrases_with_synonyms: Sá»‘ phrases unique cÃ³ synonyms
        """
        if not self.synonym_pairs:
            logger.warning("No synonym pairs detected yet")
            return {}
        
        # Extract similarity scores tá»« táº¥t cáº£ pairs
        similarities = [pair.similarity_score for pair in self.synonym_pairs]
        
        # Calculate statistics
        stats = {
            'total_synonym_pairs': len(self.synonym_pairs),
            'avg_similarity': np.mean(similarities),
            'min_similarity': np.min(similarities),
            'max_similarity': np.max(similarities),
            # Count unique phrases cÃ³ synonyms (phrase1 + phrase2 tá»« táº¥t cáº£ pairs)
            'unique_phrases_with_synonyms': len(set([pair.phrase1 for pair in self.synonym_pairs] + 
                                                  [pair.phrase2 for pair in self.synonym_pairs]))
        }
        
        logger.info(f"Synonym detection statistics: {stats}")
        return stats
    
    def save_synonyms_to_file(self, output_path: Path):
        """
        Save synonym pairs to TSV file Ä‘á»ƒ review vÃ  debug
        
        Args:
            output_path (Path): Path Ä‘á»ƒ save file
            
        Output format:
            TSV file vá»›i columns: Phrase1, Phrase2, Similarity_Score
        """
        logger.info(f"Saving {len(self.synonym_pairs)} synonym pairs to {output_path}")
        with open(output_path, 'w', encoding='utf-8') as f:
            # Write header
            f.write("Phrase1\tPhrase2\tSimilarity_Score\n")
            # Write data rows
            for pair in self.synonym_pairs:
                f.write(f"{pair.phrase1}\t{pair.phrase2}\t{pair.similarity_score:.4f}\n")
        logger.info("Synonym pairs saved successfully")

# Test function
def test_synonym_detector():
    """
    Test function cho Module 3 - Demo basic functionality
    
    Test case: Chemical/Vietnamese phrases cÃ³ synonyms rÃµ rÃ ng
    Expected synonyms:
        - "axÃ­t" â‰ˆ "axit" (Vietnamese spelling variants)
        - "kiá»m" â‰ˆ "bazÆ¡" (chemical synonyms)  
        - "dung dá»‹ch axÃ­t" â‰ˆ "dung dá»‹ch axit"
        - "pH nhá» hÆ¡n 7" â‰ˆ "pH < 7" (mathematical notation)
    """
    logger.info("Starting synonym detector test")
    detector = SynonymDetector()
    
    # Test phrases with obvious synonyms
    test_phrases = [
        "axÃ­t",           # Vietnamese spelling 1
        "axit",           # Vietnamese spelling 2  
        "kiá»m",           # Vietnamese
        "bazÆ¡",           # Vietnamese alternative
        "dung dá»‹ch axÃ­t", # Compound phrase 1
        "dung dá»‹ch axit", # Compound phrase 2
        "pH nhá» hÆ¡n 7",   # Text form
        "pH < 7"          # Symbol form
    ]
    
    print("ğŸ”— Testing Synonym Detection...")
    synonym_pairs = detector.detect_synonyms(test_phrases, similarity_threshold=0.8)
    
    print("ğŸ“Š Synonym Detection Results:")
    for pair in synonym_pairs:
        print(f"  {pair.phrase1} â‰ˆ {pair.phrase2} (score: {pair.similarity_score:.3f})")
    
    # Test mapping creation
    mapping = detector.create_synonym_mapping(synonym_pairs)
    print("\nğŸ“‹ Synonym Mapping:")
    for original, canonical in mapping.items():
        print(f"  {original} â†’ {canonical}")
    
    logger.info("Synonym detector test completed")
    return synonym_pairs

if __name__ == "__main__":
    test_synonym_detector()
