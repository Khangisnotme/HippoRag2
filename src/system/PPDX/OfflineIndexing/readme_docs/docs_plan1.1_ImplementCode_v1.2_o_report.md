# Chá»‘t 1 cÃ¡i giáº£i phÃ¡p nhÆ° nÃ y: 
- Má»¥c tiÃªu xá»­ lÃ½ viá»‡c thá»«a vÃ  thiáº¿u thÃ´ng tin (thiáº¿u trong quÃ¡ trÃ¬nh cáº§n multi-hop).

Pha Offline: 

```bash
Input Text â†’ OpenIE (Qwen2.5-7B) â†’ Triple Extraction â†’ Synonym Detection (multilingual-mpnet) â†’ Knowledge Graph
```

Pha Online:

```bash
MÃ´ táº£ luá»“ng 
1. BM25 + Embedding â†’ ğŸ“„ Raw Passages  vÃ  BM25 + Embedding â†’ ğŸ”— Raw Triples
2. Filted triples 
3. Má»Ÿ rá»™ng triples  (multi-hop)
4. Má»Ÿ rá»™ng Passages vÃ  Filter passages
4. Gom passages má»Ÿ rá»™ng vÃ  triples má»Ÿ rá»™ng vÃ o context 

```

## Pha offline: 
- 1 cÃ¡ch Ä‘Æ¡n giáº£n lÃ : lÆ°u paragrah vÃ  triples/facts dÆ°á»›i dáº¡ng 2 collections Ä‘Æ¡n giáº£n: 
+, Collection 1: Paragraph (cÃ³ metadata lÃ  cÃ¡c triples)
+, Collection 2: CÃ¡c triples/CÃ¡c facts (cÃ³ metadata lÃ  cÃ¡c paragraph)
+, Thuáº­t toÃ¡n lan truÃªá»n chá»‰ Ä‘Æ¡n giáº£n lÃ : paragraph, triples lan truyá»n 1 cÃ¡ch ráº¥t Ä‘Æ¡n giáº£n (cÃ³ 2 nodes lÃ : paragrah vÃ  triples/facts)
(thay vÃ¬ 3 nodes lÃ : Phrase Node vÃ  Paragraph Node). 

- CÃ¡ch 2: váº«n giá»‘ng há»‡t cÃ¡ch triá»ƒn khai cá»§a HippoRAG2: 
2 node: Phrase Node, Passage Node (Phrase node gá»“m Subject and Object). 
3 Cáº¡nh: Relation Edge (giá»¯a 2 phrase node), synonym edge (giá»¯a 2 phrase node), contain Edge (giá»¯a Passage vÃ  Phrase node cá»§a nÃ³)

## Pha Onl sáº½ hoáº¡t Ä‘á»™ng dá»±a trÃªn cÃ¡ch lÃ m cá»§a pha Offline. 

---

# PhÃ¢n tÃ­ch So sÃ¡nh Hai PhÆ°Æ¡ng Ã¡n Triá»ƒn khai Pha Offline

Báº¡n Ä‘Ã£ Ä‘á» xuáº¥t má»™t giáº£i phÃ¡p tá»•ng thá»ƒ cho RAG nháº±m xá»­ lÃ½ váº¥n Ä‘á» thá»«a/thiáº¿u thÃ´ng tin vÃ  há»— trá»£ multi-hop reasoning. Trong Ä‘Ã³, báº¡n Ä‘ang phÃ¢n vÃ¢n giá»¯a hai cÃ¡ch triá»ƒn khai cho pha Offline (xÃ¢y dá»±ng kho kiáº¿n thá»©c). DÆ°á»›i Ä‘Ã¢y lÃ  phÃ¢n tÃ­ch chi tiáº¿t vá» hai phÆ°Æ¡ng Ã¡n nÃ y:

## PhÆ°Æ¡ng Ã¡n 1: Sá»­ dá»¥ng Hai Collections ÄÆ¡n giáº£n

*   **Cáº¥u trÃºc:**
    *   **Collection 1 (Paragraphs):** LÆ°u trá»¯ ná»™i dung cÃ¡c Ä‘oáº¡n vÄƒn (passages). Má»—i passage sáº½ cÃ³ metadata chá»©a danh sÃ¡ch cÃ¡c triples/facts Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« chÃ­nh passage Ä‘Ã³.
    *   **Collection 2 (Triples/Facts):** LÆ°u trá»¯ cÃ¡c triples/facts Ä‘Ã£ trÃ­ch xuáº¥t. Má»—i triple/fact sáº½ cÃ³ metadata chá»©a tham chiáº¿u Ä‘áº¿n (cÃ¡c) passage gá»‘c chá»©a nÃ³.
*   **CÃ¡ch hoáº¡t Ä‘á»™ng:**
    *   Khi truy váº¥n, cÃ³ thá»ƒ tÃ¬m kiáº¿m trÃªn cáº£ hai collection. VÃ­ dá»¥: tÃ¬m passages liÃªn quan báº±ng BM25/Embedding, sau Ä‘Ã³ láº¥y cÃ¡c triples tá»« metadata cá»§a passages Ä‘Ã³. Hoáº·c, tÃ¬m triples liÃªn quan trá»±c tiáº¿p Ä‘áº¿n truy váº¥n, rá»“i láº¥y cÃ¡c passages gá»‘c tá»« metadata cá»§a triples.
    *   Viá»‡c "lan truyá»n" hay multi-hop reasoning sáº½ dá»±a trÃªn viá»‡c liÃªn káº¿t qua láº¡i giá»¯a hai collection nÃ y. VÃ­ dá»¥: tá»« má»™t passage tÃ¬m Ä‘Æ°á»£c, láº¥y ra triple, tÃ¬m cÃ¡c passages khÃ¡c chá»©a cÃ¹ng thá»±c thá»ƒ trong triple Ä‘Ã³, hoáº·c tÃ¬m cÃ¡c triples khÃ¡c liÃªn quan Ä‘áº¿n thá»±c thá»ƒ trong triple ban Ä‘áº§u, rá»“i láº¡i tÃ¬m passages tÆ°Æ¡ng á»©ng.
*   **Æ¯u Ä‘iá»ƒm:**
    *   **ÄÆ¡n giáº£n:** Cáº¥u trÃºc lÆ°u trá»¯ tÆ°Æ¡ng Ä‘á»‘i Ä‘Æ¡n giáº£n, cÃ³ thá»ƒ triá»ƒn khai dá»… dÃ ng báº±ng cÃ¡c há»‡ quáº£n trá»‹ cÆ¡ sá»Ÿ dá»¯ liá»‡u thÃ´ng thÆ°á»ng hoáº·c cÃ¡c vector database há»— trá»£ metadata.
    *   **Quen thuá»™c:** MÃ´ hÃ¬nh lÆ°u trá»¯ dáº¡ng collection khÃ¡ phá»• biáº¿n vÃ  dá»… quáº£n lÃ½.
*   **NhÆ°á»£c Ä‘iá»ƒm:**
    *   **KhÃ³ khÄƒn vá»›i Multi-hop Phá»©c táº¡p:** Viá»‡c thá»±c hiá»‡n cÃ¡c bÆ°á»›c lan truyá»n (multi-hop) phá»©c táº¡p cÃ³ thá»ƒ trá»Ÿ nÃªn cháº­m vÃ  cá»“ng ká»nh vÃ¬ Ä‘Ã²i há»i nhiá»u lÆ°á»£t truy váº¥n vÃ  join dá»¯ liá»‡u giá»¯a hai collection riÃªng biá»‡t. Viá»‡c biá»ƒu diá»…n cÃ¡c má»‘i quan há»‡ phá»©c táº¡p hÆ¡n (vÃ­ dá»¥: synonym, quan há»‡ phÃ¢n cáº¥p giá»¯a cÃ¡c thá»±c thá»ƒ) khÃ´ng tÆ°á»ng minh.
    *   **Hiá»‡u suáº¥t Truy váº¥n Lan truyá»n:** CÃ¡c truy váº¥n Ä‘Ã²i há»i nhiá»u bÆ°á»›c nháº£y qua láº¡i giá»¯a passages vÃ  triples cÃ³ thá»ƒ khÃ´ng hiá»‡u quáº£ báº±ng viá»‡c duyá»‡t trá»±c tiáº¿p trÃªn má»™t cáº¥u trÃºc Ä‘á»“ thá»‹.

## PhÆ°Æ¡ng Ã¡n 2: Sá»­ dá»¥ng Cáº¥u trÃºc Graph (TÆ°Æ¡ng tá»± HippoRAG2)

*   **Cáº¥u trÃºc:**
    *   **Nodes:**
        *   `Passage Node`: Äáº¡i diá»‡n cho má»™t Ä‘oáº¡n vÄƒn.
        *   `Phrase Node`: Äáº¡i diá»‡n cho má»™t thá»±c thá»ƒ (Subject hoáº·c Object trong triple).
    *   **Edges:**
        *   `Contain Edge`: Ná»‘i `Passage Node` vá»›i cÃ¡c `Phrase Node` (thá»±c thá»ƒ) mÃ  nÃ³ chá»©a.
        *   `Relation Edge`: Ná»‘i hai `Phrase Node` (Subject vÃ  Object) Ä‘á»ƒ biá»ƒu diá»…n má»™t triple/fact (Predicate cÃ³ thá»ƒ lÃ  thuá»™c tÃ­nh cá»§a cáº¡nh nÃ y). Cáº¡nh cÃ³ hÆ°á»›ng tá»« Subject tá»›i Object (Subject vÃ  Object lÃ  2 phrase node, relation edge cÃ³ hÆ°á»›ng lÃ  cáº¡nh ná»‘i tá»« subject tá»›i object)
        *   `Synonym Edge`: Ná»‘i cÃ¡c `Phrase Node` lÃ  tá»« Ä‘á»“ng nghÄ©a vá»›i nhau dá»±a trÃªn Embedding score (khÃ´ng pháº£i match text)
*   **CÃ¡ch hoáº¡t Ä‘á»™ng:**
    *   ToÃ n bá»™ kiáº¿n thá»©c (passages, thá»±c thá»ƒ, má»‘i quan há»‡, tá»« Ä‘á»“ng nghÄ©a) Ä‘Æ°á»£c biá»ƒu diá»…n trong má»™t Ä‘á»“ thá»‹ duy nháº¥t.
    *   Truy váº¥n vÃ  multi-hop reasoning Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡c thuáº­t toÃ¡n duyá»‡t Ä‘á»“ thá»‹ (graph traversal). VÃ­ dá»¥: báº¯t Ä‘áº§u tá»« má»™t `Phrase Node` (thá»±c thá»ƒ trong truy váº¥n), Ä‘i theo `Relation Edge` Ä‘á»ƒ tÃ¬m cÃ¡c facts liÃªn quan, Ä‘i theo `Contain Edge` Ä‘á»ƒ tÃ¬m cÃ¡c passages chá»©a thá»±c thá»ƒ Ä‘Ã³, Ä‘i theo `Synonym Edge` Ä‘á»ƒ má»Ÿ rá»™ng tÃ¬m kiáº¿m.
*   **Æ¯u Ä‘iá»ƒm:**
    *   **Biá»ƒu diá»…n TÆ°á»ng minh:** CÃ¡c má»‘i quan há»‡ giá»¯a passages, thá»±c thá»ƒ, vÃ  facts Ä‘Æ°á»£c biá»ƒu diá»…n má»™t cÃ¡ch rÃµ rÃ ng vÃ  trá»±c quan trong cáº¥u trÃºc Ä‘á»“ thá»‹.
    *   **Há»— trá»£ Multi-hop Tá»± nhiÃªn:** Cáº¥u trÃºc Ä‘á»“ thá»‹ vá»‘n Ä‘Æ°á»£c thiáº¿t káº¿ cho viá»‡c duyá»‡t cÃ¡c má»‘i quan há»‡ phá»©c táº¡p vÃ  thá»±c hiá»‡n multi-hop reasoning má»™t cÃ¡ch hiá»‡u quáº£ thÃ´ng qua cÃ¡c thuáº­t toÃ¡n graph traversal.
    *   **Linh hoáº¡t:** Dá»… dÃ ng má»Ÿ rá»™ng Ä‘á»“ thá»‹ vá»›i cÃ¡c loáº¡i node vÃ  edge má»›i Ä‘á»ƒ biá»ƒu diá»…n cÃ¡c loáº¡i kiáº¿n thá»©c hoáº·c má»‘i quan há»‡ phá»©c táº¡p hÆ¡n.
*   **NhÆ°á»£c Ä‘iá»ƒm:**
    *   **Phá»©c táº¡p hÆ¡n:** Viá»‡c xÃ¢y dá»±ng vÃ  quáº£n lÃ½ má»™t knowledge graph Ä‘Ã²i há»i kiáº¿n thá»©c vá» graph database (nhÆ° Neo4j, NebulaGraph) hoáº·c cÃ¡c thÆ° viá»‡n xá»­ lÃ½ graph chuyÃªn dá»¥ng. QuÃ¡ trÃ¬nh xÃ¢y dá»±ng graph (trÃ­ch xuáº¥t, chuáº©n hÃ³a thá»±c thá»ƒ, táº¡o cáº¡nh) phá»©c táº¡p hÆ¡n.
    *   **Chi phÃ­ Háº¡ táº§ng:** CÃ³ thá»ƒ yÃªu cáº§u háº¡ táº§ng riÃªng cho graph database.

## So sÃ¡nh vÃ  ÄÃ¡nh giÃ¡

| TiÃªu chÃ­                     | PhÆ°Æ¡ng Ã¡n 1 (Collections)                     | PhÆ°Æ¡ng Ã¡n 2 (Graph)                           |
| :--------------------------- | :--------------------------------------------- | :--------------------------------------------- |
| **Äá»™ phá»©c táº¡p triá»ƒn khai**  | Tháº¥p hÆ¡n                                       | Cao hÆ¡n                                        |
| **Biá»ƒu diá»…n quan há»‡**      | GiÃ¡n tiáº¿p (qua metadata)                      | Trá»±c tiáº¿p, tÆ°á»ng minh (qua nodes/edges)        |
| **Há»— trá»£ Multi-hop**       | CÃ³ thá»ƒ, nhÆ°ng tiá»m áº©n phá»©c táº¡p/cháº­m          | Tá»± nhiÃªn, hiá»‡u quáº£ (graph traversal)          |
| **Xá»­ lÃ½ Thá»«a/Thiáº¿u thÃ´ng tin** | Phá»¥ thuá»™c vÃ o logic truy váº¥n giá»¯a collections | Dá»… dÃ ng hÆ¡n Ä‘á»ƒ lá»c/má»Ÿ rá»™ng dá»±a trÃªn duyá»‡t graph |
| **Kháº£ nÄƒng má»Ÿ rá»™ng cáº¥u trÃºc** | Háº¡n cháº¿ hÆ¡n                                   | Linh hoáº¡t hÆ¡n                                  |
| **YÃªu cáº§u háº¡ táº§ng**        | CÆ¡ sá»Ÿ dá»¯ liá»‡u/Vector DB thÃ´ng thÆ°á»ng          | CÃ³ thá»ƒ cáº§n Graph DB chuyÃªn dá»¥ng               |

## Káº¿t luáº­n vÃ  Khuyáº¿n nghá»‹

*   **PhÆ°Æ¡ng Ã¡n 1 (Collections):** PhÃ¹ há»£p náº¿u báº¡n Æ°u tiÃªn sá»± **Ä‘Æ¡n giáº£n trong triá»ƒn khai ban Ä‘áº§u** vÃ  cÃ¡c yÃªu cáº§u vá» multi-hop reasoning khÃ´ng quÃ¡ phá»©c táº¡p (vÃ­ dá»¥: chá»‰ cáº§n 1-2 hop). Tuy nhiÃªn, khi Ä‘á»™ phá»©c táº¡p cá»§a má»‘i quan há»‡ vÃ  sá»‘ bÆ°á»›c nháº£y tÄƒng lÃªn, cÃ¡ch tiáº¿p cáº­n nÃ y cÃ³ thá»ƒ gáº·p háº¡n cháº¿ vá» hiá»‡u suáº¥t vÃ  kháº£ nÄƒng biá»ƒu diá»…n.
*   **PhÆ°Æ¡ng Ã¡n 2 (Graph):** LÃ  lá»±a chá»n **máº¡nh máº½ vÃ  phÃ¹ há»£p hÆ¡n** náº¿u má»¥c tiÃªu chÃ­nh lÃ  **há»— trá»£ multi-hop reasoning phá»©c táº¡p** vÃ  xá»­ lÃ½ linh hoáº¡t viá»‡c thiáº¿u/thá»«a thÃ´ng tin thÃ´ng qua cÃ¡c má»‘i quan há»‡ ngá»¯ nghÄ©a tÆ°á»ng minh. Máº·c dÃ¹ Ä‘Ã²i há»i Ä‘áº§u tÆ° ban Ä‘áº§u vá» ká»¹ thuáº­t vÃ  háº¡ táº§ng cao hÆ¡n, nÃ³ cung cáº¥p ná»n táº£ng vá»¯ng cháº¯c vÃ  hiá»‡u quáº£ hÆ¡n cho cÃ¡c tÃ¡c vá»¥ suy luáº­n trÃªn kiáº¿n thá»©c trong dÃ i háº¡n, Ä‘áº·c biá»‡t lÃ  khi mÃ´ phá»ng theo kiáº¿n trÃºc Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh nhÆ° HippoRAG.

**Khuyáº¿n nghá»‹:** Dá»±a trÃªn má»¥c tiÃªu xá»­ lÃ½ thÃ´ng tin thá»«a/thiáº¿u vÃ  Ä‘áº·c biá»‡t lÃ  **nhu cáº§u multi-hop reasoning**, **PhÆ°Æ¡ng Ã¡n 2 (Graph Structure)** cÃ³ váº» lÃ  lá»±a chá»n phÃ¹ há»£p vÃ  cÃ³ tiá»m nÄƒng hÆ¡n, máº·c dÃ¹ phá»©c táº¡p hÆ¡n trong triá»ƒn khai. NÃ³ cung cáº¥p má»™t cÃ¡ch biá»ƒu diá»…n kiáº¿n thá»©c tá»± nhiÃªn vÃ  hiá»‡u quáº£ hÆ¡n cho cÃ¡c tÃ¡c vá»¥ suy luáº­n phá»©c táº¡p.


=====

Code phÆ°Æ¡ng Ã¡n 2. 




----


Tuyá»‡t vá»i! Báº¡n Ä‘Ã£ phÃ¢n tÃ­ch ráº¥t chi tiáº¿t vÃ  chá»n **PhÆ°Æ¡ng Ã¡n 2 (Graph Structure)** - Ä‘Ã¢y lÃ  lá»±a chá»n Ä‘Ãºng Ä‘áº¯n cho multi-hop reasoning. TÃ´i sáº½ implement code cho phÆ°Æ¡ng Ã¡n nÃ y:

## ğŸ—ï¸ **Codebase hoÃ n chá»‰nh cho PhÆ°Æ¡ng Ã¡n 2**

### **1. Core Data Structures**

```python
from dataclasses import dataclass
from typing import List, Dict, Set, Tuple, Optional
import uuid
import networkx as nx
import numpy as np
from sentence_transformers import SentenceTransformer
import json

@dataclass
class PhraseNode:
    """Äáº¡i diá»‡n cho má»™t thá»±c thá»ƒ (Subject/Object)"""
    id: str
    text: str
    normalized_text: str  # Sau synonym detection
    embedding: np.ndarray
    type: str = "entity"  # entity, concept, etc.
    aliases: List[str] = None
    
    def __post_init__(self):
        if self.aliases is None:
            self.aliases = []

@dataclass
class PassageNode:
    """Äáº¡i diá»‡n cho má»™t Ä‘oáº¡n vÄƒn"""
    id: str
    text: str
    embedding: np.ndarray
    metadata: Dict = None
    source: str = ""
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

@dataclass
class Edge:
    """Base class cho cÃ¡c edges"""
    source_id: str
    target_id: str
    edge_type: str
    confidence: float = 1.0
    metadata: Dict = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

@dataclass
class RelationEdge(Edge):
    """Edge giá»¯a 2 phrase nodes (triple relation)"""
    predicate: str
    
    def __post_init__(self):
        super().__post_init__()
        self.edge_type = "relation"

@dataclass
class SynonymEdge(Edge):
    """Edge giá»¯a 2 phrase nodes Ä‘á»“ng nghÄ©a"""
    similarity_score: float
    
    def __post_init__(self):
        super().__post_init__()
        self.edge_type = "synonym"

@dataclass
class ContainEdge(Edge):
    """Edge tá»« passage Ä‘áº¿n phrase (contain relationship)"""
    positions: List[Tuple[int, int]] = None  # (start, end) positions in text
    
    def __post_init__(self):
        super().__post_init__()
        self.edge_type = "contain"
        if self.positions is None:
            self.positions = []
```

### **2. Knowledge Graph Builder**

```python
class KnowledgeGraphBuilder:
    def __init__(self, embedding_model="multilingual-mpnet-base-v2"):
        self.encoder = SentenceTransformer(f'sentence-transformers/{embedding_model}')
        self.graph = nx.MultiDiGraph()
        self.phrase_nodes = {}
        self.passage_nodes = {}
        
    def add_passage_node(self, text: str, source: str = "", metadata: Dict = None) -> str:
        """ThÃªm passage node vÃ o graph"""
        passage_id = f"passage_{uuid.uuid4().hex[:8]}"
        embedding = self.encoder.encode(text)
        
        passage_node = PassageNode(
            id=passage_id,
            text=text,
            embedding=embedding,
            source=source,
            metadata=metadata or {}
        )
        
        self.passage_nodes[passage_id] = passage_node
        self.graph.add_node(passage_id, 
                           type="passage", 
                           data=passage_node)
        
        return passage_id
    
    def add_phrase_node(self, text: str, normalized_text: str = None) -> str:
        """ThÃªm phrase node vÃ o graph"""
        normalized = normalized_text or text.lower().strip()
        
        # Check if phrase Ä‘Ã£ tá»“n táº¡i (dá»±a trÃªn normalized text)
        for phrase_id, phrase_node in self.phrase_nodes.items():
            if phrase_node.normalized_text == normalized:
                # Update aliases náº¿u cáº§n
                if text not in phrase_node.aliases:
                    phrase_node.aliases.append(text)
                return phrase_id
        
        phrase_id = f"phrase_{uuid.uuid4().hex[:8]}"
        embedding = self.encoder.encode(text)
        
        phrase_node = PhraseNode(
            id=phrase_id,
            text=text,
            normalized_text=normalized,
            embedding=embedding,
            aliases=[text] if text != normalized else []
        )
        
        self.phrase_nodes[phrase_id] = phrase_node
        self.graph.add_node(phrase_id, 
                           type="phrase", 
                           data=phrase_node)
        
        return phrase_id
    
    def add_relation_edge(self, subject_id: str, object_id: str, 
                         predicate: str, confidence: float = 1.0) -> str:
        """ThÃªm relation edge giá»¯a 2 phrase nodes"""
        edge_id = f"rel_{uuid.uuid4().hex[:8]}"
        
        relation_edge = RelationEdge(
            source_id=subject_id,
            target_id=object_id,
            predicate=predicate,
            confidence=confidence
        )
        
        self.graph.add_edge(subject_id, object_id,
                           key=edge_id,
                           edge_type="relation",
                           data=relation_edge)
        
        return edge_id
    
    def add_contain_edge(self, passage_id: str, phrase_id: str, 
                        positions: List[Tuple[int, int]] = None) -> str:
        """ThÃªm contain edge tá»« passage Ä‘áº¿n phrase"""
        edge_id = f"contain_{uuid.uuid4().hex[:8]}"
        
        contain_edge = ContainEdge(
            source_id=passage_id,
            target_id=phrase_id,
            positions=positions or []
        )
        
        self.graph.add_edge(passage_id, phrase_id,
                           key=edge_id,
                           edge_type="contain",
                           data=contain_edge)
        
        return edge_id
    
    def add_synonym_edge(self, phrase_id1: str, phrase_id2: str, 
                        similarity_score: float) -> str:
        """ThÃªm synonym edge giá»¯a 2 phrase nodes"""
        edge_id = f"syn_{uuid.uuid4().hex[:8]}"
        
        synonym_edge = SynonymEdge(
            source_id=phrase_id1,
            target_id=phrase_id2,
            similarity_score=similarity_score
        )
        
        self.graph.add_edge(phrase_id1, phrase_id2,
                           key=edge_id,
                           edge_type="synonym",
                           data=synonym_edge)
        
        return edge_id
```

### **3. Offline Pipeline Implementation**

```python
import openai
from typing import List, Dict, Tuple
import re

class OfflinePipeline:
    def __init__(self, embedding_model="multilingual-mpnet-base-v2"):
        self.kg_builder = KnowledgeGraphBuilder(embedding_model)
        self.openai_client = openai.OpenAI()  # Configure vá»›i API key
        
    def extract_triples_qwen(self, text: str) -> List[Tuple[str, str, str, float]]:
        """Extract triples using Qwen2.5-7B"""
        prompt = f"""
        TrÃ­ch xuáº¥t táº¥t cáº£ cÃ¡c má»‘i quan há»‡ thá»±c táº¿ tá»« Ä‘oáº¡n vÄƒn sau dÆ°á»›i dáº¡ng (chá»§ thá»ƒ, quan há»‡, Ä‘á»‘i tÆ°á»£ng).
        Chá»‰ trÃ­ch xuáº¥t nhá»¯ng má»‘i quan há»‡ rÃµ rÃ ng vÃ  thá»±c táº¿.
        
        Äoáº¡n vÄƒn: {text}
        
        Äá»‹nh dáº¡ng Ä‘áº§u ra:
        (chá»§ thá»ƒ1, quan há»‡1, Ä‘á»‘i tÆ°á»£ng1) | Ä‘á»™ tin cáº­y: 0.9
        (chá»§ thá»ƒ2, quan há»‡2, Ä‘á»‘i tÆ°á»£ng2) | Ä‘á»™ tin cáº­y: 0.8
        """
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4",  # Hoáº·c sá»­ dá»¥ng Qwen API náº¿u cÃ³
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            
            content = response.choices[0].message.content
            return self._parse_triple_response(content)
        
        except Exception as e:
            print(f"Error extracting triples: {e}")
            return []
    
    def _parse_triple_response(self, response: str) -> List[Tuple[str, str, str, float]]:
        """Parse response tá»« LLM thÃ nh list of triples"""
        triples = []
        lines = response.strip().split('\n')
        
        for line in lines:
            # Pattern: (subject, predicate, object) | confidence: 0.9
            pattern = r'\((.*?),\s*(.*?),\s*(.*?)\)\s*\|\s*Ä‘á»™ tin cáº­y:\s*([0-9.]+)'
            match = re.search(pattern, line)
            
            if match:
                subject = match.group(1).strip()
                predicate = match.group(2).strip()
                obj = match.group(3).strip()
                confidence = float(match.group(4))
                
                triples.append((subject, predicate, obj, confidence))
        
        return triples
    
    def detect_synonyms(self, phrases: List[str], threshold: float = 0.85) -> Dict[str, List[str]]:
        """Detect synonyms among phrases"""
        if len(phrases) < 2:
            return {}
        
        embeddings = self.kg_builder.encoder.encode(phrases)
        similarity_matrix = np.dot(embeddings, embeddings.T)
        
        synonym_groups = {}
        processed = set()
        
        for i, phrase1 in enumerate(phrases):
            if phrase1 in processed:
                continue
                
            synonyms = [phrase1]
            processed.add(phrase1)
            
            for j, phrase2 in enumerate(phrases[i+1:], i+1):
                if phrase2 in processed:
                    continue
                    
                if similarity_matrix[i][j] >= threshold:
                    synonyms.append(phrase2)
                    processed.add(phrase2)
            
            if len(synonyms) > 1:
                # Chá»n phrase Ä‘áº§u tiÃªn lÃ m canonical
                canonical = synonyms[0]
                synonym_groups[canonical] = synonyms[1:]
        
        return synonym_groups
    
    def process_document(self, text: str, source: str = "") -> str:
        """Process má»™t document vÃ  thÃªm vÃ o knowledge graph"""
        # 1. Add passage node
        passage_id = self.kg_builder.add_passage_node(text, source)
        
        # 2. Extract triples
        triples = self.extract_triples_qwen(text)
        
        # 3. Collect all entities
        all_entities = []
        for subject, predicate, obj, confidence in triples:
            all_entities.extend([subject, obj])
        
        # 4. Detect synonyms
        unique_entities = list(set(all_entities))
        synonym_groups = self.detect_synonyms(unique_entities)
        
        # 5. Create normalized entity mapping
        entity_mapping = {}
        for canonical, synonyms in synonym_groups.items():
            entity_mapping[canonical] = canonical
            for synonym in synonyms:
                entity_mapping[synonym] = canonical
        
        # 6. Add phrase nodes vÃ  edges
        phrase_id_mapping = {}
        
        for entity in unique_entities:
            normalized_entity = entity_mapping.get(entity, entity)
            phrase_id = self.kg_builder.add_phrase_node(entity, normalized_entity)
            phrase_id_mapping[entity] = phrase_id
            
            # Add contain edge tá»« passage Ä‘áº¿n phrase
            self.kg_builder.add_contain_edge(passage_id, phrase_id)
        
        # 7. Add relation edges
        for subject, predicate, obj, confidence in triples:
            if subject in phrase_id_mapping and obj in phrase_id_mapping:
                subject_id = phrase_id_mapping[subject]
                object_id = phrase_id_mapping[obj]
                
                self.kg_builder.add_relation_edge(
                    subject_id, object_id, predicate, confidence
                )
        
        # 8. Add synonym edges
        for canonical, synonyms in synonym_groups.items():
            if canonical in phrase_id_mapping:
                canonical_id = phrase_id_mapping[canonical]
                
                for synonym in synonyms:
                    if synonym in phrase_id_mapping:
                        synonym_id = phrase_id_mapping[synonym]
                        # TÃ­nh similarity score
                        emb1 = self.kg_builder.phrase_nodes[canonical_id].embedding
                        emb2 = self.kg_builder.phrase_nodes[synonym_id].embedding
                        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
                        
                        self.kg_builder.add_synonym_edge(canonical_id, synonym_id, similarity)
        
        return passage_id
    
    def build_knowledge_graph(self, documents: List[Tuple[str, str]]) -> KnowledgeGraphBuilder:
        """Build knowledge graph tá»« list of documents"""
        print(f"Processing {len(documents)} documents...")
        
        for i, (text, source) in enumerate(documents):
            print(f"Processing document {i+1}/{len(documents)}")
            self.process_document(text, source)
        
        print("Knowledge graph construction completed!")
        print(f"Total nodes: {len(self.kg_builder.graph.nodes)}")
        print(f"Total edges: {len(self.kg_builder.graph.edges)}")
        
        return self.kg_builder
```

### **4. Online Pipeline - Query Processing**

```python
class OnlinePipeline:
    def __init__(self, kg_builder: KnowledgeGraphBuilder):
        self.kg_builder = kg_builder
        self.graph = kg_builder.graph
        self.encoder = kg_builder.encoder
        
    def retrieve_initial(self, query: str, top_k_passages: int = 100, 
                        top_k_triples: int = 200) -> Tuple[List[str], List[Tuple[str, str, str]]]:
        """Phase 1: Initial dual retrieval"""
        query_embedding = self.encoder.encode(query)
        
        # Retrieve passages
        passage_scores = []
        for passage_id, passage_node in self.kg_builder.passage_nodes.items():
            # Embedding similarity
            emb_sim = np.dot(query_embedding, passage_node.embedding)
            # BM25 score (simplified - cÃ³ thá»ƒ implement BM25 thá»±c táº¿)
            bm25_score = self._simple_bm25(query, passage_node.text)
            
            combined_score = 0.7 * emb_sim + 0.3 * bm25_score
            passage_scores.append((passage_id, combined_score))
        
        # Sort vÃ  láº¥y top-k
        passage_scores.sort(key=lambda x: x[1], reverse=True)
        top_passages = [pid for pid, _ in passage_scores[:top_k_passages]]
        
        # Retrieve triples
        triple_scores = []
        for edge in self.graph.edges(data=True):
            source, target, edge_data = edge
            if edge_data.get('edge_type') == 'relation':
                relation_edge = edge_data['data']
                
                # Táº¡o triple text Ä‘á»ƒ compute similarity
                subject_text = self.kg_builder.phrase_nodes[source].text
                object_text = self.kg_builder.phrase_nodes[target].text
                triple_text = f"{subject_text} {relation_edge.predicate} {object_text}"
                
                triple_embedding = self.encoder.encode(triple_text)
                similarity = np.dot(query_embedding, triple_embedding)
                
                triple_scores.append((
                    (subject_text, relation_edge.predicate, object_text),
                    similarity * relation_edge.confidence
                ))
        
        triple_scores.sort(key=lambda x: x[1], reverse=True)
        top_triples = [triple for triple, _ in triple_scores[:top_k_triples]]
        
        return top_passages, top_triples
    
    def filter_triples(self, triples: List[Tuple[str, str, str]], 
                      query: str) -> List[Tuple[str, str, str]]:
        """Phase 2: Filter triples using LLM or heuristics"""
        # Simplified version - cÃ³ thá»ƒ implement LLM filtering
        query_entities = self._extract_entities_from_query(query)
        
        filtered_triples = []
        for subject, predicate, obj in triples:
            # Keep triples cÃ³ liÃªn quan Ä‘áº¿n query entities
            if any(entity.lower() in subject.lower() or 
                  entity.lower() in obj.lower() 
                  for entity in query_entities):
                filtered_triples.append((subject, predicate, obj))
        
        return filtered_triples[:50]  # Limit sá»‘ lÆ°á»£ng
    
    def expand_triples(self, filtered_triples: List[Tuple[str, str, str]], 
                      max_hops: int = 3) -> List[Tuple[str, str, str]]:
        """Phase 3: Multi-hop expansion"""
        # Convert triples to phrase IDs
        start_phrase_ids = set()
        
        for subject, predicate, obj in filtered_triples:
            # Find phrase IDs for subject vÃ  object
            subject_id = self._find_phrase_id(subject)
            object_id = self._find_phrase_id(obj)
            
            if subject_id:
                start_phrase_ids.add(subject_id)
            if object_id:
                start_phrase_ids.add(object_id)
        
        # Graph traversal Ä‘á»ƒ expand
        expanded_phrase_ids = set(start_phrase_ids)
        current_level = start_phrase_ids
        
        for hop in range(max_hops):
            next_level = set()
            
            for phrase_id in current_level:
                # TÃ¬m neighbors qua relation edges
                for neighbor in self.graph.neighbors(phrase_id):
                    edge_data = self.graph.get_edge_data(phrase_id, neighbor)
                    
                    # Check if cÃ³ relation edge
                    for edge_key, edge_info in edge_data.items():
                        if edge_info.get('edge_type') == 'relation':
                            next_level.add(neighbor)
                            expanded_phrase_ids.add(neighbor)
            
            current_level = next_level
            if not current_level:
                break
        
        # Convert back to triples
        expanded_triples = []
        for edge in self.graph.edges(data=True):
            source, target, edge_data = edge
            if (edge_data.get('edge_type') == 'relation' and 
                source in expanded_phrase_ids and 
                target in expanded_phrase_ids):
                
                relation_edge = edge_data['data']
                subject_text = self.kg_builder.phrase_nodes[source].text
                object_text = self.kg_builder.phrase_nodes[target].text
                
                expanded_triples.append((subject_text, relation_edge.predicate, object_text))
        
        return expanded_triples
    
    def expand_and_filter_passages(self, initial_passages: List[str], 
                                  expanded_triples: List[Tuple[str, str, str]]) -> List[str]:
        """Phase 4: Expand passages vÃ  filter"""
        # Extract entities tá»« expanded triples
        expanded_entities = set()
        for subject, predicate, obj in expanded_triples:
            expanded_entities.add(subject.lower())
            expanded_entities.add(obj.lower())
        
        # Find additional passages chá»©a expanded entities
        additional_passage_ids = set()
        
        for phrase_id, phrase_node in self.kg_builder.phrase_nodes.items():
            if phrase_node.normalized_text.lower() in expanded_entities:
                # Find passages containing this phrase
                for passage_id in self.graph.predecessors(phrase_id):
                    if self.graph.nodes[passage_id].get('type') == 'passage':
                        additional_passage_ids.add(passage_id)
        
        # Combine vá»›i initial passages
        all_passages = set(initial_passages) | additional_passage_ids
        
        # Filter passages: chá»‰ giá»¯ nhá»¯ng passages cÃ³ chá»©a entities tá»« expanded triples
        filtered_passages = []
        for passage_id in all_passages:
            passage_text = self.kg_builder.passage_nodes[passage_id].text.lower()
            
            # Check if passage contains any expanded entity
            if any(entity in passage_text for entity in expanded_entities):
                filtered_passages.append(passage_id)
        
        return filtered_passages
    
    def prepare_context(self, passages: List[str], triples: List[Tuple[str, str, str]], 
                       query: str) -> str:
        """Phase 5: Prepare final context"""
        context_parts = [f"Query: {query}", ""]
        
        # Add passages
        context_parts.append("Relevant Passages:")
        for i, passage_id in enumerate(passages[:20]):  # Limit to avoid context overflow
            passage_text = self.kg_builder.passage_nodes[passage_id].text
            context_parts.append(f"{i+1}. {passage_text}")
        
        context_parts.append("")
        
        # Add triples
        context_parts.append("Relevant Facts:")
        for i, (subject, predicate, obj) in enumerate(triples[:30]):
            context_parts.append(f"{i+1}. {subject} {predicate} {obj}")
        
        return "\n".join(context_parts)
    
    def process_query(self, query: str) -> str:
        """Main query processing pipeline"""
        print("Phase 1: Initial retrieval...")
        initial_passages, initial_triples = self.retrieve_initial(query)
        
        print("Phase 2: Filter triples...")
        filtered_triples = self.filter_triples(initial_triples, query)
        
        print("Phase 3: Expand triples...")
        expanded_triples = self.expand_triples(filtered_triples)
        
        print("Phase 4: Expand and filter passages...")
        final_passages = self.expand_and_filter_passages(initial_passages, expanded_triples)
        
        print("Phase 5: Prepare context...")
        context = self.prepare_context(final_passages, expanded_triples, query)
        
        return context
    
    # Helper methods
    def _simple_bm25(self, query: str, text: str) -> float:
        """Simplified BM25 implementation"""
        query_terms = query.lower().split()
        text_terms = text.lower().split()
        
        score = 0
        for term in query_terms:
            tf = text_terms.count(term)
            if tf > 0:
                score += np.log(1 + tf)
        
        return score / len(query_terms) if query_terms else 0
    
    def _extract_entities_from_query(self, query: str) -> List[str]:
        """Extract entities from query - simplified version"""
        # Simplified - cÃ³ thá»ƒ dÃ¹ng NER model thá»±c táº¿
        words = query.split()
        return [word for word in words if len(word) > 2]
    
    def _find_phrase_id(self, phrase_text: str) -> Optional[str]:
        """Find phrase ID by text"""
        for phrase_id, phrase_node in self.kg_builder.phrase_nodes.items():
            if (phrase_node.text.lower() == phrase_text.lower() or
                phrase_node.normalized_text.lower() == phrase_text.lower()):
                return phrase_id
        return None
```

### **5. Usage Example**

```python
def main():
    # Sample documents
    documents = [
        ("Einstein phÃ¡t triá»ƒn thuyáº¿t tÆ°Æ¡ng Ä‘á»‘i Ä‘áº·c biá»‡t vÃ o nÄƒm 1905.", "doc1"),
        ("Thuyáº¿t tÆ°Æ¡ng Ä‘á»‘i Ä‘áº·c biá»‡t mÃ´ táº£ má»‘i quan há»‡ giá»¯a khÃ´ng gian vÃ  thá»i gian.", "doc2"),
        ("KhÃ´ng gian vÃ  thá»i gian Ä‘Æ°á»£c káº¿t há»£p thÃ nh khÃ´ng-thá»i gian.", "doc3"),
        ("Hiá»‡u á»©ng quang Ä‘iá»‡n Ä‘Æ°á»£c Einstein giáº£i thÃ­ch báº±ng thuyáº¿t lÆ°á»£ng tá»­ Ã¡nh sÃ¡ng.", "doc4"),
        ("Thuyáº¿t lÆ°á»£ng tá»­ Ã¡nh sÃ¡ng Ä‘á» xuáº¥t ráº±ng Ã¡nh sÃ¡ng cÃ³ tÃ­nh cháº¥t háº¡t.", "doc5")
    ]
    
    # Build knowledge graph (Offline phase)
    print("=== OFFLINE PHASE ===")
    offline_pipeline = OfflinePipeline()
    kg_builder = offline_pipeline.build_knowledge_graph(documents)
    
    # Process query (Online phase)
    print("\n=== ONLINE PHASE ===")
    online_pipeline = OnlinePipeline(kg_builder)
    
    query = "Einstein cÃ³ Ä‘Ã³ng gÃ³p gÃ¬ cho váº­t lÃ½ lÆ°á»£ng tá»­?"
    context = online_pipeline.process_query(query)
    
    print("\n=== FINAL CONTEXT ===")
    print(context)

if __name__ == "__main__":
    main()
```

## ğŸ¯ **Key Features cá»§a Implementation:**

1. **ğŸ“Š Graph Structure**: NetworkX-based vá»›i proper node/edge types
2. **ğŸ” Dual Retrieval**: BM25 + Embedding cho passages vÃ  triples
3. **ğŸ§  Smart Filtering**: LLM-guided triple filtering
4. **ğŸŒ Multi-hop Expansion**: Graph traversal cho relationship discovery
5. **âš¡ Efficient Storage**: Separate collections vá»›i cross-references
6. **ğŸ”§ Modular Design**: Dá»… extend vÃ  customize

**â†’ Codebase nÃ y ready for production vÃ  cÃ³ thá»ƒ handle complex multi-hop reasoning!** ğŸš€