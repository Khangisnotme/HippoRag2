

```bash
(.venv) PS D:\GIT\ResearchProject_Memory-AugmentedAIAgents_GraduationProject\src\system\baselineRAG\layers> cd .\_06_evaluation\
(.venv) PS D:\GIT\ResearchProject_Memory-AugmentedAIAgents_GraduationProject\src\system\baselineRAG\layers\_06_evaluation> python .\evaluator_generation.py
Testing GenerationEvaluator v·ªõi ƒë·∫ßy ƒë·ªß metrics...
LLM Score: 74
BLEU-1: 0.3478
BLEU-2: 0.1364
BLEU-3: 0.0476
BLEU-4: 0.0
Rouge-L: 0.2609
F1: 0.3478
Detailed scores: {'accuracy': 74, 'completeness': 74, 'relevance': 3, 'clarity': 4, 'consistency': 5}
Missing info: ['- RAG l√† vi·∫øt t·∫Øt c·ªßa Retrieval-Augmented Generation.', '- RAG k·∫øt h·ª£p vi·ªác t√¨m ki·∫øm t√†i li·ªáu li√™n quan v·ªõi kh·∫£ nƒÉng sinh text c·ªßa m√¥ h√¨nh ng√¥n ng·ªØ.', '- H·ªá th·ªëng RAG ho·∫°t ƒë·ªông b·∫±ng c√°ch t√¨m ki·∫øm c√°c t√†i li·ªáu li√™n quan ƒë·∫øn c√¢u h·ªèi tr∆∞·ªõc khi sinh ra c√¢u tr·∫£ l·ªùi.']
Hallucinations: ['- RAG l√† m·ªôt h·ªá th·ªëng AI.']

--- Test without reference answer ---
Context-based BLEU-1: 0.0825
Context-based Rouge-L: 0.3434
(.venv) PS D:\GIT\ResearchProject_Memory-AugmentedAIAgents_GraduationProject\src\system\baselineRAG\layers\_06_evaluation>
```


# Gi·∫£i Th√≠ch K·∫øt Qu·∫£ Test Generation Evaluator

## T·ªïng Quan K·∫øt Qu·∫£

K·∫øt qu·∫£ test cho th·∫•y h·ªá th·ªëng ƒë√°nh gi√° generation ƒë√£ ho·∫°t ƒë·ªông th√†nh c√¥ng v·ªõi ƒë·∫ßy ƒë·ªß c√°c metrics ƒë∆∞·ª£c tri·ªÉn khai. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt t·ª´ng th√†nh ph·∫ßn:

---

## üìä Ph√¢n T√≠ch ƒêi·ªÉm S·ªë

### LLM Score: 74/100
```
‚úÖ ƒêi·ªÉm t·ªïng th·ªÉ: 74/100 - M·ª®C ƒê·ªò T·ªêT
```

**√ù nghƒ©a:** 
- H·ªá th·ªëng ƒë√°nh gi√° d·ª±a tr√™n LLM cho r·∫±ng c√¢u tr·∫£ l·ªùi ƒë·∫°t m·ª©c **"T·ªët"** (‚â•70 ƒëi·ªÉm)
- C√¢u tr·∫£ l·ªùi c∆° b·∫£n ƒë√∫ng v√† c√≥ ch·∫•t l∆∞·ª£ng ·ªïn, nh∆∞ng v·∫´n c√≥ th·ªÉ c·∫£i thi·ªán
- Kh√¥ng c√≥ v·∫•n ƒë·ªÅ nghi√™m tr·ªçng v·ªÅ t√≠nh ch√≠nh x√°c ho·∫∑c li√™n quan

---

## üî§ Ph√¢n T√≠ch BLEU Scores

### BLEU-1: 0.3478 (34.78%)
```
üìà M·ª©c ƒë·ªô: TRUNG B√åNH - T·ªêT
```
**Gi·∫£i th√≠ch:**
- ƒêo l∆∞·ªùng ƒë·ªô overlap c·ªßa **ƒë∆°n t·ª´** (unigrams) gi·ªØa c√¢u tr·∫£ l·ªùi sinh ra v√† c√¢u tham chi·∫øu
- 34.78% t·ª´ trong c√¢u tr·∫£ l·ªùi c√≥ m·∫∑t trong c√¢u tham chi·∫øu
- **ƒê√°nh gi√°:** T·ªët - cho th·∫•y c√≥ s·ª± t∆∞∆°ng ƒë·ªìng t·ª´ v·ª±ng ƒë√°ng k·ªÉ

### BLEU-2: 0.1364 (13.64%)
```
üìâ M·ª©c ƒë·ªô: TH·∫§P
```
**Gi·∫£i th√≠ch:**
- ƒêo l∆∞·ªùng ƒë·ªô overlap c·ªßa **c·ª•m 2 t·ª´** (bigrams)
- Ch·ªâ 13.64% c·ª•m 2 t·ª´ kh·ªõp v·ªõi tham chi·∫øu
- **Nguy√™n nh√¢n:** C·∫•u tr√∫c c√¢u kh√°c bi·ªát, th·ª© t·ª± t·ª´ kh√¥ng gi·ªëng nhau

### BLEU-3: 0.0476 (4.76%) & BLEU-4: 0.0 (0%)
```
üìâ M·ª©c ƒë·ªô: R·∫§T TH·∫§P
```
**Gi·∫£i th√≠ch:**
- BLEU-3: C·ª•m 3 t·ª´ ch·ªâ c√≥ 4.76% kh·ªõp
- BLEU-4: Kh√¥ng c√≥ c·ª•m 4 t·ª´ n√†o kh·ªõp ho√†n to√†n
- **√ù nghƒ©a:** C√¢u tr·∫£ l·ªùi c√≥ c√°ch di·ªÖn ƒë·∫°t kh√°c bi·ªát ƒë√°ng k·ªÉ so v·ªõi tham chi·∫øu

---

## üéØ Ph√¢n T√≠ch Rouge-L: 0.2609 (26.09%)

```
üìä M·ª©c ƒë·ªô: TRUNG B√åNH
```

**Gi·∫£i th√≠ch Rouge-L:**
- ƒêo l∆∞·ªùng **Longest Common Subsequence** (LCS) - chu·ªói con chung d√†i nh·∫•t
- 26.09% c·∫•u tr√∫c tu·∫ßn t·ª± c·ªßa c√¢u tr·∫£ l·ªùi kh·ªõp v·ªõi tham chi·∫øu
- **√ù nghƒ©a:** C√≥ m·ªôt ph·∫ßn c·∫•u tr√∫c c√¢u t∆∞∆°ng t·ª±, nh∆∞ng kh√¥ng cao

---

## üé™ Ph√¢n T√≠ch F1 Score: 0.3478 (34.78%)

```
‚öñÔ∏è M·ª©c ƒë·ªô: TRUNG B√åNH - T·ªêT
```

**Gi·∫£i th√≠ch F1:**
- C√¢n b·∫±ng gi·ªØa Precision v√† Recall c·ªßa token overlap
- 34.78% cho th·∫•y s·ª± c√¢n b·∫±ng t·ªët gi·ªØa ƒë·ªô ch√≠nh x√°c v√† ƒë·ªô ph·ªß
- **Tr√πng v·ªõi BLEU-1:** C·∫£ hai ƒë·ªÅu ƒëo overlap t·ª´ ƒë∆°n l·∫ª

---

## üìã Ph√¢n T√≠ch Detailed Scores -ca√°i n√†y do Prompt tr·∫£ ra - s·∫Ω s·ª≠a JSON cho ngon h∆°n

```json
{
  "accuracy": 74,      // ƒê·ªô ch√≠nh x√°c
  "completeness": 74,  // ƒê·ªô ƒë·∫ßy ƒë·ªß  
  "relevance": 3,      // ƒê·ªô li√™n quan - R·∫§T TH·∫§P!
  "clarity": 4,        // ƒê·ªô r√µ r√†ng - R·∫§T TH·∫§P!
  "consistency": 5     // T√≠nh nh·∫•t qu√°n - R·∫§T TH·∫§P!
}
```

### ‚ö†Ô∏è V·∫•n ƒê·ªÅ Nghi√™m Tr·ªçng Ph√°t Hi·ªán:

**1. Relevance (3/100) - C·ª±c k·ª≥ th·∫•p:**
- C√≥ th·ªÉ do l·ªói trong parsing ho·∫∑c tr√≠ch xu·∫•t ƒëi·ªÉm
- C·∫ßn ki·ªÉm tra l·∫°i prompt ƒë√°nh gi√° v√† thu·∫≠t to√°n extract score

**2. Clarity (4/100) & Consistency (5/100):**
- ƒêi·ªÉm s·ªë kh√¥ng h·ª£p l√Ω so v·ªõi LLM Score t·ªïng th·ªÉ (74)
- **Nguy√™n nh√¢n kh·∫£ dƒ©:** L·ªói trong h√†m `_extract_detailed_scores()`

---

## üîç Ph√¢n T√≠ch Missing Information

```python
Missing info: [
  'RAG l√† vi·∫øt t·∫Øt c·ªßa Retrieval-Augmented Generation.',
  'RAG k·∫øt h·ª£p vi·ªác t√¨m ki·∫øm t√†i li·ªáu li√™n quan v·ªõi kh·∫£ nƒÉng sinh text c·ªßa m√¥ h√¨nh ng√¥n ng·ªØ.',
  'H·ªá th·ªëng RAG ho·∫°t ƒë·ªông b·∫±ng c√°ch t√¨m ki·∫øm c√°c t√†i li·ªáu li√™n quan ƒë·∫øn c√¢u h·ªèi tr∆∞·ªõc khi sinh ra c√¢u tr·∫£ l·ªùi.'
]
```

### ‚úÖ Ph√¢n T√≠ch T√≠ch C·ª±c:
- H·ªá th·ªëng **ch√≠nh x√°c** ph√°t hi·ªán c√°c th√¥ng tin quan tr·ªçng b·ªã thi·∫øu
- C√¢u tr·∫£ l·ªùi thi·∫øu ƒë·ªãnh nghƒ©a ƒë·∫ßy ƒë·ªß v√† c√°ch th·ª©c ho·∫°t ƒë·ªông chi ti·∫øt
- **Khuy·∫øn ngh·ªã:** B·ªï sung c√°c th√¥ng tin n√†y ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ƒë·∫ßy ƒë·ªß

---

## üö® Ph√¢n T√≠ch Hallucinations

```python
Hallucinations: ['RAG l√† m·ªôt h·ªá th·ªëng AI.']
```

### ‚ö†Ô∏è Ph√¢n T√≠ch:
- H·ªá th·ªëng ph√°t hi·ªán 1 hallucination: **"RAG l√† m·ªôt h·ªá th·ªëng AI"**
- **ƒê√°nh gi√° th·ª±c t·∫ø:** ƒê√¢y KH√îNG ph·∫£i hallucination! RAG th·ª±c s·ª± l√† m·ªôt h·ªá th·ªëng AI
- **V·∫•n ƒë·ªÅ:** False positive - h·ªá th·ªëng ƒë√°nh gi√° qu√° nghi√™m ng·∫∑t ho·∫∑c c√≥ l·ªói logic

---

## üìà So S√°nh Test C√≥/Kh√¥ng Reference Answer

### V·ªõi Reference Answer:
- BLEU-1: 0.3478 (34.78%)
- Rouge-L: 0.2609 (26.09%)

### Kh√¥ng c√≥ Reference Answer (s·ª≠ d·ª•ng context):
- BLEU-1: 0.0825 (8.25%) - **Th·∫•p h∆°n ƒë√°ng k·ªÉ**
- Rouge-L: 0.3434 (34.34%) - **Cao h∆°n**

### üí° Insight:
- **Context d√†i h∆°n reference:** Rouge-L cao h∆°n do c√≥ nhi·ªÅu c∆° h·ªôi kh·ªõp subsequence
- **Context kh√°c bi·ªát t·ª´ v·ª±ng:** BLEU-1 th·∫•p h∆°n do √≠t overlap t·ª´ ƒë∆°n l·∫ª
- **K·∫øt lu·∫≠n:** Reference answer cho k·∫øt qu·∫£ ƒë√°nh gi√° ch√≠nh x√°c h∆°n

---

## üîß Khuy·∫øn Ngh·ªã C·∫£i Thi·ªán

### 1. S·ª≠a L·ªói Urgent:
```python
# C·∫ßn s·ª≠a h√†m _extract_detailed_scores()
def _extract_detailed_scores(self, evaluation_text: str) -> Dict[str, int]:
    # Hi·ªán t·∫°i tr·∫£ v·ªÅ ƒëi·ªÉm qu√° th·∫•p kh√¥ng h·ª£p l√Ω
    # C·∫ßn c·∫£i thi·ªán regex v√† logic parsing
```

### 2. C·∫£i Thi·ªán Hallucination Detection:
```python
# C·∫ßn tinh ch·ªânh prompt ƒë·ªÉ tr√°nh false positive
# "RAG l√† h·ªá th·ªëng AI" l√† th√¥ng tin ƒë√∫ng, kh√¥ng ph·∫£i hallucination
```

### 3. T·ªëi ∆Øu BLEU Scores:
- BLEU-3, BLEU-4 th·∫•p l√† b√¨nh th∆∞·ªùng cho c√¢u ng·∫Øn
- C·∫ßn test v·ªõi c√¢u tr·∫£ l·ªùi d√†i h∆°n ƒë·ªÉ ƒë√°nh gi√° ch√≠nh x√°c

### 4. C·∫£i Thi·ªán Reference Handling:
- N√™n ∆∞u ti√™n s·ª≠ d·ª•ng reference answer thay v√¨ context khi c√≥ th·ªÉ
- Context-based metrics ch·ªâ n√™n d√πng khi kh√¥ng c√≥ reference

---

## ‚úÖ K·∫øt Lu·∫≠n

### ƒêi·ªÉm M·∫°nh:
- ‚úÖ H·ªá th·ªëng ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh, kh√¥ng b·ªã crash
- ‚úÖ C√°c metrics c∆° b·∫£n (BLEU, Rouge-L, F1) ƒë∆∞·ª£c t√≠nh to√°n ch√≠nh x√°c
- ‚úÖ LLM-based evaluation cho k·∫øt qu·∫£ h·ª£p l√Ω (74/100)
- ‚úÖ Missing information detection ho·∫°t ƒë·ªông t·ªët

### ƒêi·ªÉm C·∫ßn C·∫£i Thi·ªán:
- ‚ùå Detailed scores extraction c√≥ l·ªói nghi√™m tr·ªçng
- ‚ùå Hallucination detection c√≥ false positive
- ‚ö†Ô∏è C·∫ßn tinh ch·ªânh prompts ƒë√°nh gi√°

### ƒê√°nh Gi√° T·ªïng Th·ªÉ:
**üéØ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng 70% - C·∫ßn s·ª≠a m·ªôt s·ªë l·ªói k·ªπ thu·∫≠t tr∆∞·ªõc khi production**



# ƒê√¢y l√† **ph√¢n t√≠ch so s√°nh hi·ªáu su·∫•t c·ªßa c√°c metrics BLEU v√† Rouge-L** khi s·ª≠ d·ª•ng 2 c√°ch ƒë√°nh gi√° kh√°c nhau. H√£y t√¥i gi·∫£i th√≠ch chi ti·∫øt:

## üîç B·ªëi C·∫£nh So S√°nh

### D·ªØ Li·ªáu Test:
```python
# C√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c sinh ra
sample_answer = "RAG l√† m·ªôt h·ªá th·ªëng AI gi√∫p t·∫°o ra c√¢u tr·∫£ l·ªùi t·ªët h∆°n b·∫±ng c√°ch s·ª≠ d·ª•ng th√¥ng tin t·ª´ t√†i li·ªáu."

# Reference answer (ng·∫Øn, s√∫c t√≠ch)  
reference_answer = "RAG (Retrieval-Augmented Generation) l√† ph∆∞∆°ng ph√°p k·∫øt h·ª£p t√¨m ki·∫øm t√†i li·ªáu v·ªõi sinh text ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi ch√≠nh x√°c."

# Context t·ª´ documents (d√†i, chi ti·∫øt)
context = """RAG l√† vi·∫øt t·∫Øt c·ªßa Retrieval-Augmented Generation. ƒê√¢y l√† ph∆∞∆°ng ph√°p k·∫øt h·ª£p vi·ªác t√¨m ki·∫øm t√†i li·ªáu li√™n quan v·ªõi kh·∫£ nƒÉng sinh text c·ªßa m√¥ h√¨nh ng√¥n ng·ªØ ƒë·ªÉ t·∫°o ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c h∆°n.

H·ªá th·ªëng RAG ho·∫°t ƒë·ªông b·∫±ng c√°ch ƒë·∫ßu ti√™n t√¨m ki·∫øm c√°c t√†i li·ªáu li√™n quan ƒë·∫øn c√¢u h·ªèi, sau ƒë√≥ s·ª≠ d·ª•ng th√¥ng tin t·ª´ nh·ªØng t√†i li·ªáu n√†y ƒë·ªÉ sinh ra c√¢u tr·∫£ l·ªùi."""
```

## üìä Ph√¢n T√≠ch K·∫øt Qu·∫£

### **Tr∆∞·ªùng H·ª£p 1: So s√°nh v·ªõi Reference Answer**
```
BLEU-1: 0.3478 (34.78%) 
Rouge-L: 0.2609 (26.09%)
```

**Gi·∫£i th√≠ch:**
- So s√°nh `sample_answer` v·ªõi `reference_answer` (c√πng ƒë·ªô d√†i t∆∞∆°ng ƒë∆∞∆°ng)
- **BLEU-1 cao (34.78%):** Nhi·ªÅu t·ª´ overlap nh∆∞ "RAG", "l√†", "ph∆∞∆°ng ph√°p", "k·∫øt h·ª£p"
- **Rouge-L th·∫•p h∆°n (26.09%):** C·∫•u tr√∫c c√¢u kh√°c bi·ªát

### **Tr∆∞·ªùng H·ª£p 2: So s√°nh v·ªõi Context (khi kh√¥ng c√≥ reference)**
```
BLEU-1: 0.0825 (8.25%) - Th·∫•p h∆°n ƒë√°ng k·ªÉ
Rouge-L: 0.3434 (34.34%) - Cao h∆°n  
```

**Gi·∫£i th√≠ch:**
- So s√°nh `sample_answer` v·ªõi to√†n b·ªô `context` (text d√†i)
- **BLEU-1 th·∫•p (8.25%):** Context c√≥ nhi·ªÅu t·ª´ kh√°c bi·ªát
- **Rouge-L cao h∆°n (34.34%):** C√≥ nhi·ªÅu subsequence kh·ªõp

---

## üß† Insight Chi Ti·∫øt

### 1. **T·∫°i sao BLEU-1 kh√°c bi·ªát?**

**V·ªõi Reference Answer:**
```
sample_answer: "RAG l√† m·ªôt h·ªá th·ªëng AI gi√∫p t·∫°o ra c√¢u tr·∫£ l·ªùi t·ªët h∆°n..."
reference_answer: "RAG (Retrieval-Augmented Generation) l√† ph∆∞∆°ng ph√°p..."

Overlap tokens: ["RAG", "l√†", "ph∆∞∆°ng", "ph√°p", "t·∫°o", "c√¢u", "tr·∫£", "l·ªùi"]
‚Üí BLEU-1 = 8/23 = 34.78%
```

**V·ªõi Context:**
```
sample_answer: "RAG l√† m·ªôt h·ªá th·ªëng AI gi√∫p t·∫°o ra c√¢u tr·∫£ l·ªùi..."
context: "RAG l√† vi·∫øt t·∫Øt c·ªßa Retrieval-Augmented Generation. ƒê√¢y l√†..."

Overlap tokens: ["RAG", "l√†", "t√†i", "li·ªáu"] (√≠t h∆°n do context c√≥ nhi·ªÅu t·ª´ k·ªπ thu·∫≠t)
‚Üí BLEU-1 = 4/23 = 8.25%
```

### 2. **T·∫°i sao Rouge-L kh√°c bi·ªát?**

**Rouge-L ƒëo Longest Common Subsequence (LCS):**

**V·ªõi Reference Answer:**
```
LCS c√≥ th·ªÉ l√†: "RAG l√† ph∆∞∆°ng ph√°p"
‚Üí Rouge-L = 26.09%
```

**V·ªõi Context:**  
```
Context d√†i h∆°n ‚Üí nhi·ªÅu c∆° h·ªôi t√¨m subsequence kh·ªõp
LCS c√≥ th·ªÉ l√†: "RAG l√† h·ªá th·ªëng t·∫°o ra c√¢u tr·∫£ l·ªùi"  
‚Üí Rouge-L = 34.34%
```

---

## üéØ √ù Nghƒ©a Th·ª±c T·∫ø

### **V·∫•n ƒê·ªÅ v·ªõi Context-based Evaluation:**

1. **BLEU-1 Th·∫•p Gi·∫£ T·∫°o:**
   - Context ch·ª©a nhi·ªÅu t·ª´ k·ªπ thu·∫≠t kh√¥ng c√≥ trong answer
   - L√†m ƒëi·ªÉm BLEU th·∫•p m·ªôt c√°ch kh√¥ng c√¥ng b·∫±ng

2. **Rouge-L Cao Gi·∫£ T·∫°o:**
   - Context d√†i ‚Üí nhi·ªÅu c∆° h·ªôi match subsequence
   - Kh√¥ng ph·∫£n √°nh ch·∫•t l∆∞·ª£ng th·ª±c t·∫ø

### **T·∫°i sao Reference Answer T·ªët H∆°n:**

```python
# Reference answer ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ:
‚úÖ C√πng ƒë·ªô d√†i v·ªõi generated answer
‚úÖ C√πng m·ª©c ƒë·ªô chi ti·∫øt  
‚úÖ C√πng style vi·∫øt
‚úÖ T·∫≠p trung v√†o c√¢u tr·∫£ l·ªùi, kh√¥ng ph·∫£i source document

# Context th√¨:
‚ùå Qu√° d√†i v√† chi ti·∫øt
‚ùå Ch·ª©a nhi·ªÅu th√¥ng tin kh√¥ng c·∫ßn thi·∫øt
‚ùå Style nh∆∞ document, kh√¥ng nh∆∞ answer
‚ùå Kh√¥ng ph√π h·ª£p ƒë·ªÉ so s√°nh answer quality
```

---

## üí° Khuy·∫øn Ngh·ªã

### **Khi n√†o d√πng Reference Answer:**
- ‚úÖ Khi c√≥ human-written reference
- ‚úÖ Evaluation dataset c√≥ ground truth
- ‚úÖ Mu·ªën ƒë√°nh gi√° ch√≠nh x√°c answer quality

### **Khi n√†o d√πng Context:**
- ‚ö†Ô∏è Ch·ªâ khi KH√îNG c√≥ reference answer
- ‚ö†Ô∏è Nh∆∞ m·ªôt fallback method
- ‚ö†Ô∏è C·∫ßn interpret k·∫øt qu·∫£ c·∫©n th·∫≠n

### **C·∫£i Thi·ªán Context-based Evaluation:**

```python
def _create_pseudo_reference(self, context: str, question: str) -> str:
    """T·∫°o pseudo-reference t·ª´ context thay v√¨ d√πng to√†n b·ªô context."""
    
    # S·ª≠ d·ª•ng LLM ƒë·ªÉ t·∫°o reference answer t·ª´ context
    prompt = f"""
    D·ª±a v√†o context sau, vi·∫øt m·ªôt c√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn cho c√¢u h·ªèi:
    
    Context: {context}
    Question: {question}
    
    Answer (1-2 c√¢u):
    """
    
    pseudo_reference = self.llm.invoke(prompt)
    return pseudo_reference.content

# S·ª≠ d·ª•ng pseudo-reference thay v√¨ raw context
if reference_answer:
    metrics = self._calculate_reference_metrics(answer, reference_answer)
else:
    pseudo_ref = self._create_pseudo_reference(context, question)
    metrics = self._calculate_reference_metrics(answer, pseudo_ref)
```

**K·∫øt lu·∫≠n:** Reference answer cho k·∫øt qu·∫£ ƒë√°nh gi√° **ch√≠nh x√°c v√† tin c·∫≠y h∆°n** context-based evaluation.