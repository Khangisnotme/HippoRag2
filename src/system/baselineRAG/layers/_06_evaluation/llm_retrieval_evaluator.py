# """
# llm_retrieval_evaluator.py

# Module Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ retrieval sá»­ dá»¥ng LLM lÃ m trá»ng tÃ i.
# ÄÃ¡nh giÃ¡ ngá»¯ cáº£nh, Ä‘á»™ liÃªn quan vÃ  cháº¥t lÆ°á»£ng tÃ i liá»‡u má»™t cÃ¡ch tá»± nhiÃªn.

# Input:
# - CÃ¢u há»i/truy váº¥n gá»‘c
# - Danh sÃ¡ch tÃ i liá»‡u Ä‘Æ°á»£c retrieve
# - Prompt Ä‘Ã¡nh giÃ¡ tÃ¹y chá»‰nh (optional)
# - Model LLM sá»­ dá»¥ng

# Output:  
# - Context Relevance Score: Äiá»ƒm liÃªn quan ngá»¯ cáº£nh (0-100)
# - Context Quality Score: Äiá»ƒm cháº¥t lÆ°á»£ng ná»™i dung (0-100)
# - Coverage Score: Äiá»ƒm bao phá»§ thÃ´ng tin (0-100)
# - Coherence Score: Äiá»ƒm nháº¥t quÃ¡n giá»¯a cÃ¡c tÃ i liá»‡u (0-100)
# - Overall Score: Äiá»ƒm tá»•ng thá»ƒ
# - Detailed Feedback: Pháº£n há»“i chi tiáº¿t tá»« LLM
# """

# from typing import List, Dict, Any, Optional
# from langchain_core.documents import Document
# from langchain_openai import ChatOpenAI
# from langchain.prompts import ChatPromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from langchain_core.runnables import RunnablePassthrough
# from dotenv import load_dotenv
# import re
# import json
# from dataclasses import dataclass

# # Load environment variables
# load_dotenv()

# @dataclass
# class LLMRetrievalResult:
#     """LÆ°u trá»¯ káº¿t quáº£ Ä‘Ã¡nh giÃ¡ LLM cho má»™t truy váº¥n."""
#     query_id: str
#     query_text: str
#     retrieved_docs: List[Document]
#     context_relevance: float = 0.0
#     context_quality: float = 0.0
#     coverage_score: float = 0.0
#     coherence_score: float = 0.0
#     overall_score: float = 0.0
#     detailed_feedback: str = ""

# # Prompt máº·c Ä‘á»‹nh cho Ä‘Ã¡nh giÃ¡ LLM
# DEFAULT_LLM_EVALUATION_PROMPT = """Báº¡n lÃ  chuyÃªn gia Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng há»‡ thá»‘ng retrieval trong RAG.
# Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ phÃ¹ há»£p cá»§a cÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c tÃ¬m kiáº¿m vá»›i cÃ¢u há»i Ä‘Æ°a ra.

# HÃ£y Ä‘Ã¡nh giÃ¡ theo 4 tiÃªu chÃ­ sau vÃ  cho Ä‘iá»ƒm tá»« 0-100:

# 1. CONTEXT RELEVANCE (Äá»™ liÃªn quan ngá»¯ cáº£nh):
#    - Má»©c Ä‘á»™ liÃªn quan trá»±c tiáº¿p cá»§a cÃ¡c tÃ i liá»‡u vá»›i cÃ¢u há»i
#    - TÃ i liá»‡u cÃ³ chá»©a thÃ´ng tin cáº§n thiáº¿t Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i khÃ´ng?
   
# 2. CONTEXT QUALITY (Cháº¥t lÆ°á»£ng ná»™i dung):
#    - Äá»™ chÃ­nh xÃ¡c, Ä‘áº§y Ä‘á»§ vÃ  cáº­p nháº­t cá»§a thÃ´ng tin
#    - Nguá»“n thÃ´ng tin cÃ³ Ä‘Ã¡ng tin cáº­y khÃ´ng?
   
# 3. COVERAGE (Äá»™ bao phá»§):
#    - CÃ¡c tÃ i liá»‡u cÃ³ bao phá»§ Ä‘áº§y Ä‘á»§ cÃ¡c khÃ­a cáº¡nh cá»§a cÃ¢u há»i khÃ´ng?
#    - CÃ³ thiáº¿u thÃ´ng tin quan trá»ng nÃ o khÃ´ng?
   
# 4. COHERENCE (TÃ­nh nháº¥t quÃ¡n):
#    - CÃ¡c tÃ i liá»‡u cÃ³ nháº¥t quÃ¡n vá»›i nhau khÃ´ng?
#    - CÃ³ mÃ¢u thuáº«n hoáº·c trÃ¹ng láº·p thÃ´ng tin khÃ´ng?

# Äá»ŠNH Dáº NG PHáº¢N Há»’I:
# ```json
# {
#     "context_relevance": <Ä‘iá»ƒm 0-100>,
#     "context_quality": <Ä‘iá»ƒm 0-100>, 
#     "coverage_score": <Ä‘iá»ƒm 0-100>,
#     "coherence_score": <Ä‘iá»ƒm 0-100>,
#     "overall_score": <Ä‘iá»ƒm trung bÃ¬nh>,
#     "detailed_analysis": {
#         "relevant_docs": ["doc_index_1", "doc_index_2", ...],
#         "irrelevant_docs": ["doc_index_3", ...],
#         "missing_information": ["thÃ´ng tin thiáº¿u 1", "thÃ´ng tin thiáº¿u 2", ...],
#         "contradictions": ["mÃ¢u thuáº«n 1", "mÃ¢u thuáº«n 2", ...],
#         "strengths": ["Ä‘iá»ƒm máº¡nh 1", "Ä‘iá»ƒm máº¡nh 2", ...],
#         "weaknesses": ["Ä‘iá»ƒm yáº¿u 1", "Ä‘iá»ƒm yáº¿u 2", ...]
#     },
#     "recommendations": ["khuyáº¿n nghá»‹ 1", "khuyáº¿n nghá»‹ 2", ...]
# }
# ```"""

# class LLMRetrievalEvaluator:
#     """
#     ÄÃ¡nh giÃ¡ há»‡ thá»‘ng retrieval sá»­ dá»¥ng LLM lÃ m trá»ng tÃ i.
    
#     Æ¯u Ä‘iá»ƒm:
#     - ÄÃ¡nh giÃ¡ ngá»¯ cáº£nh sÃ¢u sáº¯c
#     - Hiá»ƒu Ä‘Æ°á»£c Ã½ nghÄ©a semantic cá»§a cÃ¢u há»i vÃ  tÃ i liá»‡u
#     - PhÃ¡t hiá»‡n thÃ´ng tin thiáº¿u vÃ  mÃ¢u thuáº«n
#     - ÄÆ°a ra khuyáº¿n nghá»‹ cáº£i thiá»‡n
    
#     NhÆ°á»£c Ä‘iá»ƒm:
#     - Cháº­m hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng
#     - Chi phÃ­ cao hÆ¡n
#     - CÃ³ thá»ƒ khÃ´ng nháº¥t quÃ¡n giá»¯a cÃ¡c láº§n Ä‘Ã¡nh giÃ¡
#     """
    
#     def __init__(
#         self,
#         model_name: str = "gpt-4o-mini",
#         temperature: float = 0.1,
#         evaluation_prompt: str = DEFAULT_LLM_EVALUATION_PROMPT,
#         max_retries: int = 3
#     ):
#         """
#         Khá»Ÿi táº¡o LLM evaluator.
        
#         Args:
#             model_name: TÃªn model LLM sá»­ dá»¥ng
#             temperature: Äá»™ sÃ¡ng táº¡o (tháº¥p Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ nháº¥t quÃ¡n)
#             evaluation_prompt: Prompt hÆ°á»›ng dáº«n Ä‘Ã¡nh giÃ¡
#             max_retries: Sá»‘ láº§n thá»­ láº¡i khi parse JSON tháº¥t báº¡i
#         """
#         self.llm = ChatOpenAI(
#             model_name=model_name,
#             temperature=temperature
#         )
        
#         self.eval_prompt = ChatPromptTemplate.from_messages([
#             ("system", evaluation_prompt),
#             ("human", """TRUY Váº¤N: {query}

# CÃC TÃ€I LIá»†U ÄÆ¯á»¢C TÃŒM KIáº¾M:
# {documents}

# HÃ£y Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng retrieval theo Ä‘á»‹nh dáº¡ng JSON Ä‘Ã£ yÃªu cáº§u:""")
#         ])
        
#         self.chain = (
#             {
#                 "query": RunnablePassthrough(),
#                 "documents": RunnablePassthrough()
#             }
#             | self.eval_prompt
#             | self.llm
#             | StrOutputParser()
#         )
        
#         self.max_retries = max_retries
    
#     def evaluate_single_query(
#         self,
#         query: str,
#         documents: List[Document],
#         query_id: str = "query_1"
#     ) -> LLMRetrievalResult:
#         """
#         ÄÃ¡nh giÃ¡ retrieval cho má»™t truy váº¥n Ä‘Æ¡n láº» sá»­ dá»¥ng LLM.
        
#         Args:
#             query: CÃ¢u há»i/truy váº¥n gá»‘c
#             documents: Danh sÃ¡ch tÃ i liá»‡u Ä‘Æ°á»£c retrieve
#             query_id: ID cá»§a truy váº¥n
            
#         Returns:
#             LLMRetrievalResult chá»©a káº¿t quáº£ Ä‘Ã¡nh giÃ¡ chi tiáº¿t
#         """
#         if not documents:
#             return LLMRetrievalResult(
#                 query_id=query_id,
#                 query_text=query,
#                 retrieved_docs=[],
#                 detailed_feedback="KhÃ´ng cÃ³ tÃ i liá»‡u nÃ o Ä‘Æ°á»£c tÃ¬m kiáº¿m."
#             )
        
#         # Format documents cho LLM
#         doc_text = self._format_documents_for_llm(documents)
        
#         # Gá»i LLM Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
#         for attempt in range(self.max_retries):
#             try:
#                 evaluation_text = self.chain.invoke({
#                     "query": query,
#                     "documents": doc_text
#                 })
                
#                 # Parse káº¿t quáº£ JSON
#                 evaluation_data = self._parse_llm_response(evaluation_text)
                
#                 # Táº¡o káº¿t quáº£ Ä‘Ã¡nh giÃ¡
#                 result = LLMRetrievalResult(
#                     query_id=query_id,
#                     query_text=query,
#                     retrieved_docs=documents,
#                     context_relevance=evaluation_data.get("context_relevance", 0.0),
#                     context_quality=evaluation_data.get("context_quality", 0.0),
#                     coverage_score=evaluation_data.get("coverage_score", 0.0),
#                     coherence_score=evaluation_data.get("coherence_score", 0.0),
#                     overall_score=evaluation_data.get("overall_score", 0.0),
#                     detailed_feedback=json.dumps(evaluation_data, ensure_ascii=False, indent=2)
#                 )
                
#                 return result
                
#             except Exception as e:
#                 print(f"Attempt {attempt + 1} failed: {e}")
#                 if attempt == self.max_retries - 1:
#                     # Fallback: táº¡o káº¿t quáº£ vá»›i Ä‘iá»ƒm 0 vÃ  thÃ´ng bÃ¡o lá»—i
#                     return LLMRetrievalResult(
#                         query_id=query_id,
#                         query_text=query,
#                         retrieved_docs=documents,
#                         detailed_feedback=f"LLM evaluation failed after {self.max_retries} attempts: {str(e)}"
#                     )
        
#         return result
    
#     def evaluate_multiple_queries(
#         self,
#         queries_and_docs: List[tuple[str, List[Document], str]]
#     ) -> Dict[str, Any]:
#         """
#         ÄÃ¡nh giÃ¡ nhiá»u truy váº¥n sá»­ dá»¥ng LLM.
        
#         Args:
#             queries_and_docs: List of (query, documents, query_id)
            
#         Returns:
#             Dict chá»©a káº¿t quáº£ Ä‘Ã¡nh giÃ¡ tá»•ng há»£p
#         """
#         individual_results = []
        
#         # ÄÃ¡nh giÃ¡ tá»«ng truy váº¥n
#         for query, docs, query_id in queries_and_docs:
#             print(f"Äang Ä‘Ã¡nh giÃ¡ query: {query_id}...")
#             result = self.evaluate_single_query(query, docs, query_id)
#             individual_results.append(result)
        
#         # TÃ­nh cÃ¡c chá»‰ sá»‘ trung bÃ¬nh
#         avg_metrics = self._calculate_average_metrics(individual_results)
        
#         return {
#             "average_metrics": avg_metrics,
#             "individual_results": individual_results,
#             "total_queries": len(queries_and_docs),
#             "evaluation_summary": self._generate_summary(avg_metrics, individual_results)
#         }
    
#     def _format_documents_for_llm(self, documents: List[Document]) -> str:
#         """Format tÃ i liá»‡u thÃ nh text cho LLM Ä‘Ã¡nh giÃ¡."""
#         doc_text = ""
#         for i, doc in enumerate(documents, 1):
#             doc_text += f"--- TÃ€I LIá»†U {i} ---\n"
#             doc_text += f"Ná»™i dung: {doc.page_content[:1000]}...\n"  # Giá»›i háº¡n Ä‘á»™ dÃ i
#             if doc.metadata:
#                 doc_text += f"Metadata: {doc.metadata}\n"
#             doc_text += "\n"
#         return doc_text
    
#     def _parse_llm_response(self, response_text: str) -> Dict[str, Any]:
#         """Parse pháº£n há»“i JSON tá»« LLM."""
#         # TÃ¬m JSON block trong response
#         json_pattern = r'```json\s*(.*?)\s*```'
#         json_match = re.search(json_pattern, response_text, re.DOTALL)
        
#         if json_match:
#             json_str = json_match.group(1)
#         else:
#             # Thá»­ parse toÃ n bá»™ response nhÆ° JSON
#             json_str = response_text.strip()
        
#         try:
#             evaluation_data = json.loads(json_str)
            
#             # Validate vÃ  Ä‘áº£m báº£o cÃ³ Ä‘á»§ cÃ¡c trÆ°á»ng cáº§n thiáº¿t
#             required_fields = ["context_relevance", "context_quality", "coverage_score", "coherence_score"]
#             for field in required_fields:
#                 if field not in evaluation_data:
#                     evaluation_data[field] = 0.0
            
#             # TÃ­nh overall_score náº¿u chÆ°a cÃ³
#             if "overall_score" not in evaluation_data:
#                 scores = [evaluation_data[field] for field in required_fields]
#                 evaluation_data["overall_score"] = sum(scores) / len(scores)
            
#             return evaluation_data
            
#         except json.JSONDecodeError as e:
#             raise ValueError(f"Failed to parse LLM response as JSON: {e}\nResponse: {response_text}")
    
#     def _calculate_average_metrics(self, results: List[LLMRetrievalResult]) -> Dict[str, float]:
#         """TÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘ trung bÃ¬nh tá»« káº¿t quáº£ LLM."""
#         if not results:
#             return {}
        
#         # TÃ­nh trung bÃ¬nh cÃ¡c Ä‘iá»ƒm sá»‘
#         avg_context_relevance = sum(r.context_relevance for r in results) / len(results)
#         avg_context_quality = sum(r.context_quality for r in results) / len(results)
#         avg_coverage = sum(r.coverage_score for r in results) / len(results)
#         avg_coherence = sum(r.coherence_score for r in results) / len(results)
#         avg_overall = sum(r.overall_score for r in results) / len(results)
        
#         # Thá»‘ng kÃª bá»• sung
#         successful_evaluations = sum(1 for r in results if r.overall_score > 0)
        
#         return {
#             "avg_context_relevance": avg_context_relevance,
#             "avg_context_quality": avg_context_quality,
#             "avg_coverage_score": avg_coverage,
#             "avg_coherence_score": avg_coherence,
#             "avg_overall_score": avg_overall,
#             "successful_evaluations": successful_evaluations,
#             "success_rate": successful_evaluations / len(results) * 100
#         }
    
#     def _generate_summary(self, metrics: Dict[str, float], individual_results: List[LLMRetrievalResult]) -> str:
#         """Táº¡o bÃ¡o cÃ¡o tÃ³m táº¯t káº¿t quáº£ Ä‘Ã¡nh giÃ¡ LLM."""
#         summary = "=== BÃO CÃO ÄÃNH GIÃ LLM-BASED RETRIEVAL ===\n\n"
        
#         summary += "ğŸ“Š CÃC CHá»ˆ Sá» TRUNG BÃŒNH:\n"
#         summary += f"  â€¢ Context Relevance: {metrics.get('avg_context_relevance', 0):.1f}/100\n"
#         summary += f"  â€¢ Context Quality: {metrics.get('avg_context_quality', 0):.1f}/100\n"
#         summary += f"  â€¢ Coverage Score: {metrics.get('avg_coverage_score', 0):.1f}/100\n"
#         summary += f"  â€¢ Coherence Score: {metrics.get('avg_coherence_score', 0):.1f}/100\n"
#         summary += f"  â€¢ Overall Score: {metrics.get('avg_overall_score', 0):.1f}/100\n"
        
#         total_queries = len(individual_results)
#         successful = metrics.get('successful_evaluations', 0)
        
#         summary += f"\nğŸ“ˆ THá»NG KÃŠ ÄÃNH GIÃ:\n"
#         summary += f"  â€¢ Tá»•ng sá»‘ truy váº¥n: {total_queries}\n"
#         summary += f"  â€¢ ÄÃ¡nh giÃ¡ thÃ nh cÃ´ng: {successful}/{total_queries}\n"
#         summary += f"  â€¢ Tá»· lá»‡ thÃ nh cÃ´ng: {metrics.get('success_rate', 0):.1f}%\n"
        
#         # PhÃ¢n tÃ­ch cháº¥t lÆ°á»£ng
#         overall_score = metrics.get('avg_overall_score', 0)
#         relevance_score = metrics.get('avg_context_relevance', 0)
#         quality_score = metrics.get('avg_context_quality', 0)
#         coverage_score = metrics.get('avg_coverage_score', 0)
#         coherence_score = metrics.get('avg_coherence_score', 0)
        
#         summary += "\nğŸ¯ ÄÃNH GIÃ CHI TIáº¾T:\n"
        
#         if overall_score >= 80:
#             summary += "  â€¢ Cháº¥t lÆ°á»£ng tá»•ng thá»ƒ: XUáº¤T Sáº®C\n"
#         elif overall_score >= 60:
#             summary += "  â€¢ Cháº¥t lÆ°á»£ng tá»•ng thá»ƒ: Tá»T\n"
#         elif overall_score >= 40:
#             summary += "  â€¢ Cháº¥t lÆ°á»£ng tá»•ng thá»ƒ: TRUNG BÃŒNH\n"
#         else:
#             summary += "  â€¢ Cháº¥t lÆ°á»£ng tá»•ng thá»ƒ: Cáº¦N Cáº¢I THIá»†N\n"
        
#         # PhÃ¢n tÃ­ch tá»«ng khÃ­a cáº¡nh
#         aspects = [
#             ("Context Relevance", relevance_score, "Äá»™ liÃªn quan ngá»¯ cáº£nh"),
#             ("Context Quality", quality_score, "Cháº¥t lÆ°á»£ng ná»™i dung"),
#             ("Coverage", coverage_score, "Äá»™ bao phá»§ thÃ´ng tin"), 
#             ("Coherence", coherence_score, "TÃ­nh nháº¥t quÃ¡n")
#         ]
        
#         summary += "\nğŸ“‹ PHÃ‚N TÃCH Tá»ªNG KHÃA Cáº NH:\n"
#         for name, score, description in aspects:
#             if score >= 80:
#                 level = "XUáº¤T Sáº®C"
#             elif score >= 60:
#                 level = "Tá»T"
#             elif score >= 40:
#                 level = "TRUNG BÃŒNH"
#             else:
#                 level = "Yáº¾U"
#             summary += f"  â€¢ {description}: {score:.1f} ({level})\n"
        
#         # Khuyáº¿n nghá»‹
#         summary += "\nğŸ’¡ KHUYáº¾N NGHá»Š:\n"
#         if relevance_score < 60:
#             summary += "  â€¢ Cáº£i thiá»‡n thuáº­t toÃ¡n tÃ¬m kiáº¿m Ä‘á»ƒ tÄƒng Ä‘á»™ liÃªn quan\n"
#         if quality_score < 60:
#             summary += "  â€¢ Kiá»ƒm tra vÃ  nÃ¢ng cao cháº¥t lÆ°á»£ng nguá»“n dá»¯ liá»‡u\n"
#         if coverage_score < 60:
#             summary += "  â€¢ Má»Ÿ rá»™ng corpus Ä‘á»ƒ bao phá»§ Ä‘áº§y Ä‘á»§ hÆ¡n\n"
#         if coherence_score < 60:
#             summary += "  â€¢ Xá»­ lÃ½ duplicate vÃ  loáº¡i bá» thÃ´ng tin mÃ¢u thuáº«n\n"
        
#         return summary
    
#     def export_detailed_results(self, results: List[LLMRetrievalResult], output_file: str = "llm_evaluation_results.json"):
#         """
#         Xuáº¥t káº¿t quáº£ Ä‘Ã¡nh giÃ¡ chi tiáº¿t ra file JSON.
        
#         Args:
#             results: Danh sÃ¡ch káº¿t quáº£ Ä‘Ã¡nh giÃ¡
#             output_file: ÄÆ°á»ng dáº«n file output
#         """
#         export_data = []
        
#         for result in results:
#             export_data.append({
#                 "query_id": result.query_id,
#                 "query_text": result.query_text,
#                 "scores": {
#                     "context_relevance": result.context_relevance,
#                     "context_quality": result.context_quality,
#                     "coverage_score": result.coverage_score,
#                     "coherence_score": result.coherence_score,
#                     "overall_score": result.overall_score
#                 },
#                 "documents_count": len(result.retrieved_docs),
#                 "detailed_feedback": result.detailed_feedback
#             })
        
#         with open(output_file, 'w', encoding='utf-8') as f:
#             json.dump(export_data, f, ensure_ascii=False, indent=2)
        
#         print(f"ÄÃ£ xuáº¥t káº¿t quáº£ chi tiáº¿t ra file: {output_file}")

# if __name__ == "__main__":
#     """Test LLMRetrievalEvaluator"""
    
#     # Dá»¯ liá»‡u test máº«u
#     test_queries_and_docs = [
#         (
#             "RAG lÃ  gÃ¬ vÃ  hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o?",
#             [
#                 Document(
#                     page_content="RAG (Retrieval-Augmented Generation) lÃ  phÆ°Æ¡ng phÃ¡p káº¿t há»£p viá»‡c tÃ¬m kiáº¿m tÃ i liá»‡u liÃªn quan vá»›i kháº£ nÄƒng sinh text cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯. Há»‡ thá»‘ng RAG hoáº¡t Ä‘á»™ng báº±ng cÃ¡ch Ä‘áº§u tiÃªn tÃ¬m kiáº¿m cÃ¡c tÃ i liá»‡u cÃ³ liÃªn quan Ä‘áº¿n cÃ¢u há»i, sau Ä‘Ã³ sá»­ dá»¥ng thÃ´ng tin nÃ y Ä‘á»ƒ sinh ra cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c.",
#                     metadata={"source": "wiki_rag", "type": "definition"}
#                 ),
#                 Document(
#                     page_content="Há»‡ thá»‘ng RAG gá»“m 2 thÃ nh pháº§n chÃ­nh: Retriever (bá»™ tÃ¬m kiáº¿m) vÃ  Generator (bá»™ sinh text). Retriever sá»­ dá»¥ng embedding Ä‘á»ƒ tÃ¬m tÃ i liá»‡u liÃªn quan, sau Ä‘Ã³ Generator sá»­ dá»¥ng LLM Ä‘á»ƒ táº¡o cÃ¢u tráº£ lá»i dá»±a trÃªn ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p.",
#                     metadata={"source": "technical_guide", "type": "explanation"}
#                 ),
#                 Document(
#                     page_content="Thá»i tiáº¿t hÃ´m nay ráº¥t Ä‘áº¹p, náº¯ng vÃ ng vÃ  nhiá»‡t Ä‘á»™ khoáº£ng 25 Ä‘á»™ C. Dá»± bÃ¡o sáº½ cÃ³ mÆ°a rÃ o vÃ o chiá»u tá»‘i.",
#                     metadata={"source": "weather_report", "type": "irrelevant"}
#                 )
#             ],
#             "Q1"
#         ),
#         (
#             "Æ¯u Ä‘iá»ƒm cá»§a há»‡ thá»‘ng RAG so vá»›i LLM truyá»n thá»‘ng?",
#             [
#                 Document(
#                     page_content="RAG giÃºp LLM truy cáº­p thÃ´ng tin cáº­p nháº­t mÃ  khÃ´ng cáº§n retrain model. Äiá»u nÃ y giÃºp giáº£m chi phÃ­ vÃ  thá»i gian Ä‘Ã o táº¡o.",
#                     metadata={"source": "comparison_study", "type": "advantage"}
#                 )
#             ],
#             "Q2"
#         )
#     ]
    
#     print("Testing LLMRetrievalEvaluator...")
    
#     try:
#         evaluator = LLMRetrievalEvaluator(temperature=0.1)
        
#         # ÄÃ¡nh giÃ¡ má»™t truy váº¥n Ä‘Æ¡n láº»
#         single_result = evaluator.evaluate_single_query(
#             query=test_queries_and_docs[0][0],
#             documents=test_queries_and_docs[0][1],
#             query_id=test_queries_and_docs[0][2]
#         )
        
#         print("ğŸ“Š Káº¾T QUáº¢ ÄÃNH GIÃ Äá»¢N Láºº:")
#         print(f"Query: {single_result.query_text}")
#         print(f"Context Relevance: {single_result.context_relevance}")
#         print(f"Context Quality: {single_result.context_quality}")
#         print(f"Coverage Score: {single_result.coverage_score}")
#         print(f"Coherence Score: {single_result.coherence_score}")
#         print(f"Overall Score: {single_result.overall_score}")
        
#         # ÄÃ¡nh giÃ¡ nhiá»u truy váº¥n
#         results = evaluator.evaluate_multiple_queries(test_queries_and_docs)
        
#         print("\n" + results['evaluation_summary'])
        
#         # Xuáº¥t káº¿t quáº£ chi tiáº¿t
#         evaluator.export_detailed_results(results['individual_results'])
        
#     except Exception as e:
#         print(f"Test failed: {e}")
#         import traceback
#         traceback.print_exc()