# """
# rank_based_metrics_evaluator.py

# Module Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ retrieval sá»­ dá»¥ng cÃ¡c chá»‰ sá»‘ dá»±a trÃªn thá»© háº¡ng.
# Bao gá»“m: MRR (Mean Reciprocal Rank), MAP (Mean Average Precision), nDCG@K

# Input:
# - CÃ¢u há»i/truy váº¥n  
# - Danh sÃ¡ch tÃ i liá»‡u Ä‘Æ°á»£c retrieve theo thá»© tá»± ranking
# - Danh sÃ¡ch tÃ i liá»‡u liÃªn quan vá»›i Ä‘iá»ƒm relevance (ground truth)
# - GiÃ¡ trá»‹ K cho nDCG@K

# Output:
# - MRR: Trung bÃ¬nh nghá»‹ch Ä‘áº£o thá»© háº¡ng cá»§a káº¿t quáº£ Ä‘áº§u tiÃªn Ä‘Ãºng
# - MAP: Trung bÃ¬nh cá»§a Average Precision cho táº¥t cáº£ truy váº¥n
# - nDCG@K: Normalized Discounted Cumulative Gain táº¡i K
# - DCG@K: Discounted Cumulative Gain táº¡i K
# """

# from typing import List, Dict, Any, Tuple, Optional
# import numpy as np
# import math
# from dataclasses import dataclass

# @dataclass
# class RankedRetrievalResult:
#     """LÆ°u trá»¯ káº¿t quáº£ retrieval cÃ³ thá»© háº¡ng cho má»™t truy váº¥n."""
#     query_id: str
#     retrieved_docs: List[str]  # ID tÃ i liá»‡u theo thá»© tá»± ranking (cao -> tháº¥p)
#     relevance_scores: Dict[str, float]  # {doc_id: relevance_score}
#     k_value: int = 10

# class RankBasedMetricsEvaluator:
#     """
#     ÄÃ¡nh giÃ¡ há»‡ thá»‘ng retrieval sá»­ dá»¥ng cÃ¡c chá»‰ sá»‘ dá»±a trÃªn thá»© háº¡ng.
    
#     CÃ¡c chá»‰ sá»‘ Ä‘Æ°á»£c tÃ­nh toÃ¡n:
#     1. MRR (Mean Reciprocal Rank): 1/|Q| * Î£(1/rank_i)
#     2. MAP (Mean Average Precision): 1/|Q| * Î£(AP_q)  
#     3. nDCG@K: DCG@K / IDCG@K
#     4. DCG@K: Î£(rel_i / log2(i+1)) vá»›i i tá»« 1 Ä‘áº¿n K
    
#     Trong Ä‘Ã³:
#     - rank_i: Thá»© háº¡ng cá»§a tÃ i liá»‡u liÃªn quan Ä‘áº§u tiÃªn cho truy váº¥n i
#     - AP_q: Average Precision cho truy váº¥n q
#     - rel_i: Äiá»ƒm relevance cá»§a tÃ i liá»‡u táº¡i vá»‹ trÃ­ i
#     - IDCG: Ideal DCG (DCG tá»‘i Ä‘a cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c)
#     """
    
#     def __init__(self, relevance_threshold: float = 1.0):
#         """
#         Khá»Ÿi táº¡o evaluator.
        
#         Args:
#             relevance_threshold: NgÆ°á»¡ng Ä‘á»ƒ coi tÃ i liá»‡u lÃ  "liÃªn quan" (cho MRR, MAP)
#         """
#         self.relevance_threshold = relevance_threshold
    
#     def calculate_mrr_single_query(
#         self,
#         retrieved_docs: List[str],
#         relevance_scores: Dict[str, float],
#         query_id: str = "query_1"
#     ) -> Dict[str, Any]:
#         """
#         TÃ­nh MRR cho má»™t truy váº¥n Ä‘Æ¡n láº».
        
#         Args:
#             retrieved_docs: Danh sÃ¡ch tÃ i liá»‡u theo thá»© tá»± ranking
#             relevance_scores: Dict Ä‘iá»ƒm relevance {doc_id: score}
#             query_id: ID truy váº¥n
            
#         Returns:
#             Dict chá»©a thÃ´ng tin MRR cho truy váº¥n nÃ y
#         """
#         # TÃ¬m tÃ i liá»‡u liÃªn quan Ä‘áº§u tiÃªn
#         first_relevant_rank = None
        
#         for rank, doc_id in enumerate(retrieved_docs, 1):
#             if doc_id in relevance_scores and relevance_scores[doc_id] >= self.relevance_threshold:
#                 first_relevant_rank = rank
#                 break
        
#         # TÃ­nh reciprocal rank
#         reciprocal_rank = 1.0 / first_relevant_rank if first_relevant_rank else 0.0
        
#         return {
#             "query_id": query_id,
#             "first_relevant_rank": first_relevant_rank,
#             "reciprocal_rank": reciprocal_rank,
#             "has_relevant_result": first_relevant_rank is not None
#         }
    
#     def calculate_average_precision_single_query(
#         self,
#         retrieved_docs: List[str],
#         relevance_scores: Dict[str, float],
#         query_id: str = "query_1"
#     ) -> Dict[str, Any]:
#         """
#         TÃ­nh Average Precision cho má»™t truy váº¥n Ä‘Æ¡n láº».
        
#         AP = (1/R) * Î£(Precision@k * rel_k)
#         Trong Ä‘Ã³ R lÃ  tá»•ng sá»‘ tÃ i liá»‡u liÃªn quan
        
#         Args:
#             retrieved_docs: Danh sÃ¡ch tÃ i liá»‡u theo thá»© tá»± ranking
#             relevance_scores: Dict Ä‘iá»ƒm relevance
#             query_id: ID truy váº¥n
            
#         Returns:
#             Dict chá»©a thÃ´ng tin AP cho truy váº¥n nÃ y
#         """
#         relevant_docs = [doc for doc, score in relevance_scores.items() 
#                         if score >= self.relevance_threshold]
        
#         if not relevant_docs:
#             return {
#                 "query_id": query_id,
#                 "average_precision": 0.0,
#                 "relevant_docs_count": 0,
#                 "precision_at_k_values": []
#             }
        
#         precision_sum = 0.0
#         relevant_found = 0
#         precision_values = []
        
#         for k, doc_id in enumerate(retrieved_docs, 1):
#             if doc_id in relevance_scores and relevance_scores[doc_id] >= self.relevance_threshold:
#                 relevant_found += 1
#                 precision_at_k = relevant_found / k
#                 precision_sum += precision_at_k
#                 precision_values.append({
#                     "k": k,
#                     "doc_id": doc_id,
#                     "precision_at_k": precision_at_k
#                 })
        
#         average_precision = precision_sum / len(relevant_docs) if relevant_docs else 0.0
        
#         return {
#             "query_id": query_id,
#             "average_precision": average_precision,
#             "relevant_docs_count": len(relevant_docs),
#             "relevant_found": relevant_found,
#             "precision_at_k_values": precision_values
#         }
    
#     def calculate_dcg_at_k(
#         self,
#         retrieved_docs: List[str],
#         relevance_scores: Dict[str, float],
#         k: int = 10
#     ) -> float:
#         """
#         TÃ­nh DCG@K (Discounted Cumulative Gain).
        
#         DCG@K = Î£(rel_i / log2(i+1)) vá»›i i tá»« 1 Ä‘áº¿n K
        
#         Args:
#             retrieved_docs: Danh sÃ¡ch tÃ i liá»‡u theo thá»© tá»± ranking
#             relevance_scores: Dict Ä‘iá»ƒm relevance
#             k: Sá»‘ lÆ°á»£ng tÃ i liá»‡u top-K
            
#         Returns:
#             GiÃ¡ trá»‹ DCG@K
#         """
#         dcg = 0.0
        
#         for i, doc_id in enumerate(retrieved_docs[:k], 1):
#             relevance = relevance_scores.get(doc_id, 0.0)
#             if relevance > 0:
#                 dcg += relevance / math.log2(i + 1)
        
#         return dcg
    
#     def calculate_ideal_dcg_at_k(
#         self,
#         relevance_scores: Dict[str, float],
#         k: int = 10
#     ) -> float:
#         """
#         TÃ­nh IDCG@K (Ideal DCG) - DCG tá»‘i Ä‘a cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c.
        
#         Args:
#             relevance_scores: Dict Ä‘iá»ƒm relevance
#             k: Sá»‘ lÆ°á»£ng tÃ i liá»‡u top-K
            
#         Returns:
#             GiÃ¡ trá»‹ IDCG@K
#         """
#         # Sáº¯p xáº¿p cÃ¡c Ä‘iá»ƒm relevance theo thá»© tá»± giáº£m dáº§n
#         sorted_relevances = sorted(relevance_scores.values(), reverse=True)
        
#         idcg = 0.0
#         for i, relevance in enumerate(sorted_relevances[:k], 1):
#             if relevance > 0:
#                 idcg += relevance / math.log2(i + 1)
        
#         return idcg
    
#     def calculate_ndcg_single_query(
#         self,
#         retrieved_docs: List[str],
#         relevance_scores: Dict[str, float],
#         k: int = 10,
#         query_id: str = "query_1"
#     ) -> Dict[str, Any]:
#         """
#         TÃ­nh nDCG@K cho má»™t truy váº¥n Ä‘Æ¡n láº».
        
#         nDCG@K = DCG@K / IDCG@K
        
#         Args:
#             retrieved_docs: Danh sÃ¡ch tÃ i liá»‡u theo thá»© tá»± ranking
#             relevance_scores: Dict Ä‘iá»ƒm relevance
#             k: Sá»‘ lÆ°á»£ng tÃ i liá»‡u top-K
#             query_id: ID truy váº¥n
            
#         Returns:
#             Dict chá»©a thÃ´ng tin nDCG cho truy váº¥n nÃ y
#         """
#         dcg_k = self.calculate_dcg_at_k(retrieved_docs, relevance_scores, k)
#         idcg_k = self.calculate_ideal_dcg_at_k(relevance_scores, k)
        
#         ndcg_k = dcg_k / idcg_k if idcg_k > 0 else 0.0
        
#         return {
#             "query_id": query_id,
#             "dcg_at_k": dcg_k,
#             "idcg_at_k": idcg_k,
#             "ndcg_at_k": ndcg_k,
#             "k_value": k
#         }
    
#     def evaluate_single_query(
#         self,
#         retrieved_docs: List[str],
#         relevance_scores: Dict[str, float],
#         k: int = 10,
#         query_id: str = "query_1"
#     ) -> Dict[str, Any]:
#         """
#         ÄÃ¡nh giÃ¡ Ä‘áº§y Ä‘á»§ cho má»™t truy váº¥n Ä‘Æ¡n láº» (táº¥t cáº£ cÃ¡c chá»‰ sá»‘ rank-based).
        
#         Args:
#             retrieved_docs: Danh sÃ¡ch tÃ i liá»‡u theo thá»© tá»± ranking
#             relevance_scores: Dict Ä‘iá»ƒm relevance
#             k: GiÃ¡ trá»‹ K cho nDCG@K
#             query_id: ID truy váº¥n
            
#         Returns:
#             Dict chá»©a táº¥t cáº£ cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡
#         """
#         # TÃ­nh MRR
#         mrr_result = self.calculate_mrr_single_query(retrieved_docs, relevance_scores, query_id)
        
#         # TÃ­nh AP
#         ap_result = self.calculate_average_precision_single_query(retrieved_docs, relevance_scores, query_id)
        
#         # TÃ­nh nDCG@K
#         ndcg_result = self.calculate_ndcg_single_query(retrieved_docs, relevance_scores, k, query_id)
        
#         return {
#             "query_id": query_id,
#             "mrr": mrr_result["reciprocal_rank"],
#             "average_precision": ap_result["average_precision"],
#             "ndcg_at_k": ndcg_result["ndcg_at_k"],
#             "dcg_at_k": ndcg_result["dcg_at_k"],
#             "idcg_at_k": ndcg_result["idcg_at_k"],
#             "first_relevant_rank": mrr_result["first_relevant_rank"],
#             "relevant_docs_count": ap_result["relevant_docs_count"],
#             "relevant_found": ap_result["relevant_found"],
#             "k_value": k,
#             "detailed_mrr": mrr_result,
#             "detailed_ap": ap_result,
#             "detailed_ndcg": ndcg_result
#         }
    
#     def evaluate_multiple_queries(
#         self,
#         ranked_results: List[RankedRetrievalResult]
#     ) -> Dict[str, Any]:
#         """
#         ÄÃ¡nh giÃ¡ cho nhiá»u truy váº¥n vÃ  tÃ­nh cÃ¡c chá»‰ sá»‘ trung bÃ¬nh.
        
#         Args:
#             ranked_results: Danh sÃ¡ch káº¿t quáº£ retrieval cÃ³ ranking
            
#         Returns:
#             Dict chá»©a cÃ¡c chá»‰ sá»‘ trung bÃ¬nh vÃ  chi tiáº¿t
#         """
#         individual_results = []
        
#         # ÄÃ¡nh giÃ¡ tá»«ng truy váº¥n
#         for result in ranked_results:
#             eval_result = self.evaluate_single_query(
#                 retrieved_docs=result.retrieved_docs,
#                 relevance_scores=result.relevance_scores,
#                 k=result.k_value,
#                 query_id=result.query_id
#             )
#             individual_results.append(eval_result)
        
#         # TÃ­nh cÃ¡c chá»‰ sá»‘ trung bÃ¬nh
#         avg_metrics = self._calculate_average_metrics(individual_results)
        
#         return {
#             "average_metrics": avg_metrics,
#             "individual_results": individual_results,
#             "total_queries": len(ranked_results),
#             "evaluation_summary": self._generate_summary(avg_metrics, individual_results)
#         }
    
#     def _calculate_average_metrics(self, results: List[Dict]) -> Dict[str, float]:
#         """TÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘ trung bÃ¬nh."""
#         if not results:
#             return {}
        
#         # TÃ­nh Mean Reciprocal Rank
#         mrr_values = [r["mrr"] for r in results]
#         mean_mrr = np.mean(mrr_values)
        
#         # TÃ­nh Mean Average Precision  
#         map_values = [r["average_precision"] for r in results]
#         mean_ap = np.mean(map_values)
        
#         # TÃ­nh Mean nDCG@K
#         ndcg_values = [r["ndcg_at_k"] for r in results]
#         mean_ndcg = np.mean(ndcg_values)
        
#         # TÃ­nh Mean DCG@K
#         dcg_values = [r["dcg_at_k"] for r in results]
#         mean_dcg = np.mean(dcg_values)
        
#         return {
#             "mean_reciprocal_rank": mean_mrr,
#             "mean_average_precision": mean_ap,
#             "mean_ndcg_at_k": mean_ndcg,
#             "mean_dcg_at_k": mean_dcg,
#             "queries_with_relevant_results": sum(1 for r in results if r["first_relevant_rank"] is not None),
#             "average_first_relevant_rank": np.mean([r["first_relevant_rank"] for r in results if r["first_relevant_rank"] is not None]) if any(r["first_relevant_rank"] for r in results) else None
#         }
    
#     def _generate_summary(self, metrics: Dict[str, float], individual_results: List[Dict]) -> str:
#         """Táº¡o bÃ¡o cÃ¡o tÃ³m táº¯t káº¿t quáº£ Ä‘Ã¡nh giÃ¡."""
#         summary = "=== BÃO CÃO ÄÃNH GIÃ RANK-BASED METRICS ===\n\n"
        
#         summary += "ğŸ“Š CÃC CHá»ˆ Sá» TRUNG BÃŒNH:\n"
#         summary += f"  â€¢ Mean Reciprocal Rank (MRR): {metrics.get('mean_reciprocal_rank', 0):.3f}\n"
#         summary += f"  â€¢ Mean Average Precision (MAP): {metrics.get('mean_average_precision', 0):.3f}\n"
#         summary += f"  â€¢ Mean nDCG@K: {metrics.get('mean_ndcg_at_k', 0):.3f}\n"
#         summary += f"  â€¢ Mean DCG@K: {metrics.get('mean_dcg_at_k', 0):.3f}\n"
        
#         total_queries = len(individual_results)
#         queries_with_results = metrics.get('queries_with_relevant_results', 0)
        
#         summary += f"\nğŸ“ˆ THá»NG KÃŠ TRUY Váº¤N:\n"
#         summary += f"  â€¢ Tá»•ng sá»‘ truy váº¥n: {total_queries}\n"
#         summary += f"  â€¢ Truy váº¥n cÃ³ káº¿t quáº£ liÃªn quan: {queries_with_results}/{total_queries}\n"
#         summary += f"  â€¢ Tá»· lá»‡ thÃ nh cÃ´ng: {queries_with_results/total_queries*100:.1f}%\n"
        
#         if metrics.get('average_first_relevant_rank'):
#             summary += f"  â€¢ Vá»‹ trÃ­ trung bÃ¬nh cá»§a káº¿t quáº£ Ä‘áº§u tiÃªn: {metrics['average_first_relevant_rank']:.1f}\n"
        
#         # ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng
#         mrr = metrics.get('mean_reciprocal_rank', 0)
#         map_score = metrics.get('mean_average_precision', 0)
#         ndcg = metrics.get('mean_ndcg_at_k', 0)
        
#         summary += "\nğŸ¯ ÄÃNH GIÃ CHáº¤T LÆ¯á»¢NG:\n"
        
#         if mrr >= 0.8:
#             summary += "  â€¢ MRR: XUáº¤T Sáº®C (káº¿t quáº£ liÃªn quan thÆ°á»ng á»Ÿ vá»‹ trÃ­ cao)\n"
#         elif mrr >= 0.6:
#             summary += "  â€¢ MRR: Tá»T (káº¿t quáº£ liÃªn quan á»Ÿ vá»‹ trÃ­ khÃ¡ cao)\n"
#         elif mrr >= 0.4:
#             summary += "  â€¢ MRR: TRUNG BÃŒNH (káº¿t quáº£ liÃªn quan á»Ÿ vá»‹ trÃ­ trung bÃ¬nh)\n"
#         else:
#             summary += "  â€¢ MRR: Cáº¦N Cáº¢I THIá»†N (káº¿t quáº£ liÃªn quan á»Ÿ vá»‹ trÃ­ tháº¥p)\n"
        
#         if map_score >= 0.7:
#             summary += "  â€¢ MAP: CAO (precision tá»‘t qua nhiá»u vá»‹ trÃ­)\n"
#         elif map_score >= 0.5:
#             summary += "  â€¢ MAP: TRUNG BÃŒNH\n"
#         else:
#             summary += "  â€¢ MAP: THáº¤P (precision kÃ©m qua cÃ¡c vá»‹ trÃ­)\n"
        
#         if ndcg >= 0.8:
#             summary += "  â€¢ nDCG@K: XUáº¤T Sáº®C (ranking ráº¥t tá»‘t)\n"
#         elif ndcg >= 0.6:
#             summary += "  â€¢ nDCG@K: Tá»T (ranking khÃ¡ tá»‘t)\n"
#         elif ndcg >= 0.4:
#             summary += "  â€¢ nDCG@K: TRUNG BÃŒNH (ranking cáº§n cáº£i thiá»‡n)\n"
#         else:
#             summary += "  â€¢ nDCG@K: THáº¤P (ranking kÃ©m)\n"
        
#         return summary

# if __name__ == "__main__":
#     """Test RankBasedMetricsEvaluator"""
    
#     # Dá»¯ liá»‡u test máº«u vá»›i Ä‘iá»ƒm relevance
#     test_results = [
#         RankedRetrievalResult(
#             query_id="Q1",
#             retrieved_docs=["doc1", "doc2", "doc3", "doc4", "doc5"],
#             relevance_scores={"doc1": 3.0, "doc2": 0.0, "doc3": 2.0, "doc6": 3.0},  # doc6 khÃ´ng Ä‘Æ°á»£c retrieve
#             k_value=5
#         ),
#         RankedRetrievalResult(
#             query_id="Q2",
#             retrieved_docs=["doc7", "doc8", "doc9", "doc10"],
#             relevance_scores={"doc11": 2.0, "doc12": 1.0},  # KhÃ´ng cÃ³ doc nÃ o liÃªn quan Ä‘Æ°á»£c retrieve
#             k_value=5
#         ),
#         RankedRetrievalResult(
#             query_id="Q3",
#             retrieved_docs=["doc13", "doc14", "doc15"],
#             relevance_scores={"doc13": 3.0, "doc14": 2.0, "doc15": 1.0},  # Táº¥t cáº£ Ä‘á»u Ä‘Æ°á»£c retrieve
#             k_value=5
#         )
#     ]
    
#     print("Testing RankBasedMetricsEvaluator...")
    
#     try:
#         evaluator = RankBasedMetricsEvaluator(relevance_threshold=1.0)
        
#         # ÄÃ¡nh giÃ¡ nhiá»u truy váº¥n
#         results = evaluator.evaluate_multiple_queries(test_results)
        
#         print("ğŸ“Š Káº¾T QUáº¢ ÄÃNH GIÃ:")
#         print(f"MRR: {results['average_metrics']['mean_reciprocal_rank']:.3f}")
#         print(f"MAP: {results['average_metrics']['mean_average_precision']:.3f}")
#         print(f"nDCG@K: {results['average_metrics']['mean_ndcg_at_k']:.3f}")
        
#         print("\n" + results['evaluation_summary'])
        
#         print("\nğŸ“‹ CHI TIáº¾T Tá»ªNG TRUY Váº¤N:")
#         for result in results['individual_results']:
#             print(f"Query {result['query_id']}:")
#             print(f"  - MRR: {result['mrr']:.3f}")
#             print(f"  - AP: {result['average_precision']:.3f}")
#             print(f"  - nDCG@{result['k_value']}: {result['ndcg_at_k']:.3f}")
#             print(f"  - Vá»‹ trÃ­ Ä‘áº§u tiÃªn liÃªn quan: {result['first_relevant_rank']}")
#             print()
            
#     except Exception as e:
#         print(f"Test failed: {e}")
#         import traceback
#         traceback.print_exc()
