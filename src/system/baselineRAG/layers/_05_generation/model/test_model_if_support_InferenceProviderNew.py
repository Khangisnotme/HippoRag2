"""
Simple test script to check if InferenceClient works with small Vietnamese models
"""

from huggingface_hub import InferenceClient
from dotenv import load_dotenv
import os
import time

load_dotenv()

def test_model(model_name, test_prompt="Xin ch√†o, b·∫°n c√≥ kh·ªèe kh√¥ng?"):
    """Test a single model"""
    print(f"\n{'='*60}")
    print(f"Testing model: {model_name}")
    print(f"{'='*60}")
    
    hf_token = os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACE_TOKEN") or os.getenv("HUGGINGFACE_API_KEY")
    if not hf_token:
        print("‚ùå No HF_TOKEN found in environment variables")
        return False
    
    try:
        client = InferenceClient(token=hf_token)
        
        # Test 1: Basic text generation
        print("üîÑ Testing text generation...")
        response = client.text_generation(
            prompt=test_prompt,
            model=model_name,
            max_new_tokens=100,
            temperature=0.1
        )
        print(f"‚úÖ Text generation works:")
        print(f"Response: {response}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Text generation failed: {e}")
        
        # Test 2: Try chat completion if text generation fails
        try:
            print("üîÑ Testing chat completion...")
            messages = [{"role": "user", "content": test_prompt}]
            response = client.chat_completion(
                messages=messages,
                model=model_name,
                max_tokens=100,
                temperature=0.1
            )
            print(f"‚úÖ Chat completion works:")
            print(f"Response: {response.choices[0].message.content}")
            return True
            
        except Exception as e2:
            print(f"‚ùå Chat completion failed: {e2}")
            return False

def test_all_vietnamese_models():
    """Test all Vietnamese models"""
    models_to_test = [
        # Small models first
        "AITeamVN/Vi-Qwen2-1.5B-RAG",
        "AITeamVN/Vi-Qwen2-3B-RAG",
        "AITeamVN/GRPO-VI-Qwen2-3B-RAG",
        
        # Backup models
        "Qwen/Qwen2.5-1.5B-Instruct", 
        "Qwen/Qwen2.5-3B-Instruct",
        
        # Alternative small models
        "microsoft/DialoGPT-small",
        "gpt2",
    ]
    
    working_models = []
    failed_models = []
    
    print("üöÄ Starting Vietnamese Model Test Suite")
    print(f"Testing {len(models_to_test)} models...")
    
    for model in models_to_test:
        try:
            success = test_model(model)
            if success:
                working_models.append(model)
                print(f"‚úÖ {model} - WORKING")
            else:
                failed_models.append(model)
                print(f"‚ùå {model} - FAILED")
        except Exception as e:
            failed_models.append(model)
            print(f"‚ùå {model} - ERROR: {e}")
        
        # Small delay between tests
        time.sleep(2)
    
    # Summary
    print(f"\n{'='*60}")
    print("üìä TEST SUMMARY")
    print(f"{'='*60}")
    print(f"‚úÖ Working models ({len(working_models)}):")
    for model in working_models:
        print(f"   - {model}")
    
    print(f"\n‚ùå Failed models ({len(failed_models)}):")
    for model in failed_models:
        print(f"   - {model}")
    
    if working_models:
        print(f"\nüéâ Recommended model to use: {working_models[0]}")
    else:
        print(f"\nüòû No models are working. Check your HF_TOKEN and internet connection.")

def quick_test_single_model():
    """Quick test for a single model"""
    model_name = input("Enter model name to test (or press Enter for default): ").strip()
    if not model_name:
        model_name = "AITeamVN/GRPO-VI-Qwen2-3B-RAG"
    
    test_question = input("Enter test question (or press Enter for default): ").strip()
    if not test_question:
        test_question = "Machine learning l√† g√¨?"
    
    print(f"\nüîç Quick testing: {model_name}")
    test_model(model_name, test_question)

def test_with_rag_format():
    """Test with proper RAG format"""
    model_name = "AITeamVN/GRPO-VI-Qwen2-3B-RAG"
    
    rag_prompt = """Ch√∫ √Ω c√°c y√™u c·∫ßu sau:
- C√¢u tr·∫£ l·ªùi ph·∫£i ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß n·∫øu ng·ªØ c·∫£nh c√≥ c√¢u tr·∫£ l·ªùi. 
- Ch·ªâ s·ª≠ d·ª•ng c√°c th√¥ng tin c√≥ trong ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p.
- Ch·ªâ c·∫ßn t·ª´ ch·ªëi tr·∫£ l·ªùi v√† kh√¥ng suy lu·∫≠n g√¨ th√™m n·∫øu ng·ªØ c·∫£nh kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi.
H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh:
### Ng·ªØ c·∫£nh :
Machine Learning l√† m·ªôt nh√°nh c·ªßa tr√≠ tu·ªá nh√¢n t·∫°o cho ph√©p m√°y t√≠nh h·ªçc t·ª´ d·ªØ li·ªáu.

### C√¢u h·ªèi :
Machine learning l√† g√¨?

### Tr·∫£ l·ªùi :"""
    
    print(f"\nüéØ Testing RAG format with: {model_name}")
    test_model(model_name, rag_prompt)

if __name__ == "__main__":
    print("ü§ñ Vietnamese Model Tester")
    print("Choose an option:")
    print("1. Test all Vietnamese models")
    print("2. Quick test single model")
    print("3. Test with RAG format")
    print("4. Exit")
    
    choice = input("\nEnter your choice (1-4): ").strip()
    
    if choice == "1":
        test_all_vietnamese_models()
    elif choice == "2":
        quick_test_single_model()
    elif choice == "3":
        test_with_rag_format()
    elif choice == "4":
        print("üëã Goodbye!")
    else:
        print("‚ùå Invalid choice. Running full test...")
        test_all_vietnamese_models()
