D·ª±a tr√™n k·∫øt qu·∫£ t√¨m ki·∫øm, t√¥i c√≥ th·ªÉ cung c·∫•p cho b·∫°n th√¥ng tin v·ªÅ c√°c m√¥ h√¨nh RAG 3B t·ªët nh·∫•t cho ti·∫øng Vi·ªát:

## C√°c m√¥ h√¨nh RAG 3B t·ªët nh·∫•t cho ti·∫øng Vi·ªát

### 1. **Vi-Qwen2-3B-RAG** (AITeamVN)

[**Vi-Qwen2-3B-RAG**](https://huggingface.co/AITeamVN/Vi-Qwen2-3B-RAG) l√† m·ªôt m√¥ h√¨nh ƒë∆∞·ª£c thi·∫øt k·∫ø **chuy√™n bi·ªát cho RAG tasks** v·ªõi nh·ªØng ∆∞u ƒëi·ªÉm n·ªïi b·∫≠t:

**T√≠nh nƒÉng ch√≠nh:**
- Kh·∫£ nƒÉng **ch·ªëng nhi·ªÅu**: Tr√≠ch xu·∫•t th√¥ng tin h·ªØu √≠ch t·ª´ t√†i li·ªáu c√≥ nhi·ªÖu (1 positive + 4 negative)
- **Lo·∫°i b·ªè negative**: T·ª´ ch·ªëi tr·∫£ l·ªùi khi kh√¥ng c√≥ th√¥ng tin c·∫ßn thi·∫øt
- **T√≠ch h·ª£p th√¥ng tin**: X·ª≠ l√Ω c√¢u h·ªèi ph·ª©c t·∫°p c·∫ßn k·∫øt h·ª£p nhi·ªÅu ngu·ªìn
- **X√°c ƒë·ªãnh positive/negative**: ƒê·ªô ch√≠nh x√°c x·∫•p x·ªâ **99%**
- H·ªó tr·ª£ ng·ªØ c·∫£nh l√™n ƒë·∫øn **8192 tokens**

### 2. **Llama-3.2-3B-Instruct-Frog** (phamhai)

[**Llama-3.2-3B-Instruct-Frog**](https://huggingface.co/phamhai/Llama-3.2-3B-Instruct-Frog) ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a **ƒë·∫∑c bi·ªát cho RAG ti·∫øng Vi·ªát**:

**ƒê·∫∑c ƒëi·ªÉm:**
- D·ª±a tr√™n Llama-3.2-3B-Instruct
- **131K context length** - ƒë·ªô d√†i ng·ªØ c·∫£nh r·∫•t l·ªõn
- T·ªëi ∆∞u cho tri·ªÉn khai tr√™n thi·∫øt b·ªã bi√™n (laptop, smartphone, Jetson Xavier)
- T·∫≠p trung ch√≠nh v√†o **t·ªëi ∆∞u kh·∫£ nƒÉng RAG**

### 3. **Arcee-VyLinh** (arcee-ai)

[**Arcee-VyLinh**](https://huggingface.co/arcee-ai/Arcee-VyLinh) l√† m√¥ h√¨nh 3B parameters m·∫°nh m·∫Ω cho ti·∫øng Vi·ªát:

**∆Øu ƒëi·ªÉm:**
- D·ª±a tr√™n Qwen2.5-3B
- **32K tokens context length**
- Hi·ªáu su·∫•t c·∫°nh tranh v·ªõi c√°c m√¥ h√¨nh 4B-8B parameters
- ƒê√†o t·∫°o qua quy tr√¨nh DPO (Direct Preference Optimization)

## So s√°nh v√† khuy·∫øn ngh·ªã

**Cho RAG chuy√™n s√¢u:** **Vi-Qwen2-3B-RAG** l√† l·ª±a ch·ªçn t·ªët nh·∫•t v√¨ ƒë∆∞·ª£c thi·∫øt k·∫ø chuy√™n bi·ªát cho RAG tasks v·ªõi kh·∫£ nƒÉng x·ª≠ l√Ω nhi·ªÖu v√† t√≠ch h·ª£p th√¥ng tin xu·∫•t s·∫Øc.

**Cho ng·ªØ c·∫£nh d√†i:** **Llama-3.2-3B-Instruct-Frog** v·ªõi 131K context length ph√π h·ª£p cho c√°c t√†i li·ªáu d√†i.

**Cho m·ª•c ƒë√≠ch t·ªïng qu√°t:** **Arcee-VyLinh** c√¢n b·∫±ng t·ªët gi·ªØa hi·ªáu su·∫•t RAG v√† kh·∫£ nƒÉng x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n.

T·∫•t c·∫£ c√°c m√¥ h√¨nh n√†y ƒë·ªÅu c√≥ version quantized (GGUF) ƒë·ªÉ tri·ªÉn khai hi·ªáu qu·∫£ tr√™n ph·∫ßn c·ª©ng h·∫°n ch·∫ø, v√† ƒë·ªÅu ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a ƒë·∫∑c bi·ªát cho ti·∫øng Vi·ªát v·ªõi hi·ªáu su·∫•t v∆∞·ª£t tr·ªôi so v·ªõi c√°c m√¥ h√¨nh ƒëa ng√¥n ng·ªØ th√¥ng th∆∞·ªùng.


---


D·ª±a tr√™n k·∫øt qu·∫£ t√¨m ki·∫øm, ƒë√¢y l√† th√¥ng tin v·ªÅ c√°c m√¥ h√¨nh 1.5B t·ªët nh·∫•t cho RAG ti·∫øng Vi·ªát:

## C√°c m√¥ h√¨nh RAG 1.5B t·ªët nh·∫•t cho ti·∫øng Vi·ªát

### 1. **Vi-Qwen2-1.5B-RAG** (AITeamVN) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

[**Vi-Qwen2-1.5B-RAG**](https://huggingface.co/AITeamVN/Vi-Qwen2-1.5B-RAG) l√† l·ª±a ch·ªçn **h√†ng ƒë·∫ßu** cho RAG 1.5B:

**ƒê·∫∑c ƒëi·ªÉm n·ªïi b·∫≠t:**
- **Thi·∫øt k·∫ø chuy√™n bi·ªát cho RAG**: T∆∞∆°ng t·ª± nh∆∞ phi√™n b·∫£n 3B v√† 7B
- **C√πng t√≠nh nƒÉng m·∫°nh m·∫Ω**:
  - Kh·∫£ nƒÉng ch·ªëng nhi·ªÖu (1 positive + 4 negative)
  - Lo·∫°i b·ªè negative documents
  - T√≠ch h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn
  - X√°c ƒë·ªãnh positive/negative v·ªõi ƒë·ªô ch√≠nh x√°c ~99%
- **Ng·ªØ c·∫£nh**: L√™n ƒë·∫øn 8192 tokens
- **Phi√™n b·∫£n quantized**: [tensorblock/Vi-Qwen2-1.5B-RAG-GGUF](https://huggingface.co/tensorblock/Vi-Qwen2-1.5B-RAG-GGUF) v√† [mradermacher/Vi-Qwen2-1.5B-RAG-GGUF](https://huggingface.co/mradermacher/Vi-Qwen2-1.5B-RAG-GGUF)

### 2. **NxMobileLM-1.5B-SFT** (NTQAI) ‚≠ê‚≠ê‚≠ê‚≠ê

[**NxMobileLM-1.5B-SFT**](https://huggingface.co/NTQAI/NxMobileLM-1.5B-SFT) - T·ªëi ∆∞u cho mobile v√† edge:

**ƒê·∫∑c ƒëi·ªÉm:**
- D·ª±a tr√™n Qwen2.5-1.5B
- **T·ªëi ∆∞u ƒë·∫∑c bi·ªát cho mobile v√† edge applications**
- Fine-tuned cho hi·ªáu su·∫•t t·ªët tr√™n thi·∫øt b·ªã h·∫°n ch·∫ø
- H·ªó tr·ª£ ti·∫øng Vi·ªát t·ªët th√¥ng qua base model

### 3. **Qwen2.5-1.5B-Instruct** (Qwen) ‚≠ê‚≠ê‚≠ê

[**Qwen2.5-1.5B-Instruct**](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) - Base model m·∫°nh m·∫Ω:

**ƒê·∫∑c ƒëi·ªÉm:**
- **H·ªó tr·ª£ ƒëa ng√¥n ng·ªØ**: 29+ ng√¥n ng·ªØ bao g·ªìm ti·∫øng Vi·ªát
- **Context length**: 32K tokens  
- Hi·ªáu su·∫•t c·∫°nh tranh v·ªõi c√°c m√¥ h√¨nh l·ªõn h∆°n
- Base foundation t·ªët cho fine-tuning RAG

## So s√°nh v√† khuy·∫øn ngh·ªã

### **Khuy·∫øn ngh·ªã h√†ng ƒë·∫ßu: Vi-Qwen2-1.5B-RAG**

**T·∫°i sao ch·ªçn Vi-Qwen2-1.5B-RAG:**
- ‚úÖ **Thi·∫øt k·∫ø chuy√™n bi·ªát cho RAG**: Kh√¥ng c·∫ßn fine-tuning th√™m
- ‚úÖ **T√≠nh nƒÉng RAG ho√†n ch·ªânh**: Ch·ªëng nhi·ªÖu, t√≠ch h·ª£p th√¥ng tin, t·ª´ ch·ªëi c√¢u tr·∫£ l·ªùi kh√¥ng ch√≠nh x√°c
- ‚úÖ **T·ªëi ∆∞u cho ti·∫øng Vi·ªát**: ƒê∆∞·ª£c ƒë√†o t·∫°o v√† fine-tune ƒë·∫∑c bi·ªát cho ti·∫øng Vi·ªát
- ‚úÖ **ƒê√£ ƒë∆∞·ª£c benchmark**: C√≥ k·∫øt qu·∫£ ƒë√°nh gi√° tr√™n EvalRAGData
- ‚úÖ **Ready-to-use**: C√≥ quantized versions cho deployment d·ªÖ d√†ng

### **L·ª±a ch·ªçn thay th·∫ø:**

**NxMobileLM-1.5B-SFT** n·∫øu b·∫°n c·∫ßn:
- Tri·ªÉn khai tr√™n mobile/edge devices
- Hi·ªáu su·∫•t t·ªëi ∆∞u tr√™n ph·∫ßn c·ª©ng h·∫°n ch·∫ø

**Qwen2.5-1.5B-Instruct** n·∫øu b·∫°n:
- Mu·ªën fine-tune t·ª± custom cho RAG
- C·∫ßn base model m·∫°nh m·∫Ω ƒë·ªÉ ph√°t tri·ªÉn

## Y√™u c·∫ßu h·ªá th·ªëng

- **VRAM**: ~3.1GB cho FP16
- **Context**: 8K-32K tokens t√πy model
- **Deployment**: H·ªó tr·ª£ CPU v√† GPU, c√≥ GGUF cho efficiency cao

**K·∫øt lu·∫≠n**: **Vi-Qwen2-1.5B-RAG** l√† l·ª±a ch·ªçn t·ªët nh·∫•t cho RAG ti·∫øng Vi·ªát v·ªõi k√≠ch th∆∞·ªõc 1.5B parameters, cung c·∫•p hi·ªáu su·∫•t RAG chuy√™n nghi·ªáp trong package nh·ªè g·ªçn.


---

D·ª±a tr√™n nghi√™n c·ª©u chi ti·∫øt, ƒë√¢y l√† so s√°nh c√°c m√¥ h√¨nh RAG ti·∫øng Vi·ªát 1.5B-3B c√≥ th·ªÉ g·ªçi qua API:

## So s√°nh c√°c m√¥ h√¨nh RAG ti·∫øng Vi·ªát 1.5B-3B qua API

### **ü•á TOP CHOICE: Vi-Qwen2 Series (qua Hugging Face API)**

#### **1. Vi-Qwen2-1.5B-RAG** 
- **Platform**: Hugging Face Inference API
- **Pricing**: 
  - Free: 300 requests/hour
  - Pro ($10/month): 1000 requests/hour + premium models
- **∆Øu ƒëi·ªÉm**:
  - ‚úÖ Thi·∫øt k·∫ø chuy√™n bi·ªát cho RAG ti·∫øng Vi·ªát
  - ‚úÖ Kh·∫£ nƒÉng ch·ªëng nhi·ªÖu xu·∫•t s·∫Øc
  - ‚úÖ Context 8K tokens
  - ‚úÖ API ƒë∆°n gi·∫£n, t∆∞∆°ng th√≠ch OpenAI

#### **2. Vi-Qwen2-3B-RAG**
- **Platform**: Hugging Face Inference API  
- **Pricing**: T∆∞∆°ng t·ª± 1.5B
- **∆Øu ƒëi·ªÉm**:
  - ‚úÖ Hi·ªáu su·∫•t RAG cao h∆°n 1.5B
  - ‚úÖ T√≠nh nƒÉng RAG ho√†n ch·ªânh
  - ‚úÖ Benchmark score cao tr√™n EvalRAGData

### **ü•à ALTERNATIVE: Qwen2.5 Series (qua OpenRouter)**

#### **3. Qwen2.5-1.5B-Instruct**
- **Platform**: OpenRouter
- **Pricing**: 
  - Input: ~$0.01-0.05/M tokens
  - Output: ~$0.03-0.10/M tokens
- **∆Øu ƒëi·ªÉm**:
  - ‚úÖ H·ªó tr·ª£ 29+ ng√¥n ng·ªØ bao g·ªìm ti·∫øng Vi·ªát
  - ‚úÖ Context 32K tokens
  - ‚úÖ API t∆∞∆°ng th√≠ch OpenAI

#### **4. Qwen2.5-3B-Instruct**  
- **Platform**: OpenRouter
- **Pricing**: 
  - Input: ~$0.02-0.08/M tokens
  - Output: ~$0.05-0.15/M tokens
- **∆Øu ƒëi·ªÉm**:
  - ‚úÖ Hi·ªáu su·∫•t t·ªïng qu√°t t·ªët
  - ‚úÖ Multilingual capability m·∫°nh

### **ü•â BUDGET OPTION: Free Models**

#### **5. Llama-3.2-3B-Instruct-Frog**
- **Platform**: Hugging Face (n·∫øu available)
- **Pricing**: Free tier available
- **∆Øu ƒëi·ªÉm**:
  - ‚úÖ T·ªëi ∆∞u RAG ti·∫øng Vi·ªát
  - ‚úÖ Context 131K tokens
  - ‚úÖ Thi·∫øt k·∫ø cho edge devices

## **B·∫£ng so s√°nh chi ti·∫øt**

| Model | Size | Platform | Cost/M tokens | Context | RAG Specialized | Vietnamese Support |
|-------|------|----------|---------------|---------|-----------------|-------------------|
| **Vi-Qwen2-1.5B-RAG** | 1.5B | HF API | Free/$10/month | 8K | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Vi-Qwen2-3B-RAG** | 3B | HF API | Free/$10/month | 8K | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Qwen2.5-1.5B** | 1.5B | OpenRouter | $0.05-0.15 | 32K | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Qwen2.5-3B** | 3B | OpenRouter | $0.10-0.25 | 32K | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Llama-3.2-3B-Frog** | 3B | HF API | Free/$10/month | 131K | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

## **Khuy·∫øn ngh·ªã c·ª• th·ªÉ**

### **Cho RAG Production:**
**üéØ Vi-Qwen2-3B-RAG (Hugging Face API)**
- L√Ω do: Thi·∫øt k·∫ø chuy√™n bi·ªát cho RAG, t√≠nh nƒÉng anti-noise, ready-to-use
- Chi ph√≠: $10/month cho unlimited usage trong gi·ªõi h·∫°n h·ª£p l√Ω

### **Cho Budget Constraints:**
**üéØ Vi-Qwen2-1.5B-RAG (Hugging Face Free)**
- L√Ω do: 300 requests/hour ƒë·ªß cho development/testing
- Chi ph√≠: Free

### **Cho Context d√†i:**
**üéØ Llama-3.2-3B-Instruct-Frog**
- L√Ω do: 131K context length cho t√†i li·ªáu l·ªõn
- Chi ph√≠: Free tier + Pro n·∫øu c·∫ßn

### **Cho Multi-purpose:**
**üéØ Qwen2.5-3B-Instruct (OpenRouter)**
- L√Ω do: C√¢n b·∫±ng gi·ªØa RAG v√† general tasks
- Chi ph√≠: Pay-per-use, linh ho·∫°t

## **Code Example Setup**

```python
# Hugging Face API
from huggingface_hub import InferenceClient

client = InferenceClient(
    model="AITeamVN/Vi-Qwen2-3B-RAG",
    token="your_hf_token"
)

# OpenRouter API
import openai
openai.api_base = "https://openrouter.ai/api/v1"
openai.api_key = "your_openrouter_key"

# Usage
response = openai.ChatCompletion.create(
    model="qwen/qwen-2.5-3b-instruct",
    messages=[{"role": "user", "content": "RAG query"}]
)
```

**K·∫øt lu·∫≠n**: **Vi-Qwen2-3B-RAG qua Hugging Face API** l√† l·ª±a ch·ªçn t·ªëi ∆∞u nh·∫•t cho RAG ti·∫øng Vi·ªát v·ªõi t·ª∑ l·ªá chi ph√≠/hi·ªáu su·∫•t xu·∫•t s·∫Øc.

---

ƒê√¢y l√† code chi ti·∫øt ƒë·ªÉ s·ª≠ d·ª•ng Vi-Qwen2-3B-RAG qua Hugging Face API:

## **Setup v√† C√†i ƒë·∫∑t**

```bash
# C√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt
pip install huggingface_hub requests transformers
```

## **1. Basic API Call v·ªõi Hugging Face Client**

```python
from huggingface_hub import InferenceClient
import os

# Setup API key
HF_TOKEN = "your_huggingface_token"  # L·∫•y t·ª´ https://huggingface.co/settings/tokens
client = InferenceClient(token=HF_TOKEN)

# Model endpoint
MODEL_NAME = "AITeamVN/Vi-Qwen2-3B-RAG"

def query_vi_qwen_rag(context, question, system_prompt=None):
    """
    Query Vi-Qwen2-3B-RAG model
    """
    if system_prompt is None:
        system_prompt = "B·∫°n l√† m·ªôt tr·ª£ l√≠ Ti·∫øng Vi·ªát nhi·ªát t√¨nh v√† trung th·ª±c. H√£y lu√¥n tr·∫£ l·ªùi m·ªôt c√°ch h·ªØu √≠ch nh·∫•t c√≥ th·ªÉ."
    
    # Template theo format c·ªßa model
    template = '''Ch√∫ √Ω c√°c y√™u c·∫ßu sau:
- C√¢u tr·∫£ l·ªùi ph·∫£i ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß n·∫øu ng·ªØ c·∫£nh c√≥ c√¢u tr·∫£ l·ªùi. 
- Ch·ªâ s·ª≠ d·ª•ng c√°c th√¥ng tin c√≥ trong ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p.
- Ch·ªâ c·∫ßn t·ª´ ch·ªëi tr·∫£ l·ªùi v√† kh√¥ng suy lu·∫≠n g√¨ th√™m n·∫øu ng·ªØ c·∫£nh kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi.
H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh:
### Ng·ªØ c·∫£nh :
{context}

### C√¢u h·ªèi :
{question}

### Tr·∫£ l·ªùi :'''
    
    # T·∫°o messages
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": template.format(context=context, question=question)}
    ]
    
    try:
        # Call API
        response = client.chat_completion(
            messages=messages,
            model=MODEL_NAME,
            max_tokens=2048,
            temperature=0.1,
            stream=False
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        return f"Error: {str(e)}"

# V√≠ d·ª• s·ª≠ d·ª•ng
context = """
Vi·ªát Nam c√≥ di·ªán t√≠ch 331.212 km¬≤, d√¢n s·ªë kho·∫£ng 97 tri·ªáu ng∆∞·ªùi. 
Th·ªß ƒë√¥ l√† H√† N·ªôi, th√†nh ph·ªë l·ªõn nh·∫•t l√† Th√†nh ph·ªë H·ªì Ch√≠ Minh.
Vi·ªát Nam c√≥ 63 t·ªânh th√†nh v√† gi√°p v·ªõi Trung Qu·ªëc, L√†o, Campuchia.
"""

question = "Vi·ªát Nam c√≥ bao nhi√™u t·ªânh th√†nh?"

result = query_vi_qwen_rag(context, question)
print(result)
```

## **2. Advanced API v·ªõi Requests Library**

```python
import requests
import json

class ViQwenRAGAPI:
    def __init__(self, hf_token):
        self.hf_token = hf_token
        self.api_url = "https://api-inference.huggingface.co/models/AITeamVN/Vi-Qwen2-3B-RAG"
        self.headers = {
            "Authorization": f"Bearer {hf_token}",
            "Content-Type": "application/json"
        }
    
    def query(self, context, question, **kwargs):
        """
        Query with custom parameters
        """
        system_prompt = kwargs.get('system_prompt', 
            "B·∫°n l√† m·ªôt tr·ª£ l√≠ Ti·∫øng Vi·ªát nhi·ªát t√¨nh v√† trung th·ª±c. H√£y lu√¥n tr·∫£ l·ªùi m·ªôt c√°ch h·ªØu √≠ch nh·∫•t c√≥ th·ªÉ.")
        
        template = '''Ch√∫ √Ω c√°c y√™u c·∫ßu sau:
- C√¢u tr·∫£ l·ªùi ph·∫£i ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß n·∫øu ng·ªØ c·∫£nh c√≥ c√¢u tr·∫£ l·ªùi. 
- Ch·ªâ s·ª≠ d·ª•ng c√°c th√¥ng tin c√≥ trong ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p.
- Ch·ªâ c·∫ßn t·ª´ ch·ªëi tr·∫£ l·ªùi v√† kh√¥ng suy lu·∫≠n g√¨ th√™m n·∫øu ng·ªØ c·∫£nh kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi.
H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh:
### Ng·ªØ c·∫£nh :
{context}

### C√¢u h·ªèi :
{question}

### Tr·∫£ l·ªùi :'''
        
        payload = {
            "inputs": template.format(context=context, question=question),
            "parameters": {
                "max_new_tokens": kwargs.get('max_tokens', 2048),
                "temperature": kwargs.get('temperature', 0.1),
                "top_p": kwargs.get('top_p', 0.9),
                "do_sample": kwargs.get('do_sample', True),
                "return_full_text": False
            }
        }
        
        try:
            response = requests.post(self.api_url, headers=self.headers, json=payload)
            response.raise_for_status()
            
            result = response.json()
            if isinstance(result, list) and len(result) > 0:
                return result[0].get('generated_text', '').strip()
            else:
                return "No response generated"
                
        except requests.exceptions.RequestException as e:
            return f"API Error: {str(e)}"
        except Exception as e:
            return f"Error: {str(e)}"
    
    def batch_query(self, contexts_questions):
        """
        Batch processing for multiple queries
        """
        results = []
        for context, question in contexts_questions:
            result = self.query(context, question)
            results.append({
                'context': context[:100] + "..." if len(context) > 100 else context,
                'question': question,
                'answer': result
            })
        return results

# S·ª≠ d·ª•ng
api = ViQwenRAGAPI("your_hf_token")

# Single query
context = """
RAG (Retrieval-Augmented Generation) l√† k·ªπ thu·∫≠t k·∫øt h·ª£p gi·ªØa vi·ªác truy xu·∫•t th√¥ng tin 
v√† m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn ƒë·ªÉ t·∫°o ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c h∆°n. RAG gi√∫p m√¥ h√¨nh AI 
truy c·∫≠p th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu b√™n ngo√†i thay v√¨ ch·ªâ d·ª±a v√†o ki·∫øn th·ª©c ƒë√£ h·ªçc.
"""

question = "RAG l√† g√¨ v√† c√≥ √≠ch l·ª£i g√¨?"
answer = api.query(context, question, temperature=0.05)
print(f"C√¢u h·ªèi: {question}")
print(f"Tr·∫£ l·ªùi: {answer}")
```

## **3. Streaming Response cho Real-time**

```python
import sseclient
import requests
import json

class ViQwenRAGStreaming:
    def __init__(self, hf_token):
        self.hf_token = hf_token
        self.api_url = "https://api-inference.huggingface.co/models/AITeamVN/Vi-Qwen2-3B-RAG"
        self.headers = {
            "Authorization": f"Bearer {hf_token}",
            "Content-Type": "application/json"
        }
    
    def stream_query(self, context, question, **kwargs):
        """
        Streaming response for real-time output
        """
        template = '''Ch√∫ √Ω c√°c y√™u c·∫ßu sau:
- C√¢u tr·∫£ l·ªùi ph·∫£i ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß n·∫øu ng·ªØ c·∫£nh c√≥ c√¢u tr·∫£ l·ªùi. 
- Ch·ªâ s·ª≠ d·ª•ng c√°c th√¥ng tin c√≥ trong ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p.
- Ch·ªâ c·∫ßn t·ª´ ch·ªëi tr·∫£ l·ªùi v√† kh√¥ng suy lu·∫≠n g√¨ th√™m n·∫øu ng·ªØ c·∫£nh kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi.
H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh:
### Ng·ªØ c·∫£nh :
{context}

### C√¢u h·ªèi :
{question}

### Tr·∫£ l·ªùi :'''
        
        payload = {
            "inputs": template.format(context=context, question=question),
            "parameters": {
                "max_new_tokens": kwargs.get('max_tokens', 2048),
                "temperature": kwargs.get('temperature', 0.1),
                "stream": True
            }
        }
        
        try:
            response = requests.post(
                self.api_url, 
                headers=self.headers, 
                json=payload, 
                stream=True
            )
            response.raise_for_status()
            
            # Process streaming response
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line.decode('utf-8'))
                        if 'token' in data:
                            yield data['token']['text']
                    except:
                        continue
                        
        except Exception as e:
            yield f"Streaming error: {str(e)}"

# S·ª≠ d·ª•ng streaming
streaming_api = ViQwenRAGStreaming("your_hf_token")

context = "H√† N·ªôi l√† th·ªß ƒë√¥ c·ªßa Vi·ªát Nam, c√≥ di·ªán t√≠ch 3.359 km¬≤ v√† d√¢n s·ªë kho·∫£ng 8 tri·ªáu ng∆∞·ªùi."
question = "H√† N·ªôi c√≥ di·ªán t√≠ch bao nhi√™u?"

print("Streaming response:")
full_response = ""
for token in streaming_api.stream_query(context, question):
    print(token, end="", flush=True)
    full_response += token
print("\n\nFull response:", full_response)
```

## **4. RAG Pipeline ho√†n ch·ªânh**

```python
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss

class VietnameseRAGPipeline:
    def __init__(self, hf_token, embedding_model="dangvantuan/vietnamese-embedding"):
        self.api = ViQwenRAGAPI(hf_token)
        self.embedder = SentenceTransformer(embedding_model)
        self.documents = []
        self.document_embeddings = None
        self.index = None
    
    def add_documents(self, documents):
        """
        Add documents to the knowledge base
        """
        self.documents.extend(documents)
        
        # Create embeddings
        embeddings = self.embedder.encode(documents)
        
        if self.document_embeddings is None:
            self.document_embeddings = embeddings
        else:
            self.document_embeddings = np.vstack([self.document_embeddings, embeddings])
        
        # Build FAISS index
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
        
        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(self.document_embeddings)
        self.index.add(self.document_embeddings.astype('float32'))
    
    def retrieve_context(self, question, top_k=3):
        """
        Retrieve relevant documents for the question
        """
        if self.index is None:
            return ""
        
        # Encode question
        question_embedding = self.embedder.encode([question])
        faiss.normalize_L2(question_embedding)
        
        # Search
        scores, indices = self.index.search(question_embedding.astype('float32'), top_k)
        
        # Get relevant documents
        relevant_docs = []
        for i, idx in enumerate(indices[0]):
            if scores[0][i] > 0.3:  # Threshold for relevance
                relevant_docs.append(self.documents[idx])
        
        return "\n\n".join(relevant_docs)
    
    def answer_question(self, question, **kwargs):
        """
        Complete RAG pipeline: retrieve + generate
        """
        # Retrieve relevant context
        context = self.retrieve_context(question, top_k=kwargs.get('top_k', 3))
        
        if not context.strip():
            return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
        
        # Generate answer
        answer = self.api.query(context, question, **kwargs)
        
        return {
            'question': question,
            'context': context,
            'answer': answer,
            'context_length': len(context.split())
        }

# S·ª≠ d·ª•ng RAG Pipeline
rag = VietnameseRAGPipeline("your_hf_token")

# Add knowledge base
documents = [
    "Vi·ªát Nam c√≥ 63 t·ªânh th√†nh, bao g·ªìm 58 t·ªânh v√† 5 th√†nh ph·ªë tr·ª±c thu·ªôc trung ∆∞∆°ng.",
    "Th·ªß ƒë√¥ H√† N·ªôi c√≥ di·ªán t√≠ch 3.359 km¬≤ v√† d√¢n s·ªë kho·∫£ng 8 tri·ªáu ng∆∞·ªùi.",
    "Th√†nh ph·ªë H·ªì Ch√≠ Minh l√† th√†nh ph·ªë l·ªõn nh·∫•t Vi·ªát Nam v·ªõi d√¢n s·ªë kho·∫£ng 9 tri·ªáu ng∆∞·ªùi.",
    "Vi·ªát Nam xu·∫•t kh·∫©u ch·ªß y·∫øu g·∫°o, c√† ph√™, cao su v√† h·∫£i s·∫£n.",
    "ƒê·ªìng Vi·ªát Nam (VND) l√† ƒë∆°n v·ªã ti·ªÅn t·ªá ch√≠nh th·ª©c c·ªßa Vi·ªát Nam."
]

rag.add_documents(documents)

# Query
result = rag.answer_question("Vi·ªát Nam c√≥ bao nhi√™u t·ªânh th√†nh?")
print(f"C√¢u h·ªèi: {result['question']}")
print(f"Context: {result['context']}")
print(f"Tr·∫£ l·ªùi: {result['answer']}")
```

## **5. Error Handling v√† Rate Limiting**

```python
import time
from functools import wraps

def rate_limit(calls_per_minute=60):
    """
    Rate limiting decorator for API calls
    """
    min_interval = 60.0 / calls_per_minute
    last_called = [0.0]
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            elapsed = time.time() - last_called[0]
            left_to_wait = min_interval - elapsed
            if left_to_wait > 0:
                time.sleep(left_to_wait)
            ret = func(*args, **kwargs)
            last_called[0] = time.time()
            return ret
        return wrapper
    return decorator

class RobustViQwenRAG:
    def __init__(self, hf_token, max_retries=3):
        self.api = ViQwenRAGAPI(hf_token)
        self.max_retries = max_retries
    
    @rate_limit(calls_per_minute=50)  # Stay under rate limit
    def safe_query(self, context, question, **kwargs):
        """
        Query with retry logic and error handling
        """
        for attempt in range(self.max_retries):
            try:
                result = self.api.query(context, question, **kwargs)
                
                # Check if result indicates an error
                if result.startswith("Error:") or result.startswith("API Error:"):
                    if attempt == self.max_retries - 1:
                        return result
                    time.sleep(2 ** attempt)  # Exponential backoff
                    continue
                
                return result
                
            except Exception as e:
                if attempt == self.max_retries - 1:
                    return f"Failed after {self.max_retries} attempts: {str(e)}"
                time.sleep(2 ** attempt)
        
        return "Maximum retries exceeded"

# S·ª≠ d·ª•ng
robust_api = RobustViQwenRAG("your_hf_token")
result = robust_api.safe_query(context, question)
print(result)
```

## **Setup Token Hugging Face**

1. Truy c·∫≠p: https://huggingface.co/settings/tokens
2. T·∫°o token m·ªõi v·ªõi quy·ªÅn "Read"
3. Copy token v√† thay th·∫ø v√†o code

## **Rate Limits**

- **Free**: 300 requests/hour
- **Pro ($10/month)**: 1000 requests/hour + premium models
- **Enterprise**: Custom limits

V·ªõi code n√†y b·∫°n c√≥ th·ªÉ t√≠ch h·ª£p Vi-Qwen2-3B-RAG v√†o ·ª©ng d·ª•ng m·ªôt c√°ch hi·ªáu qu·∫£ v√† robust!


---


ƒê√¢y l√† code ƒë√£ ƒë∆∞·ª£c s·ª≠a ƒë·ªïi ƒë·ªÉ t√≠ch h·ª£p Vi-Qwen2-3B-RAG v√†o class AnswerGenerator hi·ªán t·∫°i:

```python
"""
This module helps generate answers using AI models.
It combines retrieved documents with user questions to create good answers.
"""

from typing import List, Dict, Any, Optional
from langchain_core.documents import Document
from openai import OpenAI
import httpx
from dotenv import load_dotenv
import os
import argparse
from huggingface_hub import InferenceClient
import requests

# Load environment variables
load_dotenv()

# Default system prompt
DEFAULT_SYSTEM_PROMPT = """You are a helpful assistant that answers questions based on the given context. 
If you don't know the answer, say you don't know. 
Use only the information from the context to answer. 
Keep your answers clear and simple."""

# Vietnamese system prompt for Vi-Qwen2 models
VIETNAMESE_SYSTEM_PROMPT = """B·∫°n l√† m·ªôt tr·ª£ l√≠ Ti·∫øng Vi·ªát nhi·ªát t√¨nh v√† trung th·ª±c. H√£y lu√¥n tr·∫£ l·ªùi m·ªôt c√°ch h·ªØu √≠ch nh·∫•t c√≥ th·ªÉ."""

class ViQwenRAGProvider:
    """Provider for Vi-Qwen2-RAG models via Hugging Face API"""
    
    def __init__(self, model_name: str, hf_token: str):
        self.model_name = model_name
        self.hf_token = hf_token
        self.client = InferenceClient(token=hf_token)
        self.api_url = f"https://api-inference.huggingface.co/models/{model_name}"
        self.headers = {
            "Authorization": f"Bearer {hf_token}",
            "Content-Type": "application/json"
        }
    
    def generate_completion(self, messages: List[Dict], **kwargs):
        """Generate completion using Vi-Qwen2-RAG model"""
        try:
            # Try using chat completion first
            response = self.client.chat_completion(
                messages=messages,
                model=self.model_name,
                max_tokens=kwargs.get('max_tokens', 2048),
                temperature=kwargs.get('temperature', 0.1),
                stream=False
            )
            return response.choices[0].message.content
        except:
            # Fallback to direct API call
            return self._direct_api_call(messages, **kwargs)
    
    def _direct_api_call(self, messages: List[Dict], **kwargs):
        """Direct API call as fallback"""
        # Combine messages into single prompt
        prompt = ""
        for msg in messages:
            if msg["role"] == "system":
                prompt = msg["content"] + "\n\n"
            elif msg["role"] == "user":
                prompt += msg["content"]
        
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": kwargs.get('max_tokens', 2048),
                "temperature": kwargs.get('temperature', 0.1),
                "top_p": kwargs.get('top_p', 0.9),
                "do_sample": True,
                "return_full_text": False
            }
        }
        
        try:
            response = requests.post(self.api_url, headers=self.headers, json=payload)
            response.raise_for_status()
            
            result = response.json()
            if isinstance(result, list) and len(result) > 0:
                return result[0].get('generated_text', '').strip()
            else:
                return "No response generated"
                
        except Exception as e:
            return f"API Error: {str(e)}"

class AnswerGenerator:
    """
    A class that helps generate answers using OpenAI models or Vi-Qwen2-RAG.
    
    This class can:
    - Combine documents with questions
    - Use OpenAI models or Vi-Qwen2-RAG to generate answers
    - Format answers nicely
    """
    
    def __init__(
        self,
        model_name: str = "gpt-4o-mini",
        temperature: float = 0,
        max_tokens: int = 4096,
        system_prompt: str = None
    ):
        """
        Start the AnswerGenerator with optional model settings.
        
        Args:
            model_name: Name of the model to use (OpenAI or Vi-Qwen2-RAG)
            temperature: How creative the answers should be (0.0 to 1.0)
            max_tokens: Maximum number of tokens in the response
            system_prompt: System prompt to guide the model's behavior
            
        Example:
            >>> generator = AnswerGenerator(model_name="gpt-4")
            >>> answer = generator.generate_answer("What is RAG?", documents)
            >>> 
            >>> # For Vietnamese RAG
            >>> generator = AnswerGenerator(model_name="AITeamVN/Vi-Qwen2-3B-RAG")
            >>> answer = generator.generate_answer("RAG l√† g√¨?", documents)
        """
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        # Check if it's a Vi-Qwen2 model
        self.is_vi_qwen = "Vi-Qwen2" in model_name or "vi-qwen2" in model_name.lower()
        
        # Set appropriate system prompt
        if system_prompt is None:
            self.system_prompt = VIETNAMESE_SYSTEM_PROMPT if self.is_vi_qwen else DEFAULT_SYSTEM_PROMPT
        else:
            self.system_prompt = system_prompt
            
        # Initialize appropriate client
        if self.is_vi_qwen:
            hf_token = os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACE_TOKEN")
            if not hf_token:
                raise ValueError("HF_TOKEN or HUGGINGFACE_TOKEN environment variable is required for Vi-Qwen2 models")
            self.provider = ViQwenRAGProvider(model_name, hf_token)
        else:
            # Initialize OpenAI client
            if os.getenv("OPENAI_PROXY"):
                http_client = httpx.Client(proxies=os.getenv("OPENAI_PROXY"))
                self.client = OpenAI(
                    http_client=http_client,
                    api_key=os.getenv("OPENAI_API_KEY")
                )
            else:
                self.client = OpenAI(
                    api_key=os.getenv("OPENAI_API_KEY")
                )
    
    def format_context(self, documents: List[Document]) -> str:
        """
        Combine documents into a single context string.
        
        Args:
            documents: List of documents to combine
            
        Returns:
            Combined context as a string
        """
        return "\n\n".join(doc.page_content for doc in documents)
    
    def _create_vi_qwen_prompt(self, question: str, context: str) -> str:
        """Create RAG prompt for Vi-Qwen2 models"""
        template = '''Ch√∫ √Ω c√°c y√™u c·∫ßu sau:
- C√¢u tr·∫£ l·ªùi ph·∫£i ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß n·∫øu ng·ªØ c·∫£nh c√≥ c√¢u tr·∫£ l·ªùi. 
- Ch·ªâ s·ª≠ d·ª•ng c√°c th√¥ng tin c√≥ trong ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p.
- Ch·ªâ c·∫ßn t·ª´ ch·ªëi tr·∫£ l·ªùi v√† kh√¥ng suy lu·∫≠n g√¨ th√™m n·∫øu ng·ªØ c·∫£nh kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi.
H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh:
### Ng·ªØ c·∫£nh :
{context}

### C√¢u h·ªèi :
{question}

### Tr·∫£ l·ªùi :'''
        
        return template.format(context=context, question=question)
    
    def generate_answer(
        self,
        question: str,
        documents: List[Document],
        format_context: bool = True
    ) -> str:
        """
        Generate an answer using the specified model.
        
        Args:
            question: The question to answer
            documents: List of relevant documents
            format_context: Whether to format the context (default: True)
            
        Returns:
            The generated answer
            
        Example:
            >>> answer = generator.generate_answer("What is RAG?", documents)
            >>> print(f"Generated answer: {answer}")
        """
        if format_context:
            context = self.format_context(documents)
        else:
            context = documents[0].page_content if documents else ""
            
        if self.is_vi_qwen:
            # Use Vi-Qwen2-RAG specific prompt format
            prompt = self._create_vi_qwen_prompt(question, context)
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt}
            ]
            
            return self.provider.generate_completion(
                messages,
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )
        else:
            # Use OpenAI format
            prompt = f"""Context: {context}

Question: {question}

Answer:"""
            
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            return response.choices[0].message.content
    
    def generate_answer_with_sources(
        self,
        question: str,
        documents: List[Document]
    ) -> Dict[str, Any]:
        """
        Generate an answer with source information.
        
        Args:
            question: The question to answer
            documents: List of relevant documents
            
        Returns:
            Dictionary with answer and sources
        """
        answer = self.generate_answer(question, documents)
        
        # Extract sources from document metadata
        sources = []
        for doc in documents:
            if "source" in doc.metadata:
                sources.append(doc.metadata["source"])
                
        return {
            "answer": answer,
            "sources": list(set(sources))  # Remove duplicates
        }

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="RAG Answer Generator with multiple model support")
    parser.add_argument(
        "--model", 
        type=str, 
        default="gpt-4o-mini",
        help="Model to use (e.g., gpt-4o-mini, gpt-4, AITeamVN/Vi-Qwen2-3B-RAG, AITeamVN/Vi-Qwen2-1.5B-RAG)"
    )
    parser.add_argument(
        "--temperature", 
        type=float, 
        default=0.0,
        help="Temperature for text generation (0.0 to 1.0)"
    )
    parser.add_argument(
        "--max-tokens", 
        type=int, 
        default=4096,
        help="Maximum number of tokens in response"
    )
    parser.add_argument(
        "--question", 
        type=str,
        help="Question to ask (for quick testing)"
    )
    parser.add_argument(
        "--language", 
        type=str, 
        choices=["en", "vi"], 
        default="auto",
        help="Language mode (en=English, vi=Vietnamese, auto=detect from model)"
    )
    
    return parser.parse_args()

if __name__ == "__main__":
    """
    This part runs when you run this file directly.
    It shows examples of how to use the AnswerGenerator class.
    
    Usage examples:
    python answer_generator.py --model gpt-4o-mini --question "What is RAG?"
    python answer_generator.py --model AITeamVN/Vi-Qwen2-3B-RAG --question "RAG l√† g√¨?"
    python answer_generator.py --model AITeamVN/Vi-Qwen2-1.5B-RAG --temperature 0.1
    """
    from langchain_core.documents import Document
    
    # Parse command line arguments
    args = parse_arguments()
    
    print(f"Using model: {args.model}")
    print(f"Temperature: {args.temperature}")
    print(f"Max tokens: {args.max_tokens}")
    
    # Create sample documents
    if "Vi-Qwen2" in args.model or args.language == "vi":
        # Vietnamese documents
        sample_docs = [
            Document(
                page_content="RAG l√† vi·∫øt t·∫Øt c·ªßa Retrieval-Augmented Generation. ƒê√¢y l√† k·ªπ thu·∫≠t k·∫øt h·ª£p vi·ªác truy xu·∫•t t√†i li·ªáu li√™n quan v·ªõi vi·ªác t·∫°o sinh ng√¥n ng·ªØ t·ª± nhi√™n.",
                metadata={"source": "test1_vi"}
            ),
            Document(
                page_content="RAG gi√∫p c√°c m√¥ h√¨nh ng√¥n ng·ªØ cung c·∫•p c√¢u tr·∫£ l·ªùi ch√≠nh x√°c v√† c·∫≠p nh·∫≠t h∆°n b·∫±ng c√°ch s·ª≠ d·ª•ng ki·∫øn th·ª©c t·ª´ b√™n ngo√†i.",
                metadata={"source": "test2_vi"}
            )
        ]
        default_question = "RAG l√† g√¨ v√† c√≥ l·ª£i √≠ch g√¨?"
    else:
        # English documents
        sample_docs = [
            Document(
                page_content="RAG stands for Retrieval-Augmented Generation. It combines retrieval of relevant documents with language model generation.",
                metadata={"source": "test1"}
            ),
            Document(
                page_content="RAG helps language models provide more accurate and up-to-date answers by using external knowledge.",
                metadata={"source": "test2"}
            )
        ]
        default_question = "What is RAG?"
    
    # Use provided question or default
    question = args.question if args.question else default_question
    
    # Test basic answer generation
    print(f"\nTesting basic answer generation with question: '{question}'")
    try:
        answer_gen = AnswerGenerator(
            model_name=args.model,
            temperature=args.temperature,
            max_tokens=args.max_tokens
        )
        answer = answer_gen.generate_answer(question, sample_docs)
        print(f"Generated answer: {answer}")
    except Exception as e:
        print(f"Basic generation test failed: {e}")
    
    # Test answer with sources
    print(f"\nTesting answer with sources...")
    try:
        result = answer_gen.generate_answer_with_sources(question, sample_docs)
        print(f"Answer: {result['answer']}")
        print(f"Sources: {result['sources']}")
    except Exception as e:
        print(f"Source generation test failed: {e}")
    
    # Show available models
    print(f"\nAvailable models:")
    print("OpenAI models: gpt-4o-mini, gpt-4o, gpt-4, gpt-3.5-turbo")
    print("Vietnamese RAG models: AITeamVN/Vi-Qwen2-1.5B-RAG, AITeamVN/Vi-Qwen2-3B-RAG")
    
    # Show usage examples
    print(f"\nUsage examples:")
    print("python answer_generator.py --model gpt-4o-mini --question 'What is machine learning?'")
    print("python answer_generator.py --model AITeamVN/Vi-Qwen2-3B-RAG --question 'Machine learning l√† g√¨?'")
    print("python answer_generator.py --model AITeamVN/Vi-Qwen2-1.5B-RAG --temperature 0.1 --max-tokens 2048")
```

## **Environment Variables c·∫ßn thi·∫øt**

T·∫°o file `.env`:

```bash
# For OpenAI models
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_PROXY=http://your_proxy:port  # Optional

# For Vi-Qwen2 models (required when using Vietnamese models)
HF_TOKEN=your_huggingface_token_here
# or
HUGGINGFACE_TOKEN=your_huggingface_token_here
```

## **C√°ch s·ª≠ d·ª•ng t·ª´ command line:**

```bash
# S·ª≠ d·ª•ng GPT-4o-mini (m·∫∑c ƒë·ªãnh)
python answer_generator.py

# S·ª≠ d·ª•ng GPT-4o-mini v·ªõi c√¢u h·ªèi c·ª• th·ªÉ
python answer_generator.py --model gpt-4o-mini --question "What is RAG?"

# S·ª≠ d·ª•ng Vi-Qwen2-3B-RAG cho ti·∫øng Vi·ªát
python answer_generator.py --model AITeamVN/Vi-Qwen2-3B-RAG --question "RAG l√† g√¨?"

# S·ª≠ d·ª•ng Vi-Qwen2-1.5B-RAG v·ªõi tham s·ªë t√πy ch·ªânh
python answer_generator.py --model AITeamVN/Vi-Qwen2-1.5B-RAG --temperature 0.1 --max-tokens 2048

# S·ª≠ d·ª•ng GPT-4 v·ªõi nhi·ªát ƒë·ªô cao h∆°n
python answer_generator.py --model gpt-4 --temperature 0.5 --question "Explain quantum computing"
```

## **C√°ch s·ª≠ d·ª•ng trong code kh√°c:**

```python
from answer_generator import AnswerGenerator

# S·ª≠ d·ª•ng Vi-Qwen2-3B-RAG
generator = AnswerGenerator(model_name="AITeamVN/Vi-Qwen2-3B-RAG")
answer = generator.generate_answer("RAG l√† g√¨?", documents)

# S·ª≠ d·ª•ng GPT-4o-mini
generator = AnswerGenerator(model_name="gpt-4o-mini")
answer = generator.generate_answer("What is RAG?", documents)
```

## **T√≠nh nƒÉng ch√≠nh:**

1. **T·ª± ƒë·ªông ph√°t hi·ªán model**: Code t·ª± ƒë·ªông nh·∫≠n bi·∫øt model Vi-Qwen2 v√† s·ª≠ d·ª•ng prompt format ph√π h·ª£p
2. **Command line arguments**: D·ªÖ d√†ng thay ƒë·ªïi model v√† tham s·ªë t·ª´ terminal
3. **Multilingual support**: T·ª± ƒë·ªông ch·ªçn ng√¥n ng·ªØ ph√π h·ª£p v·ªõi model
4. **Error handling**: X·ª≠ l√Ω l·ªói robust cho c·∫£ OpenAI v√† Hugging Face API
5. **Fallback mechanism**: C√≥ c∆° ch·∫ø d·ª± ph√≤ng khi API ch√≠nh g·∫∑p l·ªói

Code n√†y gi·ªØ nguy√™n structure g·ªëc nh∆∞ng th√™m kh·∫£ nƒÉng s·ª≠ d·ª•ng Vi-Qwen2-RAG models qua command line arguments!