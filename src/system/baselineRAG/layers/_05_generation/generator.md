# RAG Answer Generator Documentation

## 1. C√†i ƒë·∫∑t v√† Kh·ªüi t·∫°o

### 1.1. C√†i ƒë·∫∑t th∆∞ vi·ªán
```python
from typing import List, Dict, Any
from langchain_core.documents import Document
from openai import OpenAI
import httpx
from dotenv import load_dotenv
import os

# Load bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()
```

### 1.2. Kh·ªüi t·∫°o Generator
```python
# Kh·ªüi t·∫°o v·ªõi c·∫•u h√¨nh m·∫∑c ƒë·ªãnh
generator = AnswerGenerator()

# Ho·∫∑c kh·ªüi t·∫°o v·ªõi c·∫•u h√¨nh t√πy ch·ªânh
generator = AnswerGenerator(
    model_name="gpt-4",        # T√™n model OpenAI
    temperature=0.0,           # ƒê·ªô s√°ng t·∫°o (0.0 - 1.0)
    max_tokens=4096,          # S·ªë token t·ªëi ƒëa
    system_prompt="..."       # Prompt t√πy ch·ªânh
)
```

## 2. Chu·∫©n b·ªã D·ªØ li·ªáu

### 2.1. T·∫°o t√†i li·ªáu
```python
# T·∫°o danh s√°ch t√†i li·ªáu
tai_lieu = [
    Document(
        page_content="RAG (Retrieval-Augmented Generation) l√† m·ªôt k·ªπ thu·∫≠t k·∫øt h·ª£p vi·ªác truy xu·∫•t t√†i li·ªáu v·ªõi vi·ªác t·∫°o c√¢u tr·∫£ l·ªùi b·∫±ng m√¥ h√¨nh ng√¥n ng·ªØ.",
        metadata={"source": "tai_lieu_1"}
    ),
    Document(
        page_content="RAG gi√∫p m√¥ h√¨nh ng√¥n ng·ªØ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c v√† c·∫≠p nh·∫≠t h∆°n b·∫±ng c√°ch s·ª≠ d·ª•ng ki·∫øn th·ª©c b√™n ngo√†i.",
        metadata={"source": "tai_lieu_2"}
    )
]
```

### 2.2. Chu·∫©n b·ªã c√¢u h·ªèi
```python
cau_hoi = "RAG l√† g√¨?"
```

## 3. S·ª≠ d·ª•ng Generator

### 3.1. T·∫°o c√¢u tr·∫£ l·ªùi c∆° b·∫£n
```python
# T·∫°o c√¢u tr·∫£ l·ªùi ƒë∆°n gi·∫£n
cau_tra_loi = generator.generate_answer(
    question=cau_hoi,
    documents=tai_lieu
)
print(f"C√¢u tr·∫£ l·ªùi: {cau_tra_loi}")
```

### 3.2. T·∫°o c√¢u tr·∫£ l·ªùi v·ªõi ngu·ªìn tham kh·∫£o
```python
# T·∫°o c√¢u tr·∫£ l·ªùi k√®m ngu·ªìn
ket_qua = generator.generate_answer_with_sources(
    question=cau_hoi,
    documents=tai_lieu
)

# In k·∫øt qu·∫£
print(f"C√¢u tr·∫£ l·ªùi: {ket_qua['answer']}")
print(f"Ngu·ªìn tham kh·∫£o: {ket_qua['sources']}")
```

## 4. X·ª≠ l√Ω L·ªói

### 4.1. Try-Catch c∆° b·∫£n
```python
try:
    cau_tra_loi = generator.generate_answer(cau_hoi, tai_lieu)
except Exception as e:
    print(f"L·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi: {e}")
```

### 4.2. X·ª≠ l√Ω l·ªói API
```python
try:
    ket_qua = generator.generate_answer_with_sources(cau_hoi, tai_lieu)
except Exception as e:
    if "API" in str(e):
        print("L·ªói k·∫øt n·ªëi API. Vui l√≤ng ki·ªÉm tra l·∫°i API key v√† k·∫øt n·ªëi m·∫°ng.")
    else:
        print(f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")
```

## 5. C·∫•u h√¨nh N√¢ng cao

### 5.1. C·∫•u h√¨nh Proxy
```python
# Th√™m proxy v√†o file .env
# OPENAI_PROXY="http://your-proxy:port"

# Ho·∫∑c c·∫•u h√¨nh tr·ª±c ti·∫øp
os.environ["OPENAI_PROXY"] = "http://your-proxy:port"
```

### 5.2. T√πy ch·ªânh System Prompt
```python
custom_prompt = """B·∫°n l√† tr·ª£ l√Ω AI h·ªØu √≠ch. 
H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p.
N·∫øu kh√¥ng bi·∫øt, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt.
Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ ng·ªØ c·∫£nh ƒë·ªÉ tr·∫£ l·ªùi.
Gi·ªØ c√¢u tr·∫£ l·ªùi r√µ r√†ng v√† ƒë∆°n gi·∫£n."""

generator = AnswerGenerator(system_prompt=custom_prompt)
```

## 6. Best Practices

1. **Ch·ªçn Model ph√π h·ª£p**
   - GPT-4: Cho c√¢u tr·∫£ l·ªùi ch·∫•t l∆∞·ª£ng cao
   - GPT-3.5: Cho c√¢u tr·∫£ l·ªùi nhanh v√† ti·∫øt ki·ªám

2. **ƒêi·ªÅu ch·ªânh Temperature**
   - 0.0: C√¢u tr·∫£ l·ªùi nh·∫•t qu√°n
   - 0.7: C√¢u tr·∫£ l·ªùi s√°ng t·∫°o h∆°n

3. **Qu·∫£n l√Ω Token**
   - Gi·ªõi h·∫°n max_tokens ƒë·ªÉ ki·ªÉm so√°t chi ph√≠
   - T·ªëi ∆∞u h√≥a n·ªôi dung t√†i li·ªáu

4. **X·ª≠ l√Ω Ngu·ªìn**
   - Lu√¥n s·ª≠ d·ª•ng generate_answer_with_sources khi c·∫ßn tham kh·∫£o
   - Ki·ªÉm tra metadata c·ªßa t√†i li·ªáu

5. **X·ª≠ l√Ω L·ªói**
   - Lu√¥n s·ª≠ d·ª•ng try-catch
   - Ki·ªÉm tra k·∫øt n·ªëi API
   - X√°c th·ª±c API key 



   ---

# 26/05/2025 - Update Prompt 

## üéØ **K·∫øt lu·∫≠n v√† ƒê·ªÅ xu·∫•t**

### **‚úÖ C√ì, n√™n d√πng prompt ti·∫øng Vi·ªát cho Qwen2.5-7B:**

**L√Ω do:**
1. **Model compatibility** - Qwen2.5-7B hi·ªÉu ti·∫øng Vi·ªát r·∫•t t·ªët
2. **Instruction following** - Gi·∫£m translation errors
3. **Context processing** - X·ª≠ l√Ω ng·ªØ c·∫£nh ti·∫øng Vi·ªát ch√≠nh x√°c h∆°n
4. **Output quality** - Response t·ª± nhi√™n h∆°n

### **üöÄ Prompt c·∫£i ti·∫øn g·ªìm:**

#### **1. Adaptive System Prompts:**
- **COMPARISON_PROMPT** - cho c√¢u h·ªèi so s√°nh ("nh·ªè h∆°n", "ph·∫£i kh√¥ng")
- **FACTUAL_PROMPT** - cho c√¢u h·ªèi th√¥ng tin ("l√† g√¨", "n√†o", "khi n√†o")
- **GENERAL_PROMPT** - cho c√°c c√¢u h·ªèi kh√°c

#### **2. Enhanced Context Formatting:**
```
üìö T√ÄI LI·ªÜU THAM KH·∫¢O:
üìÑ T√†i li·ªáu 1 (Title):
Content...

‚ùì C√ÇU H·ªéI: Question

üí° TR·∫¢ L·ªúI:
```

#### **3. Vietnamese-specific Processing:**
- Clean prefixes ("Tr·∫£ l·ªùi:", "ƒê√°p √°n:")
- Normalize punctuation
- Format cho ti·∫øng Vi·ªát

### **üìä So s√°nh v·ªõi prompt hi·ªán t·∫°i:**

| Aspect | Prompt hi·ªán t·∫°i | Prompt c·∫£i ti·∫øn |
|--------|-----------------|-----------------|
| **Specificity** | ‚ùå Generic | ‚úÖ Question-type specific |
| **Instructions** | ‚ùå Vague | ‚úÖ Clear, detailed |
| **Context format** | ‚ùå Raw text | ‚úÖ Structured, visual |
| **Answer guidance** | ‚ùå "T·ª´ ch·ªëi n·∫øu kh√¥ng bi·∫øt" | ‚úÖ "T√¨m th√¥ng tin trong t√†i li·ªáu" |
| **Post-processing** | ‚ùå None | ‚úÖ Vietnamese-specific cleaning |

### **üî• C√°ch s·ª≠ d·ª•ng:**

```python
# Thay th·∫ø generator hi·ªán t·∫°i
from optimized_vietnamese_prompts import OptimizedVietnameseGenerator

generator = OptimizedVietnameseGenerator("Qwen/Qwen2.5-7B-Instruct")
answer = generator.generate_answer(question, documents)
```

**K·∫øt qu·∫£ mong ƒë·ª£i:** TƒÉng accuracy t·ª´ 40-50% l√™n 80-90% cho data ti·∫øng Vi·ªát v·ªõi Qwen2.5-7B!