"""
non_rank_metrics_evaluator.py

Module ƒë√°nh gi√° hi·ªáu qu·∫£ retrieval s·ª≠ d·ª•ng c√°c ch·ªâ s·ªë kh√¥ng d·ª±a tr√™n th·ª© h·∫°ng.
Bao g·ªìm: Accuracy, Precision, Recall@K, F1-Score, Hit Rate@K

Input: 
- C√¢u h·ªèi/truy v·∫•n
- Danh s√°ch t√†i li·ªáu ƒë∆∞·ª£c retrieve
- Danh s√°ch t√†i li·ªáu li√™n quan th·ª±c t·∫ø (ground truth)
- Gi√° tr·ªã K cho Recall@K v√† Hit Rate@K

Output:
- Accuracy: T·ª∑ l·ªá d·ª± ƒëo√°n ƒë√∫ng t·ªïng th·ªÉ
  C√¥ng th·ª©c: Accuracy = (TP + TN) / (TP + TN + FP + FN)
  Trong ƒë√≥:
  + TP (True Positive): S·ªë t√†i li·ªáu li√™n quan ƒë∆∞·ª£c retrieve ƒë√∫ng
  + TN (True Negative): S·ªë t√†i li·ªáu kh√¥ng li√™n quan kh√¥ng ƒë∆∞·ª£c retrieve  
  (T√≠nh ƒë∆∞·ª£c khi bi·∫øt t·ªïng s·ªë t√†i li·ªáu trong corpus)
  + FP (False Positive): S·ªë t√†i li·ªáu kh√¥ng li√™n quan ƒë∆∞·ª£c retrieve
  + FN (False Negative): S·ªë t√†i li·ªáu li√™n quan kh√¥ng ƒë∆∞·ª£c retrieve

- Precision: T·ª∑ l·ªá t√†i li·ªáu li√™n quan trong k·∫øt qu·∫£ retrieve
  C√¥ng th·ª©c: Precision = TP / (TP + FP)
  √ù nghƒ©a: Trong s·ªë c√°c t√†i li·ªáu ƒë∆∞·ª£c retrieve, bao nhi√™u ph·∫ßn trƒÉm l√† li√™n quan

- Recall@K: T·ª∑ l·ªá t√†i li·ªáu li√™n quan ƒë∆∞·ª£c t√¨m th·∫•y trong top-K
  C√¥ng th·ª©c: Recall@K = TP / (TP + FN) = |Relevant ‚à© Retrieved@K| / |Relevant|
  Trong ƒë√≥:
  + Retrieved@K: T·∫≠p h·ª£p K t√†i li·ªáu ƒë·∫ßu ti√™n ƒë∆∞·ª£c retrieve
  + Relevant: T·∫≠p h·ª£p t·∫•t c·∫£ t√†i li·ªáu li√™n quan th·ª±c t·∫ø

- F1-Score: ƒêi·ªÉm c√¢n b·∫±ng gi·ªØa Precision v√† Recall
  C√¥ng th·ª©c: F1 = 2 * (Precision * Recall) / (Precision + Recall)
  √ù nghƒ©a: Trung b√¨nh ƒëi·ªÅu h√≤a c·ªßa Precision v√† Recall

- Hit Rate@K: T·ª∑ l·ªá truy v·∫•n c√≥ √≠t nh·∫•t 1 t√†i li·ªáu li√™n quan trong top-K
  C√¥ng th·ª©c: Hit Rate@K = S·ªë truy v·∫•n c√≥ √≠t nh·∫•t 1 hit trong top-K / T·ªïng s·ªë truy v·∫•n
  Trong ƒë√≥:
  + Hit: Truy v·∫•n c√≥ √≠t nh·∫•t m·ªôt t√†i li·ªáu li√™n quan trong top-K k·∫øt qu·∫£
"""

from typing import List, Dict, Any, Set, Tuple
from langchain_core.documents import Document
import numpy as np
from dataclasses import dataclass

@dataclass
class RetrievalResult:
    """L∆∞u tr·ªØ k·∫øt qu·∫£ ƒë√°nh gi√° retrieval cho m·ªôt truy v·∫•n."""
    query_id: str
    retrieved_docs: List[str]  # ID c·ªßa c√°c t√†i li·ªáu ƒë∆∞·ª£c retrieve
    relevant_docs: List[str]   # ID c·ªßa c√°c t√†i li·ªáu li√™n quan (ground truth)
    k_value: int = 10

class NonRankMetricsEvaluator:
    """
    ƒê√°nh gi√° h·ªá th·ªëng retrieval s·ª≠ d·ª•ng c√°c ch·ªâ s·ªë kh√¥ng d·ª±a tr√™n th·ª© h·∫°ng.
    
    C√°c ch·ªâ s·ªë ƒë∆∞·ª£c t√≠nh to√°n:
    1. Accuracy: (TP + TN) / (TP + TN + FP + FN)
    2. Precision: TP / (TP + FP) = s·ªë ƒë√∫ng / t·ªïng d·ª± ƒëo√°n
    3. Recall@K: TP/ (TP + FN) = |Relevant ‚à© Retrieved@K| / |Relevant| = s·ªë ƒë√∫ng / t·ªïng ƒë√∫ng
    4. F1-Score: 2 * (Precision * Recall) / (Precision + Recall)
    5. Hit Rate@K: S·ªë truy v·∫•n c√≥ √≠t nh·∫•t 1 hit trong top-K / T·ªïng s·ªë truy v·∫•n
    
    Trong ƒë√≥:
    - TP: True Positive (t√†i li·ªáu li√™n quan ƒë∆∞·ª£c retrieve)
    - TN: True Negative (t√†i li·ªáu kh√¥ng li√™n quan kh√¥ng ƒë∆∞·ª£c retrieve)
    - FP: False Positive (t√†i li·ªáu kh√¥ng li√™n quan ƒë∆∞·ª£c retrieve)
    - FN: False Negative (t√†i li·ªáu li√™n quan kh√¥ng ƒë∆∞·ª£c retrieve)
    - Hit: Truy v·∫•n c√≥ √≠t nh·∫•t m·ªôt t√†i li·ªáu li√™n quan trong top-K
    """
    
    def __init__(self, total_docs_in_corpus: int = None):
        """
        Kh·ªüi t·∫°o evaluator.
        
        Args:
            total_docs_in_corpus: T·ªïng s·ªë t√†i li·ªáu trong corpus (c·∫ßn ƒë·ªÉ t√≠nh Accuracy)
        """
        self.total_docs = total_docs_in_corpus
    
    def evaluate_single_query(
        self, 
        retrieved_docs: List[str], 
        relevant_docs: List[str],
        k: int = 10,
        query_id: str = "query_1"
    ) -> Dict[str, float]:
        """
        ƒê√°nh gi√° retrieval cho m·ªôt truy v·∫•n ƒë∆°n l·∫ª.
        
        Args:
            retrieved_docs: Danh s√°ch ID t√†i li·ªáu ƒë∆∞·ª£c retrieve (theo th·ª© t·ª± relevance)
            relevant_docs: Danh s√°ch ID t√†i li·ªáu li√™n quan th·ª±c t·∫ø
            k: S·ªë l∆∞·ª£ng t√†i li·ªáu top-K ƒë·ªÉ t√≠nh Recall@K v√† Hit Rate@K
            query_id: ID c·ªßa truy v·∫•n
            
        Returns:
            Dict ch·ª©a c√°c ch·ªâ s·ªë ƒë√°nh gi√°
        """
        # Chuy·ªÉn v·ªÅ set ƒë·ªÉ t√≠nh to√°n d·ªÖ d√†ng
        retrieved_set = set(retrieved_docs[:k])  # Ch·ªâ l·∫•y top-K
        relevant_set = set(relevant_docs)
        
        # T√≠nh c√°c th√†nh ph·∫ßn c∆° b·∫£n
        tp = len(retrieved_set & relevant_set)  # True Positive
        fp = len(retrieved_set - relevant_set)  # False Positive  
        fn = len(relevant_set - retrieved_set)  # False Negative
        
        # T√≠nh Precision
        precision = tp / len(retrieved_set) if len(retrieved_set) > 0 else 0.0
        
        # T√≠nh Recall@K
        recall_at_k = tp / len(relevant_set) if len(relevant_set) > 0 else 0.0
        
        # T√≠nh F1-Score
        f1_score = (2 * precision * recall_at_k) / (precision + recall_at_k) if (precision + recall_at_k) > 0 else 0.0
        
        # T√≠nh Hit Rate@K (cho truy v·∫•n ƒë∆°n l·∫ª: 1 n·∫øu c√≥ √≠t nh·∫•t 1 hit, 0 n·∫øu kh√¥ng)
        hit_rate_single = 1.0 if tp > 0 else 0.0
        
        # T√≠nh Accuracy (n·∫øu bi·∫øt t·ªïng s·ªë t√†i li·ªáu trong corpus)
        accuracy = None
        if self.total_docs:
            tn = self.total_docs - len(relevant_set) - fp  # True Negative
            accuracy = (tp + tn) / self.total_docs if self.total_docs > 0 else 0.0
        
        return {
            "query_id": query_id,
            "precision": precision,
            "recall_at_k": recall_at_k,
            "f1_score": f1_score,
            "hit_rate_single": hit_rate_single,  # 1 ho·∫∑c 0 cho truy v·∫•n n√†y
            "accuracy": accuracy,
            "true_positive": tp,
            "false_positive": fp,
            "false_negative": fn,
            "retrieved_count": len(retrieved_set),
            "relevant_count": len(relevant_set),
            "k_value": k,
            "has_hit": tp > 0  # Boolean: c√≥ hit hay kh√¥ng
        }
    
    def evaluate_multiple_queries(
        self, 
        retrieval_results: List[RetrievalResult]
    ) -> Dict[str, Any]:
        """
        ƒê√°nh gi√° retrieval cho nhi·ªÅu truy v·∫•n v√† t√≠nh trung b√¨nh.
        
        Args:
            retrieval_results: Danh s√°ch k·∫øt qu·∫£ retrieval cho c√°c truy v·∫•n
            
        Returns:
            Dict ch·ª©a c√°c ch·ªâ s·ªë trung b√¨nh v√† chi ti·∫øt cho t·ª´ng truy v·∫•n
        """
        individual_results = []
        
        # ƒê√°nh gi√° t·ª´ng truy v·∫•n
        for result in retrieval_results:
            eval_result = self.evaluate_single_query(
                retrieved_docs=result.retrieved_docs,
                relevant_docs=result.relevant_docs,
                k=result.k_value,
                query_id=result.query_id
            )
            individual_results.append(eval_result)
        
        # T√≠nh c√°c ch·ªâ s·ªë trung b√¨nh
        avg_metrics = self._calculate_average_metrics(individual_results)
        
        # T√≠nh Hit Rate@K cho t·∫≠p truy v·∫•n
        hit_rate_at_k = self._calculate_hit_rate(individual_results)
        avg_metrics["hit_rate_at_k"] = hit_rate_at_k
        
        return {
            "average_metrics": avg_metrics,
            "individual_results": individual_results,
            "total_queries": len(retrieval_results),
            "queries_with_hits": sum(1 for r in individual_results if r["has_hit"]),
            "evaluation_summary": self._generate_summary(avg_metrics)
        }
    
    def _calculate_hit_rate(self, results: List[Dict]) -> float:
        """
        T√≠nh Hit Rate@K cho t·∫≠p truy v·∫•n.
        
        Hit Rate@K = S·ªë truy v·∫•n c√≥ √≠t nh·∫•t 1 t√†i li·ªáu li√™n quan trong top-K / T·ªïng s·ªë truy v·∫•n
        
        Args:
            results: Danh s√°ch k·∫øt qu·∫£ ƒë√°nh gi√° cho t·ª´ng truy v·∫•n
            
        Returns:
            Hit Rate@K (0.0 - 1.0)
        """
        if not results:
            return 0.0
        
        queries_with_hits = sum(1 for result in results if result["has_hit"])
        total_queries = len(results)
        
        return queries_with_hits / total_queries
    
    def _calculate_average_metrics(self, results: List[Dict]) -> Dict[str, float]:
        """T√≠nh to√°n c√°c ch·ªâ s·ªë trung b√¨nh t·ª´ k·∫øt qu·∫£ c·ªßa nhi·ªÅu truy v·∫•n."""
        if not results:
            return {}
        
        # T√≠nh trung b√¨nh cho c√°c ch·ªâ s·ªë ch√≠nh
        avg_precision = np.mean([r["precision"] for r in results])
        avg_recall = np.mean([r["recall_at_k"] for r in results])
        avg_f1 = np.mean([r["f1_score"] for r in results])
        
        # T√≠nh trung b√¨nh accuracy (n·∫øu c√≥)
        accuracies = [r["accuracy"] for r in results if r["accuracy"] is not None]
        avg_accuracy = np.mean(accuracies) if accuracies else None
        
        # T√≠nh c√°c th·ªëng k√™ b·ªï sung
        total_tp = sum(r["true_positive"] for r in results)
        total_fp = sum(r["false_positive"] for r in results)
        total_fn = sum(r["false_negative"] for r in results)
        
        # Macro-averaged metrics (trung b√¨nh ƒë∆°n gi·∫£n)
        macro_metrics = {
            "macro_precision": avg_precision,
            "macro_recall": avg_recall,
            "macro_f1": avg_f1,
            "macro_accuracy": avg_accuracy
        }
        
        # Micro-averaged metrics (t√≠nh tr√™n t·ªïng TP, FP, FN)
        micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
        micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
        micro_f1 = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0.0
        
        micro_metrics = {
            "micro_precision": micro_precision,
            "micro_recall": micro_recall,
            "micro_f1": micro_f1
        }
        
        return {**macro_metrics, **micro_metrics}
    
    def _generate_summary(self, metrics: Dict[str, float]) -> str:
        """T·∫°o b√°o c√°o t√≥m t·∫Øt k·∫øt qu·∫£ ƒë√°nh gi√°."""
        summary = "=== B√ÅO C√ÅO ƒê√ÅNH GI√Å NON-RANK METRICS ===\n\n"
        
        # Add corpus information
        if self.total_docs:
            summary += "üìö TH√îNG TIN CORPUS:\n"
            summary += f"  ‚Ä¢ T·ªïng s·ªë t√†i li·ªáu unique: {self.total_docs}\n"
            summary += "  ‚Ä¢ C√°ch t√≠nh: L·∫•y union c·ªßa t·∫•t c·∫£ document IDs t·ª´ retrieved_docs v√† supporting_facts\n\n"
        
        # Add explanation of averaging methods
        summary += "üìä PH∆Ø∆†NG PH√ÅP T√çNH TRUNG B√åNH:\n"
        summary += "  ‚Ä¢ MACRO-AVERAGED: T√≠nh trung b√¨nh ƒë∆°n gi·∫£n c·ªßa c√°c metrics t·ª´ t·ª´ng truy v·∫•n.\n"
        summary += "    - ∆Øu ƒëi·ªÉm: M·ªói truy v·∫•n c√≥ tr·ªçng s·ªë b·∫±ng nhau\n"
        summary += "    - Ph√π h·ª£p khi: C√°c truy v·∫•n c√≥ t·∫ßm quan tr·ªçng nh∆∞ nhau\n"
        summary += "    - V√≠ d·ª•: C√≥ 3 truy v·∫•n v·ªõi Precision l·∫ßn l∆∞·ª£t l√† 0.8, 0.6, 0.4\n"
        summary += "      MACRO Precision = (0.8 + 0.6 + 0.4) / 3 = 0.6\n\n"
        
        summary += "  ‚Ä¢ MICRO-AVERAGED: T√≠nh metrics tr√™n t·ªïng s·ªë TP, FP, FN c·ªßa t·∫•t c·∫£ truy v·∫•n.\n"
        summary += "    - ∆Øu ƒëi·ªÉm: Ph·∫£n √°nh hi·ªáu su·∫•t t·ªïng th·ªÉ tr√™n to√†n b·ªô d·ªØ li·ªáu\n"
        summary += "    - Ph√π h·ª£p khi: C·∫ßn ƒë√°nh gi√° hi·ªáu su·∫•t tr√™n t·∫≠p d·ªØ li·ªáu l·ªõn\n"
        summary += "    - V√≠ d·ª•:\n"
        summary += "      + Truy v·∫•n 1: TP=2, FP=1, FN=1\n"
        summary += "      + Truy v·∫•n 2: TP=3, FP=2, FN=0\n"
        summary += "      + Truy v·∫•n 3: TP=1, FP=1, FN=2\n"
        summary += "      T·ªïng: TP=6, FP=4, FN=3\n"
        summary += "      MICRO Precision = 6/(6+4) = 0.6\n\n"
        
        summary += "  S·ª± kh√°c bi·ªát ch√≠nh:\n"
        summary += "  - MACRO: M·ªói truy v·∫•n c√≥ tr·ªçng s·ªë b·∫±ng nhau, kh√¥ng quan t√¢m s·ªë l∆∞·ª£ng t√†i li·ªáu\n"
        summary += "  - MICRO: C√°c truy v·∫•n c√≥ nhi·ªÅu t√†i li·ªáu s·∫Ω c√≥ ·∫£nh h∆∞·ªüng l·ªõn h∆°n ƒë·∫øn k·∫øt qu·∫£ cu·ªëi c√πng\n\n"
        
        summary += "üìä MACRO-AVERAGED METRICS:\n"
        summary += f"  ‚Ä¢ Precision: {metrics.get('macro_precision', 0):.3f}\n"
        summary += f"  ‚Ä¢ Recall@K: {metrics.get('macro_recall', 0):.3f}\n"
        summary += f"  ‚Ä¢ F1-Score: {metrics.get('macro_f1', 0):.3f}\n"
        summary += f"  ‚Ä¢ Hit Rate@K: {metrics.get('hit_rate_at_k', 0):.3f}\n"
        
        if metrics.get('macro_accuracy') is not None:
            summary += f"  ‚Ä¢ Accuracy: {metrics['macro_accuracy']:.3f}\n"
        
        summary += "\nüìà MICRO-AVERAGED METRICS:\n"
        summary += f"  ‚Ä¢ Precision: {metrics.get('micro_precision', 0):.3f}\n"
        summary += f"  ‚Ä¢ Recall@K: {metrics.get('micro_recall', 0):.3f}\n"
        summary += f"  ‚Ä¢ F1-Score: {metrics.get('micro_f1', 0):.3f}\n"
        
        # ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng
        hit_rate = metrics.get('hit_rate_at_k', 0)
        precision = metrics.get('macro_precision', 0)
        recall = metrics.get('macro_recall', 0)
        
        summary += "\nüéØ ƒê√ÅNH GI√Å CH·∫§T L∆Ø·ª¢NG:\n"
        
        if hit_rate >= 0.8:
            summary += "  ‚Ä¢ Hit Rate: XU·∫§T S·∫ÆC (‚â•80% truy v·∫•n c√≥ k·∫øt qu·∫£ li√™n quan)\n"
        elif hit_rate >= 0.6:
            summary += "  ‚Ä¢ Hit Rate: T·ªêT (60-80% truy v·∫•n c√≥ k·∫øt qu·∫£ li√™n quan)\n"
        elif hit_rate >= 0.4:
            summary += "  ‚Ä¢ Hit Rate: TRUNG B√åNH (40-60% truy v·∫•n c√≥ k·∫øt qu·∫£ li√™n quan)\n"
        else:
            summary += "  ‚Ä¢ Hit Rate: C·∫¶N C·∫¢I THI·ªÜN (<40% truy v·∫•n c√≥ k·∫øt qu·∫£ li√™n quan)\n"
        
        if precision >= 0.7:
            summary += "  ‚Ä¢ Precision: CAO (√≠t nhi·ªÖu trong k·∫øt qu·∫£)\n"
        elif precision >= 0.5:
            summary += "  ‚Ä¢ Precision: TRUNG B√åNH\n"
        else:
            summary += "  ‚Ä¢ Precision: TH·∫§P (nhi·ªÅu k·∫øt qu·∫£ kh√¥ng li√™n quan)\n"
        
        if recall >= 0.7:
            summary += "  ‚Ä¢ Recall: CAO (t√¨m ƒë∆∞·ª£c nhi·ªÅu t√†i li·ªáu li√™n quan)\n"
        elif recall >= 0.5:
            summary += "  ‚Ä¢ Recall: TRUNG B√åNH\n"
        else:
            summary += "  ‚Ä¢ Recall: TH·∫§P (b·ªè s√≥t nhi·ªÅu t√†i li·ªáu li√™n quan)\n"
        
        return summary

if __name__ == "__main__":
    """Test NonRankMetricsEvaluator v·ªõi Hit Rate@K"""
    
    # D·ªØ li·ªáu test m·∫´u
    test_results = [
        RetrievalResult(
            query_id="Q1",
            retrieved_docs=["doc1", "doc2", "doc3", "doc4", "doc5"],
            relevant_docs=["doc1", "doc3", "doc6"],  # doc6 kh√¥ng ƒë∆∞·ª£c retrieve
            k_value=5
        ),
        RetrievalResult(
            query_id="Q2", 
            retrieved_docs=["doc7", "doc8", "doc9", "doc10"],
            relevant_docs=["doc11", "doc12"],  # Kh√¥ng c√≥ t√†i li·ªáu n√†o li√™n quan ƒë∆∞·ª£c retrieve
            k_value=5
        ),
        RetrievalResult(
            query_id="Q3",
            retrieved_docs=["doc13", "doc14", "doc15"],
            relevant_docs=["doc13", "doc14"],  # T·∫•t c·∫£ ƒë·ªÅu ƒë∆∞·ª£c retrieve
            k_value=5
        )
    ]
    
    print("Testing NonRankMetricsEvaluator v·ªõi Hit Rate@K...")
    
    try:
        # Kh·ªüi t·∫°o evaluator v·ªõi 100 t√†i li·ªáu trong corpus
        evaluator = NonRankMetricsEvaluator(total_docs_in_corpus=100)
        
        # ƒê√°nh gi√° nhi·ªÅu truy v·∫•n
        results = evaluator.evaluate_multiple_queries(test_results)
        
        print("üìä K·∫æT QU·∫¢ ƒê√ÅNH GI√Å:")
        print(f"T·ªïng s·ªë truy v·∫•n: {results['total_queries']}")
        print(f"S·ªë truy v·∫•n c√≥ hit: {results['queries_with_hits']}")
        print(f"Hit Rate@K: {results['average_metrics']['hit_rate_at_k']:.3f}")
        
        print("\n" + results['evaluation_summary'])
        
        print("\nüìã CHI TI·∫æT T·ª™NG TRUY V·∫§N:")
        for result in results['individual_results']:
            print(f"Query {result['query_id']}:")
            print(f"  - Precision: {result['precision']:.3f}")
            print(f"  - Recall@K: {result['recall_at_k']:.3f}")
            print(f"  - F1-Score: {result['f1_score']:.3f}")
            print(f"  - Has Hit: {'‚úÖ' if result['has_hit'] else '‚ùå'}")
            print(f"  - TP/FP/FN: {result['true_positive']}/{result['false_positive']}/{result['false_negative']}")
            print()
            
    except Exception as e:
        print(f"Test failed: {e}")
        import traceback
        traceback.print_exc()
