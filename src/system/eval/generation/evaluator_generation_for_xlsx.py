"""
evaluator_generation_for_xlsx.py

Script Ä‘Ã¡nh giÃ¡ generation tá»« file Excel vá»›i BATCH PROCESSING:

  â€¢ Tham sá»‘ --sheet Ä‘á»ƒ chá»n sheet input (máº·c Ä‘á»‹nh baseline_result_QA_raw)
  â€¢ Tham sá»‘ --start_row vÃ  --end_row Ä‘á»ƒ giá»›i háº¡n dÃ²ng Ä‘Ã¡nh giÃ¡
  â€¢ Tham sá»‘ --batch_size Ä‘á»ƒ thiáº¿t láº­p kÃ­ch thÆ°á»›c batch (máº·c Ä‘á»‹nh 10)
  â€¢ Tham sá»‘ --max_workers Ä‘á»ƒ thiáº¿t láº­p sá»‘ worker song song (máº·c Ä‘á»‹nh 3)

Input sheet chá»©a cÃ¡c cá»™t:
  - question_id
  - question
  - answer           (sáº½ rename thÃ nh reference_answer)
  - ai_answer        (sáº½ rename thÃ nh generated_answer)
  - supporting_facts (VD: "[['doc1',0],['doc2',1]]")

Output sheet "generation_evaluated" vá»›i cá»™t:
  â€¢ Input nguyÃªn gá»‘c
  â€¢ gen_bleu_1..gen_f1
  â€¢ gen_overall_score, gen_relevance_score, gen_completeness_score
  â€¢ gen_missing_info, gen_hallucinations
  â€¢ gen_answer_length, gen_quality_assessment
"""

import pandas as pd
import ast
import argparse
import os
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dotenv import load_dotenv, find_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import threading

from langchain_core.documents import Document
from generation_evaluator import GenerationEvaluator

# Tá»± Ä‘á»™ng tÃ¬m file .env gáº§n nháº¥t trong cÃ¢y thÆ° má»¥c
env_file = find_dotenv()
if env_file:
    load_dotenv(env_file)
    print(f"âœ… ÄÃ£ tÃ¬m tháº¥y vÃ  load file .env: {env_file}")
else:
    print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file .env trong cÃ¢y thÆ° má»¥c")

def check_openai_api_key():
    """Kiá»ƒm tra vÃ  láº¥y OpenAI API key tá»« biáº¿n mÃ´i trÆ°á»ng."""
    # Debug: in táº¥t cáº£ biáº¿n mÃ´i trÆ°á»ng chá»©a "OPENAI"
    print("ğŸ” Debug: Táº¥t cáº£ biáº¿n mÃ´i trÆ°á»ng chá»©a 'OPENAI':")
    for key, value in os.environ.items():
        if "OPENAI" in key:
            print(f"  {key} = {value[:10]}{'...' if len(value) > 10 else ''}")
    
    api_key = os.getenv("OPENAI_API_KEY")
    print(f"ğŸ” Debug: os.getenv('OPENAI_API_KEY') = {repr(api_key)}")
    
    if not api_key:
        print("âŒ API key is None or empty")
        raise ValueError(
            "âŒ KhÃ´ng tÃ¬m tháº¥y OpenAI API key!\n"
            "Vui lÃ²ng lÃ m theo cÃ¡c bÆ°á»›c sau:\n"
            "1. Táº¡o API key táº¡i: https://platform.openai.com/account/api-keys\n"
            "2. Táº¡o file .env trong thÆ° má»¥c gá»‘c dá»± Ã¡n vá»›i ná»™i dung:\n"
            "   OPENAI_API_KEY=sk-your-actual-api-key-here\n"
            "3. Hoáº·c thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng:\n"
            "   - Windows: set OPENAI_API_KEY=sk-your-actual-api-key-here\n"
            "   - Linux/Mac: export OPENAI_API_KEY=sk-your-actual-api-key-here\n"
            "4. Khá»Ÿi Ä‘á»™ng láº¡i terminal vÃ  cháº¡y láº¡i script"
        )
    
    if api_key == "your_api_key" or api_key == "sk-your-actual-api-key-here":
        print("âŒ API key is placeholder value")
        raise ValueError(
            "âŒ API key lÃ  giÃ¡ trá»‹ máº«u!\n"
            "Vui lÃ²ng thay tháº¿ báº±ng API key tháº­t cá»§a báº¡n trong file .env hoáº·c biáº¿n mÃ´i trÆ°á»ng."
        )
    
    if not api_key.startswith("sk-"):
        print(f"âŒ API key format invalid: {api_key[:20]}...")
        raise ValueError(
            "âŒ Äá»‹nh dáº¡ng API key khÃ´ng há»£p lá»‡!\n"
            "API key pháº£i báº¯t Ä‘áº§u báº±ng 'sk-'"
        )
    
    print(f"âœ… API key há»£p lá»‡: {api_key[:10]}...{api_key[-4:]}")
    return api_key

class BatchProgressTracker:
    """Class theo dÃµi tiáº¿n Ä‘á»™ xá»­ lÃ½ batch."""
    
    def __init__(self, total_batches: int, total_rows: int):
        self.total_batches = total_batches
        self.total_rows = total_rows
        self.completed_batches = 0
        self.completed_rows = 0
        self.lock = Lock()
        self.start_time = time.time()
    
    def update_progress(self, batch_size: int):
        """Cáº­p nháº­t tiáº¿n Ä‘á»™ khi hoÃ n thÃ nh má»™t batch."""
        with self.lock:
            self.completed_batches += 1
            self.completed_rows += batch_size
            
            elapsed_time = time.time() - self.start_time
            progress_pct = (self.completed_rows / self.total_rows) * 100
            
            if self.completed_rows > 0:
                avg_time_per_row = elapsed_time / self.completed_rows
                remaining_rows = self.total_rows - self.completed_rows
                eta_seconds = avg_time_per_row * remaining_rows
                eta_minutes = eta_seconds / 60
                
                print(f"ğŸ“Š [{self.completed_batches}/{self.total_batches}] "
                      f"{self.completed_rows}/{self.total_rows} dÃ²ng "
                      f"({progress_pct:.1f}%) - "
                      f"ETA: {eta_minutes:.1f} phÃºt")

class ExcelGenerationEvaluator:
    """
    Class Ä‘Ã¡nh giÃ¡ generation tá»« file Excel vá»›i batch processing.
    Äá»c dá»¯ liá»‡u, cháº¡y GenerationEvaluator theo batch song song vÃ  ghi káº¿t quáº£.
    """
    
    def __init__(self, model_name: str, temperature: float, batch_size: int = 10, max_workers: int = 3):
        # Kiá»ƒm tra API key trÆ°á»›c khi khá»Ÿi táº¡o
        check_openai_api_key()
        
        self.model_name = model_name
        self.temperature = temperature
        self.batch_size = batch_size
        self.max_workers = max_workers
        
        print(f"âš™ï¸ Cáº¥u hÃ¬nh batch processing:")
        print(f"  â€¢ Batch size: {self.batch_size}")
        print(f"  â€¢ Max workers: {self.max_workers}")
        print(f"  â€¢ Model: {self.model_name}")

    def _create_evaluator(self) -> GenerationEvaluator:
        """Táº¡o má»™t instance má»›i cá»§a GenerationEvaluator cho má»—i worker."""
        return GenerationEvaluator(
            model_name=self.model_name,
            temperature=self.temperature
        )

    def _process_single_row(
        self, 
        evaluator: GenerationEvaluator,
        row_data: Dict[str, Any], 
        doc_map: Dict[str, Document],
        row_index: int,
        global_index: int
    ) -> Dict[str, Any]:
        """
        Xá»­ lÃ½ má»™t dÃ²ng dá»¯ liá»‡u duy nháº¥t.
        
        Args:
            evaluator: Instance cá»§a GenerationEvaluator
            row_data: Dá»¯ liá»‡u cá»§a dÃ²ng
            doc_map: Mapping tá»« doc_id Ä‘áº¿n Document
            row_index: Index trong batch (0-based)
            global_index: Index toÃ n cá»¥c trong dataset
        
        Returns:
            Dict chá»©a káº¿t quáº£ Ä‘Ã¡nh giÃ¡
        """
        try:
            # Láº¥y documents cho cÃ¢u há»i nÃ y
            docs = [doc_map[doc_id] for doc_id in row_data["relevant_docs"] if doc_id in doc_map]
            
            # Ã‰p kiá»ƒu cÃ¡c giÃ¡ trá»‹ text thÃ nh string
            current_question = str(row_data["question"])
            current_generated_answer = str(row_data["generated_answer"])
            current_reference_answer = str(row_data["reference_answer"])
            
            # Gá»i evaluator
            res = evaluator.evaluate_answer(
                question=current_question,
                answer=current_generated_answer,
                documents=docs,
                reference_answer=current_reference_answer
            )
            
            # Thu tháº­p káº¿t quáº£
            result_entry = {
                # Input columns (giá»¯ nguyÃªn)
                "question_id": row_data["question_id"],
                "question": row_data["question"],
                "reference_answer": row_data["reference_answer"],
                "generated_answer": row_data["generated_answer"],
                "supporting_facts": row_data["supporting_facts"],
                
                # Reference-based metrics
                "gen_bleu_1": res.bleu_1,
                "gen_bleu_2": res.bleu_2,
                "gen_bleu_3": res.bleu_3,
                "gen_bleu_4": res.bleu_4,
                "gen_rouge_l": res.rouge_l,
                "gen_f1": res.f1,
                
                # LLM-based primary scores
                "gen_overall_score": res.llm_score,
                "gen_relevance_score": res.detailed_scores.get("relevance", 0),
                "gen_completeness_score": res.detailed_scores.get("completeness", 0),
                
                # Error analysis
                "gen_missing_info": "; ".join(res.missing_information),
                "gen_hallucinations": "; ".join(res.hallucinations),
                
                # Language quality
                "gen_answer_length": res.answer_quality.get("length", 0),
                "gen_quality_assessment": res.answer_quality.get("assessment", ""),
                
                # Metadata for tracking
                "_batch_index": row_index,
                "_global_index": global_index
            }
            
            return result_entry
            
        except Exception as e:
            print(f"    âŒ Lá»—i Ä‘Ã¡nh giÃ¡ dÃ²ng {global_index}: {e}")
            # Tráº£ vá» entry vá»›i giÃ¡ trá»‹ máº·c Ä‘á»‹nh
            return {
                "question_id": row_data["question_id"],
                "question": row_data["question"],
                "reference_answer": row_data["reference_answer"],
                "generated_answer": row_data["generated_answer"],
                "supporting_facts": row_data["supporting_facts"],
                "gen_bleu_1": 0, "gen_bleu_2": 0, "gen_bleu_3": 0, "gen_bleu_4": 0,
                "gen_rouge_l": 0, "gen_f1": 0,
                "gen_overall_score": 0, "gen_relevance_score": 0, "gen_completeness_score": 0,
                "gen_missing_info": f"Error: {str(e)}", "gen_hallucinations": "",
                "gen_answer_length": 0, "gen_quality_assessment": f"Error: {str(e)}",
                "_batch_index": row_index,
                "_global_index": global_index
            }

    def _process_batch(
        self, 
        batch_data: List[Tuple[int, Dict[str, Any]]], 
        doc_map: Dict[str, Document],
        batch_id: int,
        progress_tracker: BatchProgressTracker
    ) -> List[Dict[str, Any]]:
        """
        Xá»­ lÃ½ má»™t batch dá»¯ liá»‡u.
        
        Args:
            batch_data: List cÃ¡c tuple (global_index, row_data)
            doc_map: Mapping tá»« doc_id Ä‘áº¿n Document
            batch_id: ID cá»§a batch
            progress_tracker: Tracker theo dÃµi tiáº¿n Ä‘á»™
        
        Returns:
            List káº¿t quáº£ Ä‘Ã¡nh giÃ¡
        """
        thread_id = threading.get_ident()
        print(f"ğŸ”„ Batch {batch_id+1} báº¯t Ä‘áº§u (Thread {thread_id}) - {len(batch_data)} dÃ²ng")
        
        # Táº¡o evaluator riÃªng cho batch nÃ y
        evaluator = self._create_evaluator()
        
        batch_results = []
        batch_start_time = time.time()
        
        for row_index, (global_index, row_data) in enumerate(batch_data):
            result = self._process_single_row(
                evaluator=evaluator,
                row_data=row_data,
                doc_map=doc_map,
                row_index=row_index,
                global_index=global_index
            )
            batch_results.append(result)
        
        batch_time = time.time() - batch_start_time
        avg_time_per_row = batch_time / len(batch_data)
        
        print(f"âœ… Batch {batch_id+1} hoÃ n thÃ nh (Thread {thread_id}) - "
              f"{batch_time:.1f}s ({avg_time_per_row:.2f}s/dÃ²ng)")
        
        # Cáº­p nháº­t tiáº¿n Ä‘á»™
        progress_tracker.update_progress(len(batch_data))
        
        return batch_results

    def evaluate_excel(
        self,
        input_path: Path,
        output_path: Path,
        sheet_name: str,
        start_row: Optional[int] = None,
        end_row: Optional[int] = None
    ) -> None:
        """
        ÄÃ¡nh giÃ¡ generation tá»« file Excel vá»›i batch processing.
        
        Args:
            input_path: ÄÆ°á»ng dáº«n file Excel Ä‘áº§u vÃ o
            output_path: ÄÆ°á»ng dáº«n file Excel Ä‘áº§u ra
            sheet_name: TÃªn sheet chá»©a dá»¯ liá»‡u
            start_row: DÃ²ng báº¯t Ä‘áº§u (0-based, inclusive)
            end_row: DÃ²ng káº¿t thÃºc (0-based, exclusive)
        """
        print(f"ğŸ“– Äá»c file: {input_path}")
        print(f"ğŸ“„ Sheet: {sheet_name}")
        
        # Kiá»ƒm tra file tá»“n táº¡i
        if not input_path.exists():
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file: {input_path}")
        
        # Äá»c sheet generation
        try:
            df = pd.read_excel(input_path, sheet_name=sheet_name)
        except ValueError as e:
            if "Worksheet" in str(e):
                # Liá»‡t kÃª cÃ¡c sheet cÃ³ sáºµn
                xl_file = pd.ExcelFile(input_path)
                available_sheets = xl_file.sheet_names
                raise ValueError(
                    f"KhÃ´ng tÃ¬m tháº¥y sheet '{sheet_name}' trong file.\n"
                    f"CÃ¡c sheet cÃ³ sáºµn: {available_sheets}"
                )
            raise
        
        print(f"ğŸ“Š Tá»•ng sá»‘ dÃ²ng trong sheet: {len(df)}")
        
        # Giá»›i háº¡n dÃ²ng theo tham sá»‘
        original_len = len(df)
        if start_row is not None:
            df = df.iloc[start_row:]
        if end_row is not None:
            if start_row is not None:
                df = df.iloc[:end_row-start_row]
            else:
                df = df.iloc[:end_row]
        
        print(f"â–¶ï¸ ÄÃ¡nh giÃ¡ {len(df)} dÃ²ng tá»« sheet '{sheet_name}' (tá»« dÃ²ng {start_row or 0} Ä‘áº¿n {end_row or original_len})")

        # Kiá»ƒm tra cá»™t báº¯t buá»™c
        required_cols = {"question_id", "question", "answer", "ai_answer", "supporting_facts"}
        missing_cols = required_cols - set(df.columns)
        if missing_cols:
            available_cols = list(df.columns)
            raise ValueError(
                f"âŒ Thiáº¿u cá»™t báº¯t buá»™c trong sheet '{sheet_name}': {missing_cols}\n"
                f"CÃ¡c cá»™t cÃ³ sáºµn: {available_cols}\n"
                f"Cáº§n cÃ³ Ä‘á»§ cÃ¡c cá»™t: {required_cols}"
            )

        # Rename cá»™t Ä‘á»ƒ phÃ¹ há»£p vá»›i GenerationEvaluator
        df = df.rename(columns={
            "answer": "reference_answer",
            "ai_answer": "generated_answer"
        })
        
        print("âœ… ÄÃ£ mapping cá»™t: answer -> reference_answer, ai_answer -> generated_answer")

        # Táº¡o map doc_id -> Document tá»« supporting_facts
        print("ğŸ”„ XÃ¢y dá»±ng document mapping tá»« supporting_facts...")
        doc_map: Dict[str, Document] = {}
        
        for idx, row in df.iterrows():
            try:
                # Parse supporting_facts: "[['doc1', 'content1'], ['doc2', 'content2']]"
                facts = ast.literal_eval(row["supporting_facts"])
                for item in facts:
                    if len(item) >= 2:
                        doc_id, content = item[0], item[1]
                        if doc_id not in doc_map:
                            doc_map[doc_id] = Document(
                                page_content=str(content),
                                metadata={"source": f"supporting_facts_row_{idx}"}
                            )
            except Exception as e:
                print(f"âš ï¸ Lá»—i parse supporting_facts táº¡i dÃ²ng {idx}: {e}")
                continue
        
        print(f"ğŸ“š ÄÃ£ táº¡o {len(doc_map)} documents tá»« supporting_facts")

        # Parse supporting_facts thÃ nh danh sÃ¡ch doc_id
        def parse_supporting_facts(sf_str: str) -> List[str]:
            try:
                facts = ast.literal_eval(sf_str)
                return [item[0] for item in facts if len(item) >= 1]
            except:
                return []
        
        df["relevant_docs"] = df["supporting_facts"].apply(parse_supporting_facts)

        # Chuáº©n bá»‹ dá»¯ liá»‡u cho batch processing
        print("ğŸš€ Chuáº©n bá»‹ batch processing...")
        
        # Chuyá»ƒn Ä‘á»•i DataFrame thÃ nh list cÃ¡c dict
        all_data = []
        for global_idx, (_, row) in enumerate(df.iterrows()):
            row_data = {
                "question_id": row["question_id"],
                "question": row["question"],
                "reference_answer": row["reference_answer"],
                "generated_answer": row["generated_answer"],
                "supporting_facts": row["supporting_facts"],
                "relevant_docs": row["relevant_docs"]
            }
            all_data.append((global_idx, row_data))
        
        # Chia thÃ nh cÃ¡c batch
        batches = []
        for i in range(0, len(all_data), self.batch_size):
            batch = all_data[i:i + self.batch_size]
            batches.append(batch)
        
        total_batches = len(batches)
        print(f"ğŸ“¦ Chia thÃ nh {total_batches} batch (má»—i batch {self.batch_size} dÃ²ng)")
        print(f"âš¡ Sá»­ dá»¥ng {self.max_workers} worker song song")
        
        # Khá»Ÿi táº¡o progress tracker
        progress_tracker = BatchProgressTracker(total_batches, len(all_data))
        
        # Xá»­ lÃ½ batch song song
        all_results = []
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit táº¥t cáº£ batch
            future_to_batch = {
                executor.submit(
                    self._process_batch, 
                    batch_data, 
                    doc_map, 
                    batch_id,
                    progress_tracker
                ): batch_id 
                for batch_id, batch_data in enumerate(batches)
            }
            
            # Thu tháº­p káº¿t quáº£ theo thá»© tá»± hoÃ n thÃ nh
            batch_results = {}
            for future in as_completed(future_to_batch):
                batch_id = future_to_batch[future]
                try:
                    results = future.result()
                    batch_results[batch_id] = results
                except Exception as e:
                    print(f"âŒ Lá»—i xá»­ lÃ½ batch {batch_id + 1}: {e}")
                    batch_results[batch_id] = []
        
        # Sáº¯p xáº¿p káº¿t quáº£ theo thá»© tá»± batch gá»‘c
        for batch_id in sorted(batch_results.keys()):
            all_results.extend(batch_results[batch_id])
        
        # Sáº¯p xáº¿p láº¡i theo global_index Ä‘á»ƒ Ä‘áº£m báº£o thá»© tá»±
        all_results.sort(key=lambda x: x.get("_global_index", 0))
        
        # XÃ³a metadata columns
        for result in all_results:
            result.pop("_batch_index", None)
            result.pop("_global_index", None)
        
        total_time = time.time() - start_time
        avg_time_per_row = total_time / len(all_data) if all_data else 0
        
        print(f"\nâ±ï¸ HoÃ n thÃ nh trong {total_time:.1f}s ({avg_time_per_row:.2f}s/dÃ²ng)")

        # Táº¡o DataFrame káº¿t quáº£ vÃ  xuáº¥t Excel
        print("ğŸ’¾ LÆ°u káº¿t quáº£...")
        df_output = pd.DataFrame(all_results)
        
        # Táº¡o thÆ° má»¥c output náº¿u chÆ°a tá»“n táº¡i
        output_path.parent.mkdir(exist_ok=True, parents=True)
        
        # Ghi file Excel
        with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
            df_output.to_excel(writer, sheet_name="generation_evaluated", index=False)
        
        print(f"âœ… Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o: {output_path}")
        print(f"ğŸ“Š Tá»•ng sá»‘ dÃ²ng Ä‘Ã£ Ä‘Ã¡nh giÃ¡: {len(all_results)}")
        
        # In thá»‘ng kÃª tÃ³m táº¯t
        if all_results:
            avg_llm_score = sum(r.get("gen_overall_score", 0) for r in all_results) / len(all_results)
            avg_bleu1 = sum(r.get("gen_bleu_1", 0) for r in all_results) / len(all_results)
            avg_rouge_l = sum(r.get("gen_rouge_l", 0) for r in all_results) / len(all_results)
            
            print("\nğŸ“ˆ THá»NG KÃŠ Tá»”NG Káº¾T:")
            print(f"  â€¢ LLM Score trung bÃ¬nh: {avg_llm_score:.2f}")
            print(f"  â€¢ BLEU-1 trung bÃ¬nh: {avg_bleu1:.3f}")
            print(f"  â€¢ Rouge-L trung bÃ¬nh: {avg_rouge_l:.3f}")
            print(f"  â€¢ Thá»i gian xá»­ lÃ½: {total_time:.1f}s")
            print(f"  â€¢ Tá»‘c Ä‘á»™: {avg_time_per_row:.2f}s/dÃ²ng")

def main():
    """HÃ m main vá»›i argument parsing."""
    # XÃ¡c Ä‘á»‹nh Ä‘Æ°á»ng dáº«n máº·c Ä‘á»‹nh
    script_dir = Path(__file__).parent
    default_input = script_dir.parent / "input" / "main_vimqa_dev_300lines.xlsx"
    default_output = script_dir / "outputs" / "main_vimqa_dev_300lines_generation_evaluated.xlsx"
    
    parser = argparse.ArgumentParser(
        description="ÄÃ¡nh giÃ¡ generation tá»« Excel vá»›i LLM vÃ  cÃ¡c metrics tá»± Ä‘á»™ng (cÃ³ batch processing)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
VÃ­ dá»¥ sá»­ dá»¥ng:
  python evaluator_generation_for_xlsx.py --start_row 0 --end_row 5
  python evaluator_generation_for_xlsx.py -i data.xlsx -o result.xlsx -s my_sheet
  python evaluator_generation_for_xlsx.py --model gpt-4 --temperature 0.1
  python evaluator_generation_for_xlsx.py --batch_size 20 --max_workers 5
        """
    )
    
    parser.add_argument(
        "--input", "-i", 
        default=str(default_input),
        help=f"ÄÆ°á»ng dáº«n file Excel Ä‘áº§u vÃ o (máº·c Ä‘á»‹nh: {default_input})"
    )
    parser.add_argument(
        "--output", "-o", 
        default=str(default_output),
        help=f"ÄÆ°á»ng dáº«n file Excel Ä‘áº§u ra (máº·c Ä‘á»‹nh: {default_output})"
    )
    parser.add_argument(
        "--sheet", "-s", 
        default="baseline_result_QA_raw",
        help="TÃªn sheet chá»©a dá»¯ liá»‡u generation (máº·c Ä‘á»‹nh: baseline_result_QA_raw)"
    )
    parser.add_argument(
        "--start_row", 
        type=int,
        help="DÃ²ng báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡ (tÃ­nh tá»« 0, máº·c Ä‘á»‹nh: Ä‘Ã¡nh giÃ¡ tá»« Ä‘áº§u)"
    )
    parser.add_argument(
        "--end_row", 
        type=int,
        help="DÃ²ng káº¿t thÃºc Ä‘Ã¡nh giÃ¡ (khÃ´ng bao gá»“m, máº·c Ä‘á»‹nh: Ä‘Ã¡nh giÃ¡ Ä‘áº¿n cuá»‘i)"
    )
    parser.add_argument(
        "--model", "-m", 
        default="gpt-4o-mini",
        help="TÃªn model LLM Ä‘Ã¡nh giÃ¡ (máº·c Ä‘á»‹nh: gpt-4o-mini)"
    )
    parser.add_argument(
        "--temperature", "-t", 
        type=float, 
        default=0.0,
        help="Temperature cho LLM (máº·c Ä‘á»‹nh: 0.0)"
    )
    parser.add_argument(
        "--batch_size", "-b",
        type=int,
        default=10,
        help="KÃ­ch thÆ°á»›c má»—i batch (máº·c Ä‘á»‹nh: 10)"
    )
    parser.add_argument(
        "--max_workers", "-w",
        type=int,
        default=3,
        help="Sá»‘ worker song song tá»‘i Ä‘a (máº·c Ä‘á»‹nh: 3)"
    )
    
    args = parser.parse_args()
    
    print("ğŸš€ GENERATION EVALUATOR WITH BATCH PROCESSING")
    print("=" * 60)
    print(f"ğŸ“ Input: {args.input}")
    print(f"ğŸ“ Output: {args.output}")
    print(f"ğŸ“„ Sheet: {args.sheet}")
    print(f"ğŸ¤– Model: {args.model}")
    print(f"ğŸŒ¡ï¸ Temperature: {args.temperature}")
    print(f"ğŸ“¦ Batch size: {args.batch_size}")
    print(f"âš¡ Max workers: {args.max_workers}")
    if args.start_row is not None or args.end_row is not None:
        print(f"ğŸ“ Pháº¡m vi: dÃ²ng {args.start_row or 0} Ä‘áº¿n {args.end_row or 'cuá»‘i'}")
    print("=" * 60)

    try:
        evaluator = ExcelGenerationEvaluator(
            model_name=args.model,
            temperature=args.temperature,
            batch_size=args.batch_size,
            max_workers=args.max_workers
        )
        
        evaluator.evaluate_excel(
            input_path=Path(args.input),
            output_path=Path(args.output),
            sheet_name=args.sheet,
            start_row=args.start_row,
            end_row=args.end_row
        )
        
        print("\nğŸ‰ ÄÃ¡nh giÃ¡ hoÃ n thÃ nh thÃ nh cÃ´ng!")
        
    except Exception as e:
        print(f"\nâŒ Lá»—i trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
