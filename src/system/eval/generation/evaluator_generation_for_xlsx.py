"""
evaluator_generation_for_xlsx.py

Script ƒë√°nh gi√° generation t·ª´ file Excel v·ªõi BATCH PROCESSING:

  ‚Ä¢ Tham s·ªë --sheet ƒë·ªÉ ch·ªçn sheet input (m·∫∑c ƒë·ªãnh baseline_result_QA_raw)
  ‚Ä¢ Tham s·ªë --start_row v√† --end_row ƒë·ªÉ gi·ªõi h·∫°n d√≤ng ƒë√°nh gi√°
  ‚Ä¢ Tham s·ªë --batch_size ƒë·ªÉ thi·∫øt l·∫≠p k√≠ch th∆∞·ªõc batch (m·∫∑c ƒë·ªãnh 10)
  ‚Ä¢ Tham s·ªë --max_workers ƒë·ªÉ thi·∫øt l·∫≠p s·ªë worker song song (m·∫∑c ƒë·ªãnh 3)

Input sheet ch·ª©a c√°c c·ªôt:
  - question_id
  - question
  - answer           (s·∫Ω rename th√†nh reference_answer)
  - ai_answer        (s·∫Ω rename th√†nh generated_answer)
  - supporting_facts (VD: "[['doc1',0],['doc2',1]]")

Output sheet "generation_evaluated" v·ªõi c·ªôt:
  ‚Ä¢ Input nguy√™n g·ªëc
  ‚Ä¢ gen_bleu_1..gen_f1
  ‚Ä¢ gen_overall_score, gen_relevance_score, gen_completeness_score
  ‚Ä¢ gen_missing_info, gen_hallucinations
  ‚Ä¢ gen_answer_length, gen_quality_assessment
"""

import pandas as pd
import ast
import argparse
import os
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dotenv import load_dotenv, find_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import threading

from langchain_core.documents import Document
from generation_evaluator import GenerationEvaluator

# T·ª± ƒë·ªông t√¨m file .env g·∫ßn nh·∫•t trong c√¢y th∆∞ m·ª•c
env_file = find_dotenv()
if env_file:
    load_dotenv(env_file)
    print(f"‚úÖ ƒê√£ t√¨m th·∫•y v√† load file .env: {env_file}")
else:
    print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file .env trong c√¢y th∆∞ m·ª•c")

def check_openai_api_key():
    """Ki·ªÉm tra v√† l·∫•y OpenAI API key t·ª´ bi·∫øn m√¥i tr∆∞·ªùng."""
    # Debug: in t·∫•t c·∫£ bi·∫øn m√¥i tr∆∞·ªùng ch·ª©a "OPENAI"
    print("üîç Debug: T·∫•t c·∫£ bi·∫øn m√¥i tr∆∞·ªùng ch·ª©a 'OPENAI':")
    for key, value in os.environ.items():
        if "OPENAI" in key:
            print(f"  {key} = {value[:10]}{'...' if len(value) > 10 else ''}")
    
    api_key = os.getenv("OPENAI_API_KEY")
    print(f"üîç Debug: os.getenv('OPENAI_API_KEY') = {repr(api_key)}")
    
    if not api_key:
        print("‚ùå API key is None or empty")
        raise ValueError(
            "‚ùå Kh√¥ng t√¨m th·∫•y OpenAI API key!\n"
            "Vui l√≤ng l√†m theo c√°c b∆∞·ªõc sau:\n"
            "1. T·∫°o API key t·∫°i: https://platform.openai.com/account/api-keys\n"
            "2. T·∫°o file .env trong th∆∞ m·ª•c g·ªëc d·ª± √°n v·ªõi n·ªôi dung:\n"
            "   OPENAI_API_KEY=sk-your-actual-api-key-here\n"
            "3. Ho·∫∑c thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng:\n"
            "   - Windows: set OPENAI_API_KEY=sk-your-actual-api-key-here\n"
            "   - Linux/Mac: export OPENAI_API_KEY=sk-your-actual-api-key-here\n"
            "4. Kh·ªüi ƒë·ªông l·∫°i terminal v√† ch·∫°y l·∫°i script"
        )
    
    if api_key == "your_api_key" or api_key == "sk-your-actual-api-key-here":
        print("‚ùå API key is placeholder value")
        raise ValueError(
            "‚ùå API key l√† gi√° tr·ªã m·∫´u!\n"
            "Vui l√≤ng thay th·∫ø b·∫±ng API key th·∫≠t c·ªßa b·∫°n trong file .env ho·∫∑c bi·∫øn m√¥i tr∆∞·ªùng."
        )
    
    if not api_key.startswith("sk-"):
        print(f"‚ùå API key format invalid: {api_key[:20]}...")
        raise ValueError(
            "‚ùå ƒê·ªãnh d·∫°ng API key kh√¥ng h·ª£p l·ªá!\n"
            "API key ph·∫£i b·∫Øt ƒë·∫ßu b·∫±ng 'sk-'"
        )
    
    print(f"‚úÖ API key h·ª£p l·ªá: {api_key[:10]}...{api_key[-4:]}")
    return api_key

class BatchProgressTracker:
    """Class theo d√µi ti·∫øn ƒë·ªô x·ª≠ l√Ω batch."""
    
    def __init__(self, total_batches: int, total_rows: int):
        self.total_batches = total_batches
        self.total_rows = total_rows
        self.completed_batches = 0
        self.completed_rows = 0
        self.lock = Lock()
        self.start_time = time.time()
    
    def update_progress(self, batch_size: int):
        """C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô khi ho√†n th√†nh m·ªôt batch."""
        with self.lock:
            self.completed_batches += 1
            self.completed_rows += batch_size
            
            elapsed_time = time.time() - self.start_time
            progress_pct = (self.completed_rows / self.total_rows) * 100
            
            if self.completed_rows > 0:
                avg_time_per_row = elapsed_time / self.completed_rows
                remaining_rows = self.total_rows - self.completed_rows
                eta_seconds = avg_time_per_row * remaining_rows
                eta_minutes = eta_seconds / 60
                
                print(f"üìä [{self.completed_batches}/{self.total_batches}] "
                      f"{self.completed_rows}/{self.total_rows} d√≤ng "
                      f"({progress_pct:.1f}%) - "
                      f"ETA: {eta_minutes:.1f} ph√∫t")

class ExcelGenerationEvaluator:
    """
    Class ƒë√°nh gi√° generation t·ª´ file Excel v·ªõi batch processing.
    ƒê·ªçc d·ªØ li·ªáu, ch·∫°y GenerationEvaluator theo batch song song v√† ghi k·∫øt qu·∫£.
    """
    
    def __init__(self, model_name: str, temperature: float, batch_size: int = 10, max_workers: int = 3):
        # Ki·ªÉm tra API key tr∆∞·ªõc khi kh·ªüi t·∫°o
        check_openai_api_key()
        
        self.model_name = model_name
        self.temperature = temperature
        self.batch_size = batch_size
        self.max_workers = max_workers
        
        print(f"‚öôÔ∏è C·∫•u h√¨nh batch processing:")
        print(f"  ‚Ä¢ Batch size: {self.batch_size}")
        print(f"  ‚Ä¢ Max workers: {self.max_workers}")
        print(f"  ‚Ä¢ Model: {self.model_name}")

    def _create_evaluator(self) -> GenerationEvaluator:
        """T·∫°o m·ªôt instance m·ªõi c·ªßa GenerationEvaluator cho m·ªói worker."""
        return GenerationEvaluator(
            model_name=self.model_name,
            temperature=self.temperature
        )

    def _process_single_row(
        self, 
        evaluator: GenerationEvaluator,
        row_data: Dict[str, Any], 
        doc_map: Dict[str, Document],
        row_index: int,
        global_index: int
    ) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω m·ªôt d√≤ng d·ªØ li·ªáu duy nh·∫•t.
        
        Args:
            evaluator: Instance c·ªßa GenerationEvaluator
            row_data: D·ªØ li·ªáu c·ªßa d√≤ng
            doc_map: Mapping t·ª´ doc_id ƒë·∫øn Document
            row_index: Index trong batch (0-based)
            global_index: Index to√†n c·ª•c trong dataset
        
        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√°
        """
        try:
            # L·∫•y documents cho c√¢u h·ªèi n√†y
            docs = [doc_map[doc_id] for doc_id in row_data["relevant_docs"] if doc_id in doc_map]
            
            # √âp ki·ªÉu c√°c gi√° tr·ªã text th√†nh string
            current_question = str(row_data["question"])
            current_generated_answer = str(row_data["generated_answer"])
            current_reference_answer = str(row_data["reference_answer"])
            
            # G·ªçi evaluator
            res = evaluator.evaluate_answer(
                question=current_question,
                answer=current_generated_answer,
                documents=docs,
                reference_answer=current_reference_answer
            )
            
            # Thu th·∫≠p k·∫øt qu·∫£
            result_entry = {
                # Input columns (gi·ªØ nguy√™n)
                "question_id": row_data["question_id"],
                "question": row_data["question"],
                "reference_answer": row_data["reference_answer"],
                "generated_answer": row_data["generated_answer"],
                "supporting_facts": row_data["supporting_facts"],
                
                # Reference-based metrics
                "gen_bleu_1": res.bleu_1,
                "gen_bleu_2": res.bleu_2,
                "gen_bleu_3": res.bleu_3,
                "gen_bleu_4": res.bleu_4,
                "gen_rouge_l": res.rouge_l,
                "gen_f1": res.f1,
                
                # LLM-based primary scores
                "gen_overall_score": res.llm_score,
                "gen_relevance_score": res.detailed_scores.get("relevance", 0),
                "gen_completeness_score": res.detailed_scores.get("completeness", 0),
                
                # Error analysis
                "gen_missing_info": "; ".join(res.missing_information),
                "gen_hallucinations": "; ".join(res.hallucinations),
                
                # Language quality
                "gen_answer_length": res.answer_quality.get("length", 0),
                "gen_quality_assessment": res.answer_quality.get("assessment", ""),
                
                # Metadata for tracking
                "_batch_index": row_index,
                "_global_index": global_index
            }
            
            return result_entry
            
        except Exception as e:
            print(f"    ‚ùå L·ªói ƒë√°nh gi√° d√≤ng {global_index}: {e}")
            # Tr·∫£ v·ªÅ entry v·ªõi gi√° tr·ªã m·∫∑c ƒë·ªãnh
            return {
                "question_id": row_data["question_id"],
                "question": row_data["question"],
                "reference_answer": row_data["reference_answer"],
                "generated_answer": row_data["generated_answer"],
                "supporting_facts": row_data["supporting_facts"],
                "gen_bleu_1": 0, "gen_bleu_2": 0, "gen_bleu_3": 0, "gen_bleu_4": 0,
                "gen_rouge_l": 0, "gen_f1": 0,
                "gen_overall_score": 0, "gen_relevance_score": 0, "gen_completeness_score": 0,
                "gen_missing_info": f"Error: {str(e)}", "gen_hallucinations": "",
                "gen_answer_length": 0, "gen_quality_assessment": f"Error: {str(e)}",
                "_batch_index": row_index,
                "_global_index": global_index
            }

    def _process_batch(
        self, 
        batch_data: List[Tuple[int, Dict[str, Any]]], 
        doc_map: Dict[str, Document],
        batch_id: int,
        progress_tracker: BatchProgressTracker
    ) -> List[Dict[str, Any]]:
        """
        X·ª≠ l√Ω m·ªôt batch d·ªØ li·ªáu.
        
        Args:
            batch_data: List c√°c tuple (global_index, row_data)
            doc_map: Mapping t·ª´ doc_id ƒë·∫øn Document
            batch_id: ID c·ªßa batch
            progress_tracker: Tracker theo d√µi ti·∫øn ƒë·ªô
        
        Returns:
            List k·∫øt qu·∫£ ƒë√°nh gi√°
        """
        thread_id = threading.get_ident()
        print(f"üîÑ Batch {batch_id+1} b·∫Øt ƒë·∫ßu (Thread {thread_id}) - {len(batch_data)} d√≤ng")
        
        # T·∫°o evaluator ri√™ng cho batch n√†y
        evaluator = self._create_evaluator()
        
        batch_results = []
        batch_start_time = time.time()
        
        for row_index, (global_index, row_data) in enumerate(batch_data):
            result = self._process_single_row(
                evaluator=evaluator,
                row_data=row_data,
                doc_map=doc_map,
                row_index=row_index,
                global_index=global_index
            )
            batch_results.append(result)
        
        batch_time = time.time() - batch_start_time
        avg_time_per_row = batch_time / len(batch_data)
        
        print(f"‚úÖ Batch {batch_id+1} ho√†n th√†nh (Thread {thread_id}) - "
              f"{batch_time:.1f}s ({avg_time_per_row:.2f}s/d√≤ng)")
        
        # C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô
        progress_tracker.update_progress(len(batch_data))
        
        return batch_results

    def evaluate_excel(
        self,
        input_path: Path,
        output_path: Path,
        sheet_name: str,
        start_row: Optional[int] = None,
        end_row: Optional[int] = None
    ) -> None:
        """
        ƒê√°nh gi√° generation t·ª´ file Excel v·ªõi batch processing.
        
        Args:
            input_path: ƒê∆∞·ªùng d·∫´n file Excel ƒë·∫ßu v√†o
            output_path: ƒê∆∞·ªùng d·∫´n file Excel ƒë·∫ßu ra
            sheet_name: T√™n sheet ch·ª©a d·ªØ li·ªáu
            start_row: D√≤ng b·∫Øt ƒë·∫ßu (0-based, inclusive)
            end_row: D√≤ng k·∫øt th√∫c (0-based, exclusive)
        """
        print(f"üìñ ƒê·ªçc file: {input_path}")
        print(f"üìÑ Sheet: {sheet_name}")
        
        # Ki·ªÉm tra file t·ªìn t·∫°i
        if not input_path.exists():
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file: {input_path}")
        
        # ƒê·ªçc sheet generation
        try:
            df = pd.read_excel(input_path, sheet_name=sheet_name)
        except ValueError as e:
            if "Worksheet" in str(e):
                # Li·ªát k√™ c√°c sheet c√≥ s·∫µn
                xl_file = pd.ExcelFile(input_path)
                available_sheets = xl_file.sheet_names
                raise ValueError(
                    f"Kh√¥ng t√¨m th·∫•y sheet '{sheet_name}' trong file.\n"
                    f"C√°c sheet c√≥ s·∫µn: {available_sheets}"
                )
            raise
        
        print(f"üìä T·ªïng s·ªë d√≤ng trong sheet: {len(df)}")
        
        # Gi·ªõi h·∫°n d√≤ng theo tham s·ªë
        original_len = len(df)
        if start_row is not None:
            df = df.iloc[start_row:]
        if end_row is not None:
            if start_row is not None:
                df = df.iloc[:end_row-start_row]
            else:
                df = df.iloc[:end_row]
        
        print(f"‚ñ∂Ô∏è ƒê√°nh gi√° {len(df)} d√≤ng t·ª´ sheet '{sheet_name}' (t·ª´ d√≤ng {start_row or 0} ƒë·∫øn {end_row or original_len})")

        # Ki·ªÉm tra c·ªôt b·∫Øt bu·ªôc
        required_cols = {"question_id", "question", "answer", "ai_answer", "supporting_facts"}
        missing_cols = required_cols - set(df.columns)
        if missing_cols:
            available_cols = list(df.columns)
            raise ValueError(
                f"‚ùå Thi·∫øu c·ªôt b·∫Øt bu·ªôc trong sheet '{sheet_name}': {missing_cols}\n"
                f"C√°c c·ªôt c√≥ s·∫µn: {available_cols}\n"
                f"C·∫ßn c√≥ ƒë·ªß c√°c c·ªôt: {required_cols}"
            )

        # Rename c·ªôt ƒë·ªÉ ph√π h·ª£p v·ªõi GenerationEvaluator
        df = df.rename(columns={
            "answer": "reference_answer",
            "ai_answer": "generated_answer"
        })
        
        print("‚úÖ ƒê√£ mapping c·ªôt: answer -> reference_answer, ai_answer -> generated_answer")

        # T·∫°o map doc_id -> Document t·ª´ supporting_facts
        print("üîÑ X√¢y d·ª±ng document mapping t·ª´ supporting_facts...")
        doc_map: Dict[str, Document] = {}
        
        for idx, row in df.iterrows():
            try:
                # Parse supporting_facts: "[['doc1', 'content1'], ['doc2', 'content2']]"
                facts = ast.literal_eval(row["supporting_facts"])
                for item in facts:
                    if len(item) >= 2:
                        doc_id, content = item[0], item[1]
                        if doc_id not in doc_map:
                            doc_map[doc_id] = Document(
                                page_content=str(content),
                                metadata={"source": f"supporting_facts_row_{idx}"}
                            )
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói parse supporting_facts t·∫°i d√≤ng {idx}: {e}")
                continue
        
        print(f"üìö ƒê√£ t·∫°o {len(doc_map)} documents t·ª´ supporting_facts")

        # Parse supporting_facts th√†nh danh s√°ch doc_id
        def parse_supporting_facts(sf_str: str) -> List[str]:
            try:
                facts = ast.literal_eval(sf_str)
                return [item[0] for item in facts if len(item) >= 1]
            except:
                return []
        
        df["relevant_docs"] = df["supporting_facts"].apply(parse_supporting_facts)

        # Chu·∫©n b·ªã d·ªØ li·ªáu cho batch processing
        print("üöÄ Chu·∫©n b·ªã batch processing...")
        
        # Chuy·ªÉn ƒë·ªïi DataFrame th√†nh list c√°c dict
        all_data = []
        for global_idx, (_, row) in enumerate(df.iterrows()):
            row_data = {
                "question_id": row["question_id"],
                "question": row["question"],
                "reference_answer": row["reference_answer"],
                "generated_answer": row["generated_answer"],
                "supporting_facts": row["supporting_facts"],
                "relevant_docs": row["relevant_docs"]
            }
            all_data.append((global_idx, row_data))
        
        # Chia th√†nh c√°c batch
        batches = []
        for i in range(0, len(all_data), self.batch_size):
            batch = all_data[i:i + self.batch_size]
            batches.append(batch)
        
        total_batches = len(batches)
        print(f"üì¶ Chia th√†nh {total_batches} batch (m·ªói batch {self.batch_size} d√≤ng)")
        print(f"‚ö° S·ª≠ d·ª•ng {self.max_workers} worker song song")
        
        # Kh·ªüi t·∫°o progress tracker
        progress_tracker = BatchProgressTracker(total_batches, len(all_data))
        
        # X·ª≠ l√Ω batch song song
        all_results = []
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit t·∫•t c·∫£ batch
            future_to_batch = {
                executor.submit(
                    self._process_batch, 
                    batch_data, 
                    doc_map, 
                    batch_id,
                    progress_tracker
                ): batch_id 
                for batch_id, batch_data in enumerate(batches)
            }
            
            # Thu th·∫≠p k·∫øt qu·∫£ theo th·ª© t·ª± ho√†n th√†nh
            batch_results = {}
            for future in as_completed(future_to_batch):
                batch_id = future_to_batch[future]
                try:
                    results = future.result()
                    batch_results[batch_id] = results
                except Exception as e:
                    print(f"‚ùå L·ªói x·ª≠ l√Ω batch {batch_id + 1}: {e}")
                    batch_results[batch_id] = []
        
        # S·∫Øp x·∫øp k·∫øt qu·∫£ theo th·ª© t·ª± batch g·ªëc
        for batch_id in sorted(batch_results.keys()):
            all_results.extend(batch_results[batch_id])
        
        # S·∫Øp x·∫øp l·∫°i theo global_index ƒë·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª±
        all_results.sort(key=lambda x: x.get("_global_index", 0))
        
        # X√≥a metadata columns
        for result in all_results:
            result.pop("_batch_index", None)
            result.pop("_global_index", None)
        
        total_time = time.time() - start_time
        avg_time_per_row = total_time / len(all_data) if all_data else 0
        
        print(f"\n‚è±Ô∏è Ho√†n th√†nh trong {total_time:.1f}s ({avg_time_per_row:.2f}s/d√≤ng)")

        # T·∫°o DataFrame k·∫øt qu·∫£ v√† xu·∫•t Excel
        print("üíæ L∆∞u k·∫øt qu·∫£...")
        df_output = pd.DataFrame(all_results)
        
        # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a t·ªìn t·∫°i
        output_path.parent.mkdir(exist_ok=True, parents=True)
        
        # Ghi file Excel
        with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
            df_output.to_excel(writer, sheet_name="generation_evaluated", index=False)
        
        print(f"‚úÖ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {output_path}")
        print(f"üìä T·ªïng s·ªë d√≤ng ƒë√£ ƒë√°nh gi√°: {len(all_results)}")
        
        # In th·ªëng k√™ t√≥m t·∫Øt
        if all_results:
            avg_llm_score = sum(r.get("gen_overall_score", 0) for r in all_results) / len(all_results)
            avg_bleu1 = sum(r.get("gen_bleu_1", 0) for r in all_results) / len(all_results)
            avg_rouge_l = sum(r.get("gen_rouge_l", 0) for r in all_results) / len(all_results)
            
            print("\nüìà TH·ªêNG K√ä T·ªîNG K·∫æT:")
            print(f"  ‚Ä¢ LLM Score trung b√¨nh: {avg_llm_score:.2f}")
            print(f"  ‚Ä¢ BLEU-1 trung b√¨nh: {avg_bleu1:.3f}")
            print(f"  ‚Ä¢ Rouge-L trung b√¨nh: {avg_rouge_l:.3f}")
            print(f"  ‚Ä¢ Th·ªùi gian x·ª≠ l√Ω: {total_time:.1f}s")
            print(f"  ‚Ä¢ T·ªëc ƒë·ªô: {avg_time_per_row:.2f}s/d√≤ng")

def main():
    """H√†m main v·ªõi argument parsing."""
    # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh
    script_dir = Path(__file__).parent
    default_input = script_dir.parent / "input" / "main_vimqa_dev_300lines.xlsx"
    default_output = script_dir / "outputs" / "main_vimqa_dev_300lines_generation_evaluated.xlsx"
    
    parser = argparse.ArgumentParser(
        description="ƒê√°nh gi√° generation t·ª´ Excel v·ªõi LLM v√† c√°c metrics t·ª± ƒë·ªông (c√≥ batch processing)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
V√≠ d·ª• s·ª≠ d·ª•ng:
  python evaluator_generation_for_xlsx.py --start_row 0 --end_row 5
  python evaluator_generation_for_xlsx.py -i data.xlsx -o result.xlsx -s my_sheet
  python evaluator_generation_for_xlsx.py --model gpt-4 --temperature 0.1
  python evaluator_generation_for_xlsx.py --batch_size 20 --max_workers 5
        """
    )
    
    parser.add_argument(
        "--input", "-i", 
        default=str(default_input),
        help=f"ƒê∆∞·ªùng d·∫´n file Excel ƒë·∫ßu v√†o (m·∫∑c ƒë·ªãnh: {default_input})"
    )
    parser.add_argument(
        "--output", "-o", 
        default=str(default_output),
        help=f"ƒê∆∞·ªùng d·∫´n file Excel ƒë·∫ßu ra (m·∫∑c ƒë·ªãnh: {default_output})"
    )
    parser.add_argument(
        "--sheet", "-s", 
        default="baseline_result_QA_raw",
        help="T√™n sheet ch·ª©a d·ªØ li·ªáu generation (m·∫∑c ƒë·ªãnh: baseline_result_QA_raw)"
    )
    parser.add_argument(
        "--start_row", 
        type=int,
        help="D√≤ng b·∫Øt ƒë·∫ßu ƒë√°nh gi√° (t√≠nh t·ª´ 0, m·∫∑c ƒë·ªãnh: ƒë√°nh gi√° t·ª´ ƒë·∫ßu)"
    )
    parser.add_argument(
        "--end_row", 
        type=int,
        help="D√≤ng k·∫øt th√∫c ƒë√°nh gi√° (kh√¥ng bao g·ªìm, m·∫∑c ƒë·ªãnh: ƒë√°nh gi√° ƒë·∫øn cu·ªëi)"
    )
    parser.add_argument(
        "--model", "-m", 
        default="gpt-4o-mini",
        help="T√™n model LLM ƒë√°nh gi√° (m·∫∑c ƒë·ªãnh: gpt-4o-mini)"
    )
    parser.add_argument(
        "--temperature", "-t", 
        type=float, 
        default=0.0,
        help="Temperature cho LLM (m·∫∑c ƒë·ªãnh: 0.0)"
    )
    parser.add_argument(
        "--batch_size", "-b",
        type=int,
        default=10,
        help="K√≠ch th∆∞·ªõc m·ªói batch (m·∫∑c ƒë·ªãnh: 10)"
    )
    parser.add_argument(
        "--max_workers", "-w",
        type=int,
        default=3,
        help="S·ªë worker song song t·ªëi ƒëa (m·∫∑c ƒë·ªãnh: 3)"
    )
    
    args = parser.parse_args()
    
    print("üöÄ GENERATION EVALUATOR WITH BATCH PROCESSING")
    print("=" * 60)
    print(f"üìÅ Input: {args.input}")
    print(f"üìÅ Output: {args.output}")
    print(f"üìÑ Sheet: {args.sheet}")
    print(f"ü§ñ Model: {args.model}")
    print(f"üå°Ô∏è Temperature: {args.temperature}")
    print(f"üì¶ Batch size: {args.batch_size}")
    print(f"‚ö° Max workers: {args.max_workers}")
    if args.start_row is not None or args.end_row is not None:
        print(f"üìç Ph·∫°m vi: d√≤ng {args.start_row or 0} ƒë·∫øn {args.end_row or 'cu·ªëi'}")
    print("=" * 60)

    try:
        evaluator = ExcelGenerationEvaluator(
            model_name=args.model,
            temperature=args.temperature,
            batch_size=args.batch_size,
            max_workers=args.max_workers
        )
        
        evaluator.evaluate_excel(
            input_path=Path(args.input),
            output_path=Path(args.output),
            sheet_name=args.sheet,
            start_row=args.start_row,
            end_row=args.end_row
        )
        
        print("\nüéâ ƒê√°nh gi√° ho√†n th√†nh th√†nh c√¥ng!")
        
    except Exception as e:
        print(f"\n‚ùå L·ªói trong qu√° tr√¨nh ƒë√°nh gi√°: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
