#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VIMQA Dataset Analysis Tool
T·∫°o b√°o c√°o ph√¢n t√≠ch chi ti·∫øt t·ª´ file vimqa_dev_300_vi.json

Author: Doan Ngoc Cuong
Date: 2025-06-06
"""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter, defaultdict
import re
import numpy as np
import os
from datetime import datetime

class VIMQAAnalyzer:
    def __init__(self, data_file):
        """
        Kh·ªüi t·∫°o analyzer v·ªõi file d·ªØ li·ªáu VIMQA
        
        Args:
            data_file (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON ch·ª©a d·ªØ li·ªáu VIMQA
        """
        self.data_file = data_file
        self.data = None
        self.subset_300 = None
        self.analysis_results = {}
        
    def load_data(self):
        """ƒê·ªçc d·ªØ li·ªáu t·ª´ file JSON"""
        try:
            with open(self.data_file, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
            
            # L·∫•y 300 c√¢u ƒë·∫ßu ti√™n
            self.subset_300 = self.data[:300]
            print(f"‚úÖ ƒê√£ ƒë·ªçc {len(self.data)} c√¢u h·ªèi, ph√¢n t√≠ch 300 c√¢u ƒë·∫ßu ti√™n")
            return True
        except Exception as e:
            print(f"‚ùå L·ªói ƒë·ªçc file: {e}")
            return False
    
    def classify_question_type(self, question):
        """Ph√¢n lo·∫°i lo·∫°i c√¢u h·ªèi"""
        question_lower = question.lower()
        
        if question.endswith('?') and any(word in question_lower for word in ['ph·∫£i kh√¥ng', 'c√≥ ph·∫£i', 'ƒë√∫ng kh√¥ng', 'sai kh√¥ng']):
            return 'Yes/No'
        elif question_lower.startswith('ai '):
            return 'Who'
        elif question_lower.startswith('g√¨ ') or 'g√¨?' in question_lower or question_lower.startswith('c√°i g√¨'):
            return 'What'
        elif question_lower.startswith('ƒë√¢u ') or '·ªü ƒë√¢u' in question_lower:
            return 'Where'
        elif question_lower.startswith('khi n√†o') or 'nƒÉm n√†o' in question_lower or 'ng√†y n√†o' in question_lower:
            return 'When'
        elif question_lower.startswith('t·∫°i sao') or question_lower.startswith('v√¨ sao'):
            return 'Why'
        elif question_lower.startswith('nh∆∞ th·∫ø n√†o') or question_lower.startswith('b·∫±ng c√°ch n√†o'):
            return 'How'
        elif question_lower.startswith('bao nhi√™u'):
            return 'How many/much'
        else:
            return 'Other'
    
    def classify_domain(self, title):
        """Ph√¢n lo·∫°i domain d·ª±a tr√™n title"""
        title_lower = title.lower()
        
        if any(word in title_lower for word in ['f.c.', 'football', 'b√≥ng ƒë√°', 'uefa', 'champions league', 'premier league', 'world cup']):
            return 'Th·ªÉ thao'
        elif any(word in title_lower for word in ['t·ªïng th·ªëng', 'ch·ªß t·ªãch', 'th·ªß t∆∞·ªõng', 'ho√†ng ƒë·∫ø', 'vua', 'nh√† ', 'c√°ch m·∫°ng', 'chi·∫øn tranh']):
            return 'Ch√≠nh tr·ªã/L·ªãch s·ª≠'
        elif any(word in title_lower for word in ['nhi·ªát k·∫ø', 'm·∫°ch', 'ƒëi·ªán', 'h√≥a h·ªçc', 'v·∫≠t l√Ω', 'sinh h·ªçc', 'to√°n h·ªçc']):
            return 'Khoa h·ªçc/C√¥ng ngh·ªá'
        elif any(word in title_lower for word in ['phim', 'di·ªÖn vi√™n', 'ca sƒ©', 'nh·∫°c', 'album', 'ngh·ªá sƒ©', 'c√¥ g√°i l·∫Øm chi√™u']):
            return 'Gi·∫£i tr√≠/Ngh·ªá thu·∫≠t'
        elif any(word in title_lower for word in ['h·∫£i c·∫©u', 'ƒë·ªông v·∫≠t', 'th·ª±c v·∫≠t', 's√¥ng', 'n√∫i', 'bi·ªÉn']):
            return 'ƒê·ªãa l√Ω/T·ª± nhi√™n'
        elif any(word in title_lower for word in ['gi·∫£i nobel', 'gi·∫£i th∆∞·ªüng', 'danh hi·ªáu']):
            return 'Gi·∫£i th∆∞·ªüng'
        elif len(title.split()) <= 3 and not any(char.islower() for char in title):
            return 'Nh√¢n v·∫≠t'
        else:
            return 'Kh√°c'
    
    def classify_answer_type(self, answer):
        """Ph√¢n lo·∫°i lo·∫°i c√¢u tr·∫£ l·ªùi"""
        answer_lower = answer.lower().strip()
        
        if answer_lower in ['ƒë√∫ng', 'sai', 'c√≥', 'kh√¥ng', 'yes', 'no']:
            return 'Yes/No'
        elif re.match(r'^\d+$', answer_lower):
            return 'Number'
        elif re.match(r'^\d{4}$', answer_lower):
            return 'Year'
        elif any(word in answer_lower for word in ['th√°ng', 'ng√†y', 'nƒÉm']):
            return 'Date/Time'
        elif len(answer.split()) == 1:
            return 'Single Word'
        elif len(answer.split()) <= 3:
            return 'Short Phrase'
        else:
            return 'Long Answer'
    
    def analyze_multihop_type(self, item):
        """Ph√¢n t√≠ch lo·∫°i multi-hop reasoning"""
        num_facts = len(item['supporting_facts'])
        
        if num_facts == 1:
            return 'Single-hop'
        elif num_facts == 2:
            facts = item['supporting_facts']
            if len(set([fact[0] for fact in facts])) == 2:
                return 'Bridge entity (2-hop)'
            else:
                return 'Multiple properties (2-hop)'
        else:
            return 'Complex (3+ hop)'
    
    def get_question_domain(self, item):
        """L·∫•y domain ch√≠nh c·ªßa c√¢u h·ªèi d·ª±a tr√™n supporting facts"""
        domains_in_question = []
        for fact in item['supporting_facts']:
            title = fact[0]
            domain = self.classify_domain(title)
            domains_in_question.append(domain)
        
        if domains_in_question:
            return Counter(domains_in_question).most_common(1)[0][0]
        return 'Kh√°c'
    
    def basic_statistics(self):
        """Ph√¢n t√≠ch th·ªëng k√™ c∆° b·∫£n"""
        print("\n=== PH√ÇN T√çCH TH·ªêNG K√ä C∆† B·∫¢N ===")
        
        # ƒê·ªô d√†i c√¢u h·ªèi
        question_lengths = [len(item['question'].split()) for item in self.subset_300]
        
        # ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi
        answer_lengths = [len(item['answer'].split()) for item in self.subset_300]
        
        # S·ªë supporting facts
        supporting_facts_counts = [len(item['supporting_facts']) for item in self.subset_300]
        
        # S·ªë context
        context_counts = [len(item['context']) for item in self.subset_300]
        
        stats = {
            'total_questions': len(self.subset_300),
            'avg_question_length': np.mean(question_lengths),
            'min_question_length': min(question_lengths),
            'max_question_length': max(question_lengths),
            'std_question_length': np.std(question_lengths),
            'avg_answer_length': np.mean(answer_lengths),
            'min_answer_length': min(answer_lengths),
            'max_answer_length': max(answer_lengths),
            'std_answer_length': np.std(answer_lengths),
            'avg_supporting_facts': np.mean(supporting_facts_counts),
            'min_supporting_facts': min(supporting_facts_counts),
            'max_supporting_facts': max(supporting_facts_counts),
            'avg_context_count': np.mean(context_counts),
            'supporting_facts_distribution': dict(Counter(supporting_facts_counts))
        }
        
        self.analysis_results['basic_stats'] = stats
        
        print(f"T·ªïng s·ªë c√¢u h·ªèi: {stats['total_questions']}")
        print(f"ƒê·ªô d√†i c√¢u h·ªèi: TB={stats['avg_question_length']:.1f}, Min={stats['min_question_length']}, Max={stats['max_question_length']}")
        print(f"ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi: TB={stats['avg_answer_length']:.1f}, Min={stats['min_answer_length']}, Max={stats['max_answer_length']}")
        print(f"S·ªë supporting facts: TB={stats['avg_supporting_facts']:.1f}, Min={stats['min_supporting_facts']}, Max={stats['max_supporting_facts']}")
        
        return stats
    
    def analyze_question_types(self):
        """Ph√¢n t√≠ch lo·∫°i c√¢u h·ªèi"""
        print("\n=== PH√ÇN T√çCH LO·∫†I C√ÇU H·ªéI ===")
        
        question_types = [self.classify_question_type(item['question']) for item in self.subset_300]
        qt_dist = Counter(question_types)
        
        self.analysis_results['question_types'] = dict(qt_dist)
        
        print("Ph√¢n b·ªë lo·∫°i c√¢u h·ªèi:")
        for qtype, count in qt_dist.most_common():
            print(f"  {qtype}: {count} c√¢u ({count/len(self.subset_300)*100:.1f}%)")
        
        return qt_dist
    
    def analyze_answer_types(self):
        """Ph√¢n t√≠ch lo·∫°i c√¢u tr·∫£ l·ªùi"""
        print("\n=== PH√ÇN T√çCH LO·∫†I C√ÇU TR·∫¢ L·ªúI ===")
        
        answer_types = [self.classify_answer_type(item['answer']) for item in self.subset_300]
        at_dist = Counter(answer_types)
        
        self.analysis_results['answer_types'] = dict(at_dist)
        
        print("Ph√¢n b·ªë lo·∫°i c√¢u tr·∫£ l·ªùi:")
        for atype, count in at_dist.most_common():
            print(f"  {atype}: {count} c√¢u ({count/len(self.subset_300)*100:.1f}%)")
        
        return at_dist
    
    def analyze_domains(self):
        """Ph√¢n t√≠ch domain"""
        print("\n=== PH√ÇN T√çCH DOMAIN ===")
        
        # Ph√¢n t√≠ch t·∫•t c·∫£ titles
        all_titles = []
        for item in self.subset_300:
            for context_item in item['context']:
                title = context_item[0]
                all_titles.append(title)
        
        unique_titles = len(set(all_titles))
        title_counts = Counter(all_titles)
        
        # Ph√¢n t√≠ch domain cho t·ª´ng c√¢u h·ªèi
        question_domains = [self.get_question_domain(item) for item in self.subset_300]
        domain_dist = Counter(question_domains)
        
        # Ph√¢n t√≠ch domain cho t·∫•t c·∫£ titles
        domains = [self.classify_domain(title) for title in all_titles]
        title_domain_dist = Counter(domains)
        
        self.analysis_results['domains'] = {
            'unique_titles': unique_titles,
            'top_titles': dict(title_counts.most_common(20)),
            'question_domain_distribution': dict(domain_dist),
            'title_domain_distribution': dict(title_domain_dist)
        }
        
        print(f"S·ªë title unique: {unique_titles}")
        print("\nTop 10 title ph·ªï bi·∫øn:")
        for title, count in title_counts.most_common(10):
            print(f"  {title}: {count} l·∫ßn")
        
        print("\nPh√¢n b·ªë domain (theo c√¢u h·ªèi):")
        for domain, count in domain_dist.most_common():
            print(f"  {domain}: {count} c√¢u ({count/len(self.subset_300)*100:.1f}%)")
        
        return domain_dist, title_counts
    
    def analyze_multihop_reasoning(self):
        """Ph√¢n t√≠ch multi-hop reasoning"""
        print("\n=== PH√ÇN T√çCH MULTI-HOP REASONING ===")
        
        multihop_types = [self.analyze_multihop_type(item) for item in self.subset_300]
        mh_dist = Counter(multihop_types)
        
        # Ph√¢n t√≠ch type field
        if 'type' in self.subset_300[0]:
            types = [item['type'] for item in self.subset_300]
            type_dist = Counter(types)
            self.analysis_results['types'] = dict(type_dist)
        
        self.analysis_results['multihop_reasoning'] = dict(mh_dist)
        
        print("Ph√¢n b·ªë lo·∫°i multi-hop reasoning:")
        for mtype, count in mh_dist.most_common():
            print(f"  {mtype}: {count} c√¢u ({count/len(self.subset_300)*100:.1f}%)")
        
        if 'type' in self.subset_300[0]:
            print("\nPh√¢n b·ªë type field:")
            for typ, count in type_dist.most_common():
                print(f"  {typ}: {count} c√¢u ({count/len(self.subset_300)*100:.1f}%)")
        
        return mh_dist
    
    def create_visualizations(self, output_dir="./"):
        """T·∫°o c√°c bi·ªÉu ƒë·ªì visualization"""
        print("\n=== T·∫†O BI·ªÇU ƒê·ªí VISUALIZATION ===")
        
        # Set font cho ti·∫øng Vi·ªát
        plt.rcParams['font.family'] = ['DejaVu Sans']
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Ph√¢n b·ªë lo·∫°i c√¢u h·ªèi
        qt_dist = Counter([self.classify_question_type(item['question']) for item in self.subset_300])
        axes[0,0].pie(qt_dist.values(), labels=qt_dist.keys(), autopct='%1.1f%%')
        axes[0,0].set_title('Phan bo loai cau hoi')
        
        # 2. Ph√¢n b·ªë domain
        domain_dist = Counter([self.get_question_domain(item) for item in self.subset_300])
        axes[0,1].pie(domain_dist.values(), labels=domain_dist.keys(), autopct='%1.1f%%')
        axes[0,1].set_title('Phan bo domain')
        
        # 3. Ph√¢n b·ªë s·ªë supporting facts
        sf_counts = [len(item['supporting_facts']) for item in self.subset_300]
        sf_dist = Counter(sf_counts)
        axes[1,0].bar(sf_dist.keys(), sf_dist.values())
        axes[1,0].set_title('Phan bo so supporting facts')
        axes[1,0].set_xlabel('So supporting facts')
        axes[1,0].set_ylabel('So cau hoi')
        
        # 4. Ph√¢n b·ªë ƒë·ªô d√†i c√¢u h·ªèi
        question_lengths = [len(item['question'].split()) for item in self.subset_300]
        axes[1,1].hist(question_lengths, bins=20, alpha=0.7)
        axes[1,1].set_title('Phan bo do dai cau hoi')
        axes[1,1].set_xlabel('So tu')
        axes[1,1].set_ylabel('So cau hoi')
        
        plt.tight_layout()
        
        chart_path = os.path.join(output_dir, 'vimqa_analysis_charts.png')
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì t·∫°i: {chart_path}")
        return chart_path
    
    def analyze_rag_kg_suitability(self):
        """Ph√¢n t√≠ch s·ª± ph√π h·ª£p v·ªõi RAG-KG"""
        print("\n=== PH√ÇN T√çCH S·ª∞ PH√ô H·ª¢P V·ªöI RAG-KG ===")
        
        # T√≠nh to√°n c√°c ch·ªâ s·ªë ph√π h·ª£p
        bridge_entity_ratio = len([item for item in self.subset_300 if self.analyze_multihop_type(item) == 'Bridge entity (2-hop)']) / len(self.subset_300)
        multihop_ratio = len([item for item in self.subset_300 if len(item['supporting_facts']) >= 2]) / len(self.subset_300)
        domain_diversity = len(set([self.get_question_domain(item) for item in self.subset_300]))
        
        suitability_score = (
            bridge_entity_ratio * 4 +  # Bridge entity reasoning (tr·ªçng s·ªë cao)
            multihop_ratio * 3 +       # Multi-hop reasoning
            min(domain_diversity / 5, 1) * 2 +  # Domain diversity
            1  # Base score
        ) / 10 * 10  # Chu·∫©n h√≥a v·ªÅ thang 10
        
        suitability_analysis = {
            'bridge_entity_ratio': bridge_entity_ratio,
            'multihop_ratio': multihop_ratio,
            'domain_diversity': domain_diversity,
            'suitability_score': suitability_score,
            'assessment': 'R·∫•t ph√π h·ª£p' if suitability_score >= 8 else 'Ph√π h·ª£p' if suitability_score >= 6 else 'Trung b√¨nh'
        }
        
        self.analysis_results['rag_kg_suitability'] = suitability_analysis
        
        print(f"T·ª∑ l·ªá bridge entity reasoning: {bridge_entity_ratio:.1%}")
        print(f"T·ª∑ l·ªá multi-hop reasoning: {multihop_ratio:.1%}")
        print(f"S·ªë domain kh√°c nhau: {domain_diversity}")
        print(f"ƒêi·ªÉm ph√π h·ª£p v·ªõi RAG-KG: {suitability_score:.1f}/10 ({suitability_analysis['assessment']})")
        
        return suitability_analysis
    
    def generate_examples(self, num_examples=5):
        """T·∫°o c√°c v√≠ d·ª• minh h·ªça"""
        print(f"\n=== {num_examples} V√ç D·ª§ MINH H·ªåA ===")
        
        examples = []
        for i in range(min(num_examples, len(self.subset_300))):
            item = self.subset_300[i]
            example = {
                'id': item['_id'],
                'question': item['question'],
                'answer': item['answer'],
                'supporting_facts': item['supporting_facts'],
                'type': item.get('type', 'N/A'),
                'question_type': self.classify_question_type(item['question']),
                'multihop_type': self.analyze_multihop_type(item),
                'domain': self.get_question_domain(item)
            }
            examples.append(example)
            
            print(f"\nV√≠ d·ª• {i+1}:")
            print(f"  Question: {example['question']}")
            print(f"  Answer: {example['answer']}")
            print(f"  Supporting facts: {example['supporting_facts']}")
            print(f"  Type: {example['type']}")
            print(f"  Question type: {example['question_type']}")
            print(f"  Multi-hop type: {example['multihop_type']}")
            print(f"  Domain: {example['domain']}")
        
        self.analysis_results['examples'] = examples
        return examples
    
    def save_results(self, output_dir="./"):
        """L∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch"""
        # L∆∞u JSON
        json_path = os.path.join(output_dir, 'vimqa_analysis_results.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ JSON t·∫°i: {json_path}")
        return json_path
    
    def generate_markdown_report(self, output_dir="./"):
        """T·∫°o b√°o c√°o Markdown chi ti·∫øt"""
        print("\n=== T·∫†O B√ÅO C√ÅO MARKDOWN ===")
        
        report_path = os.path.join(output_dir, 'vimqa_analysis_report.md')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# B√°o c√°o ph√¢n t√≠ch Dataset VIMQA - 300 c√¢u ƒë·∫ßu ti√™n\n\n")
            f.write(f"**Ng√†y t·∫°o:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**File d·ªØ li·ªáu:** {os.path.basename(self.data_file)}\n\n")
            
            # T√≥m t·∫Øt
            f.write("## T√≥m t·∫Øt\n\n")
            stats = self.analysis_results['basic_stats']
            f.write(f"- **T·ªïng s·ªë c√¢u h·ªèi ph√¢n t√≠ch:** {stats['total_questions']}\n")
            f.write(f"- **ƒê·ªô d√†i c√¢u h·ªèi trung b√¨nh:** {stats['avg_question_length']:.1f} t·ª´\n")
            f.write(f"- **ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi trung b√¨nh:** {stats['avg_answer_length']:.1f} t·ª´\n")
            f.write(f"- **S·ªë supporting facts trung b√¨nh:** {stats['avg_supporting_facts']:.1f}\n")
            f.write(f"- **S·ªë title unique:** {self.analysis_results['domains']['unique_titles']}\n\n")
            
            # Ph√¢n b·ªë lo·∫°i c√¢u h·ªèi
            f.write("## Ph√¢n b·ªë lo·∫°i c√¢u h·ªèi\n\n")
            f.write("| Lo·∫°i c√¢u h·ªèi | S·ªë l∆∞·ª£ng | T·ª∑ l·ªá |\n")
            f.write("|--------------|----------|-------|\n")
            for qtype, count in sorted(self.analysis_results['question_types'].items(), key=lambda x: x[1], reverse=True):
                f.write(f"| {qtype} | {count} | {count/stats['total_questions']*100:.1f}% |\n")
            f.write("\n")
            
            # Ph√¢n b·ªë domain
            f.write("## Ph√¢n b·ªë domain\n\n")
            f.write("| Domain | S·ªë l∆∞·ª£ng | T·ª∑ l·ªá |\n")
            f.write("|--------|----------|-------|\n")
            for domain, count in sorted(self.analysis_results['domains']['question_domain_distribution'].items(), key=lambda x: x[1], reverse=True):
                f.write(f"| {domain} | {count} | {count/stats['total_questions']*100:.1f}% |\n")
            f.write("\n")
            
            # Multi-hop reasoning
            f.write("## Ph√¢n b·ªë Multi-hop Reasoning\n\n")
            f.write("| Lo·∫°i reasoning | S·ªë l∆∞·ª£ng | T·ª∑ l·ªá |\n")
            f.write("|----------------|----------|-------|\n")
            for mtype, count in sorted(self.analysis_results['multihop_reasoning'].items(), key=lambda x: x[1], reverse=True):
                f.write(f"| {mtype} | {count} | {count/stats['total_questions']*100:.1f}% |\n")
            f.write("\n")
            
            # S·ª± ph√π h·ª£p v·ªõi RAG-KG
            f.write("## S·ª± ph√π h·ª£p v·ªõi RAG-KG\n\n")
            suitability = self.analysis_results['rag_kg_suitability']
            f.write(f"- **ƒêi·ªÉm ph√π h·ª£p:** {suitability['suitability_score']:.1f}/10\n")
            f.write(f"- **ƒê√°nh gi√°:** {suitability['assessment']}\n")
            f.write(f"- **T·ª∑ l·ªá bridge entity reasoning:** {suitability['bridge_entity_ratio']:.1%}\n")
            f.write(f"- **T·ª∑ l·ªá multi-hop reasoning:** {suitability['multihop_ratio']:.1%}\n")
            f.write(f"- **S·ªë domain kh√°c nhau:** {suitability['domain_diversity']}\n\n")
            
            # Top titles
            f.write("## Top 10 titles ph·ªï bi·∫øn\n\n")
            f.write("| Title | S·ªë l·∫ßn xu·∫•t hi·ªán |\n")
            f.write("|-------|------------------|\n")
            for title, count in list(self.analysis_results['domains']['top_titles'].items())[:10]:
                f.write(f"| {title} | {count} |\n")
            f.write("\n")
            
            # V√≠ d·ª•
            f.write("## V√≠ d·ª• minh h·ªça\n\n")
            for i, example in enumerate(self.analysis_results['examples'][:3]):
                f.write(f"### V√≠ d·ª• {i+1}\n")
                f.write(f"- **C√¢u h·ªèi:** {example['question']}\n")
                f.write(f"- **C√¢u tr·∫£ l·ªùi:** {example['answer']}\n")
                f.write(f"- **Supporting facts:** {example['supporting_facts']}\n")
                f.write(f"- **Lo·∫°i c√¢u h·ªèi:** {example['question_type']}\n")
                f.write(f"- **Lo·∫°i reasoning:** {example['multihop_type']}\n")
                f.write(f"- **Domain:** {example['domain']}\n\n")
        
        print(f"‚úÖ ƒê√£ t·∫°o b√°o c√°o Markdown t·∫°i: {report_path}")
        return report_path
    
    def run_full_analysis(self, output_dir="./"):
        """Ch·∫°y ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß"""
        print("üöÄ B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH DATASET VIMQA")
        print("=" * 50)
        
        if not self.load_data():
            return False
        
        # Ch·∫°y c√°c ph√¢n t√≠ch
        self.basic_statistics()
        self.analyze_question_types()
        self.analyze_answer_types()
        self.analyze_domains()
        self.analyze_multihop_reasoning()
        self.analyze_rag_kg_suitability()
        self.generate_examples()
        
        # T·∫°o outputs
        chart_path = self.create_visualizations(output_dir)
        json_path = self.save_results(output_dir)
        report_path = self.generate_markdown_report(output_dir)
        
        print("\n" + "=" * 50)
        print("‚úÖ HO√ÄN TH√ÄNH PH√ÇN T√çCH!")
        print(f"üìä Bi·ªÉu ƒë·ªì: {chart_path}")
        print(f"üìÑ B√°o c√°o: {report_path}")
        print(f"üíæ D·ªØ li·ªáu: {json_path}")
        
        return True

def main():
    """H√†m main ƒë·ªÉ ch·∫°y ph√¢n t√≠ch"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Ph√¢n t√≠ch dataset VIMQA')
    parser.add_argument('input_file', help='ƒê∆∞·ªùng d·∫´n ƒë·∫øn file vimqa_dev_300_vi.json')
    parser.add_argument('--output_dir', '-o', default='./', help='Th∆∞ m·ª•c output (m·∫∑c ƒë·ªãnh: ./)')
    
    args = parser.parse_args()
    
    # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Ch·∫°y ph√¢n t√≠ch
    analyzer = VIMQAAnalyzer(args.input_file)
    success = analyzer.run_full_analysis(args.output_dir)
    
    if success:
        print("\nüéâ Ph√¢n t√≠ch ho√†n th√†nh th√†nh c√¥ng!")
    else:
        print("\n‚ùå C√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh ph√¢n t√≠ch!")

if __name__ == "__main__":
    main()

